{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Activation\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import glob\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-26 10:39:40.472460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-26 10:39:40.502412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-26 10:39:40.502554: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_10 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_9 (Rescaling)        (None, 32, 32, 3)    0           ['input_10[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_56 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_9[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_78 (BatchN  (None, 28, 28, 32)  128         ['conv2d_56[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_78 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_78[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_78 (Separable  (None, 28, 28, 32)  1312        ['activation_78[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_79 (BatchN  (None, 28, 28, 32)  128         ['separable_conv2d_78[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_79 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_79[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_79 (Separable  (None, 28, 28, 32)  1312        ['activation_79[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_45 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_79[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_57 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_56[0][0]']              \n",
      "                                                                                                  \n",
      " add_39 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_45[0][0]',       \n",
      "                                                                  'conv2d_57[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_80 (BatchN  (None, 14, 14, 32)  128         ['add_39[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_80 (Activation)     (None, 14, 14, 32)   0           ['batch_normalization_80[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_80 (Separable  (None, 14, 14, 64)  2336        ['activation_80[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_81 (BatchN  (None, 14, 14, 64)  256         ['separable_conv2d_80[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_81 (Activation)     (None, 14, 14, 64)   0           ['batch_normalization_81[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_81 (Separable  (None, 14, 14, 64)  4672        ['activation_81[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_46 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_81[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_58 (Conv2D)             (None, 7, 7, 64)     2048        ['add_39[0][0]']                 \n",
      "                                                                                                  \n",
      " add_40 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_46[0][0]',       \n",
      "                                                                  'conv2d_58[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_82 (BatchN  (None, 7, 7, 64)    256         ['add_40[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_82 (Activation)     (None, 7, 7, 64)     0           ['batch_normalization_82[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_82 (Separable  (None, 7, 7, 128)   8768        ['activation_82[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_83 (BatchN  (None, 7, 7, 128)   512         ['separable_conv2d_82[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_83 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_83[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_83 (Separable  (None, 7, 7, 128)   17536       ['activation_83[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_47 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_83[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_59 (Conv2D)             (None, 4, 4, 128)    8192        ['add_40[0][0]']                 \n",
      "                                                                                                  \n",
      " add_41 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_47[0][0]',       \n",
      "                                                                  'conv2d_59[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_84 (BatchN  (None, 4, 4, 128)   512         ['add_41[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_84 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_84[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_84 (Separable  (None, 4, 4, 256)   33920       ['activation_84[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_85 (BatchN  (None, 4, 4, 256)   1024        ['separable_conv2d_84[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_85 (Activation)     (None, 4, 4, 256)    0           ['batch_normalization_85[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_85 (Separable  (None, 4, 4, 256)   67840       ['activation_85[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_48 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_85[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_60 (Conv2D)             (None, 2, 2, 256)    32768       ['add_41[0][0]']                 \n",
      "                                                                                                  \n",
      " add_42 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_48[0][0]',       \n",
      "                                                                  'conv2d_60[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_6 (Flatten)            (None, 1024)         0           ['add_42[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_35 (Dropout)           (None, 1024)         0           ['flatten_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_33 (Dense)               (None, 256)          262400      ['dropout_35[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_36 (Dropout)           (None, 256)          0           ['dense_33[0][0]']               \n",
      "                                                                                                  \n",
      " dense_34 (Dense)               (None, 128)          32896       ['dropout_36[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 128)          0           ['dense_34[0][0]']               \n",
      "                                                                                                  \n",
      " dense_35 (Dense)               (None, 100)          12900       ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "782/782 [==============================] - 6s 6ms/step - loss: 4.4506 - accuracy: 0.0347 - val_loss: 4.1403 - val_accuracy: 0.0821\n",
      "Epoch 2/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 4.0785 - accuracy: 0.0767 - val_loss: 3.7783 - val_accuracy: 0.1297\n",
      "Epoch 3/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8532 - accuracy: 0.1074 - val_loss: 3.6159 - val_accuracy: 0.1581\n",
      "Epoch 4/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.7080 - accuracy: 0.1303 - val_loss: 3.4649 - val_accuracy: 0.1844\n",
      "Epoch 5/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.5806 - accuracy: 0.1500 - val_loss: 3.3351 - val_accuracy: 0.2035\n",
      "Epoch 6/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.4839 - accuracy: 0.1639 - val_loss: 3.2576 - val_accuracy: 0.2167\n",
      "Epoch 7/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.3900 - accuracy: 0.1820 - val_loss: 3.1958 - val_accuracy: 0.2251\n",
      "Epoch 8/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.3167 - accuracy: 0.1953 - val_loss: 3.1148 - val_accuracy: 0.2412\n",
      "Epoch 9/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.2366 - accuracy: 0.2062 - val_loss: 3.0353 - val_accuracy: 0.2607\n",
      "Epoch 10/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.1704 - accuracy: 0.2201 - val_loss: 3.0042 - val_accuracy: 0.2626\n",
      "Epoch 11/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.1012 - accuracy: 0.2291 - val_loss: 2.9789 - val_accuracy: 0.2640\n",
      "Epoch 12/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0389 - accuracy: 0.2425 - val_loss: 2.9193 - val_accuracy: 0.2759\n",
      "Epoch 13/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.9859 - accuracy: 0.2538 - val_loss: 2.8603 - val_accuracy: 0.2907\n",
      "Epoch 14/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9299 - accuracy: 0.2634 - val_loss: 2.8766 - val_accuracy: 0.2862\n",
      "Epoch 15/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8864 - accuracy: 0.2701 - val_loss: 2.7781 - val_accuracy: 0.3073\n",
      "Epoch 16/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8426 - accuracy: 0.2795 - val_loss: 2.7584 - val_accuracy: 0.3093\n",
      "Epoch 17/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7961 - accuracy: 0.2889 - val_loss: 2.7250 - val_accuracy: 0.3154\n",
      "Epoch 18/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7572 - accuracy: 0.2942 - val_loss: 2.6819 - val_accuracy: 0.3260\n",
      "Epoch 19/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7235 - accuracy: 0.3040 - val_loss: 2.7123 - val_accuracy: 0.3163\n",
      "Epoch 20/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6827 - accuracy: 0.3113 - val_loss: 2.6853 - val_accuracy: 0.3294\n",
      "Epoch 21/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6397 - accuracy: 0.3204 - val_loss: 2.6229 - val_accuracy: 0.3373\n",
      "Epoch 22/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6053 - accuracy: 0.3276 - val_loss: 2.6076 - val_accuracy: 0.3403\n",
      "Epoch 23/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5795 - accuracy: 0.3304 - val_loss: 2.6235 - val_accuracy: 0.3352\n",
      "Epoch 24/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5431 - accuracy: 0.3402 - val_loss: 2.5804 - val_accuracy: 0.3506\n",
      "Epoch 25/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5080 - accuracy: 0.3451 - val_loss: 2.5571 - val_accuracy: 0.3505\n",
      "Epoch 26/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4749 - accuracy: 0.3519 - val_loss: 2.5204 - val_accuracy: 0.3580\n",
      "Epoch 27/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4455 - accuracy: 0.3587 - val_loss: 2.5552 - val_accuracy: 0.3527\n",
      "Epoch 28/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4217 - accuracy: 0.3625 - val_loss: 2.6219 - val_accuracy: 0.3366\n",
      "Epoch 29/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3922 - accuracy: 0.3665 - val_loss: 2.5087 - val_accuracy: 0.3579\n",
      "Epoch 30/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3640 - accuracy: 0.3742 - val_loss: 2.4959 - val_accuracy: 0.3666\n",
      "Epoch 31/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3468 - accuracy: 0.3788 - val_loss: 2.4869 - val_accuracy: 0.3686\n",
      "Epoch 32/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3083 - accuracy: 0.3859 - val_loss: 2.4976 - val_accuracy: 0.3679\n",
      "Epoch 33/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2928 - accuracy: 0.3874 - val_loss: 2.4855 - val_accuracy: 0.3722\n",
      "Epoch 34/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2602 - accuracy: 0.3939 - val_loss: 2.4831 - val_accuracy: 0.3665\n",
      "Epoch 35/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2456 - accuracy: 0.3988 - val_loss: 2.5261 - val_accuracy: 0.3650\n",
      "Epoch 36/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2321 - accuracy: 0.4009 - val_loss: 2.4757 - val_accuracy: 0.3719\n",
      "Epoch 37/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1900 - accuracy: 0.4117 - val_loss: 2.4446 - val_accuracy: 0.3729\n",
      "Epoch 38/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1881 - accuracy: 0.4103 - val_loss: 2.4981 - val_accuracy: 0.3728\n",
      "Epoch 39/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1561 - accuracy: 0.4152 - val_loss: 2.4364 - val_accuracy: 0.3809\n",
      "Epoch 40/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1334 - accuracy: 0.4202 - val_loss: 2.4879 - val_accuracy: 0.3760\n",
      "Epoch 41/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1108 - accuracy: 0.4250 - val_loss: 2.4262 - val_accuracy: 0.3823\n",
      "Epoch 42/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0861 - accuracy: 0.4309 - val_loss: 2.4532 - val_accuracy: 0.3836\n",
      "Epoch 43/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0706 - accuracy: 0.4342 - val_loss: 2.4438 - val_accuracy: 0.3821\n",
      "Epoch 44/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0469 - accuracy: 0.4388 - val_loss: 2.5919 - val_accuracy: 0.3605\n",
      "Epoch 45/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0304 - accuracy: 0.4456 - val_loss: 2.4718 - val_accuracy: 0.3788\n",
      "Epoch 46/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0243 - accuracy: 0.4461 - val_loss: 2.5217 - val_accuracy: 0.3743\n",
      "Epoch 47/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9950 - accuracy: 0.4507 - val_loss: 2.4267 - val_accuracy: 0.3900\n",
      "Epoch 48/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9757 - accuracy: 0.4562 - val_loss: 2.5399 - val_accuracy: 0.3762\n",
      "Epoch 49/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9600 - accuracy: 0.4576 - val_loss: 2.4469 - val_accuracy: 0.3855\n",
      "Epoch 50/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9425 - accuracy: 0.4594 - val_loss: 2.4241 - val_accuracy: 0.3928\n",
      "Epoch 51/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9225 - accuracy: 0.4666 - val_loss: 2.4108 - val_accuracy: 0.3950\n",
      "Epoch 52/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9118 - accuracy: 0.4694 - val_loss: 2.4766 - val_accuracy: 0.3870\n",
      "Epoch 53/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8968 - accuracy: 0.4698 - val_loss: 2.4443 - val_accuracy: 0.3925\n",
      "Epoch 54/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8773 - accuracy: 0.4794 - val_loss: 2.4151 - val_accuracy: 0.3951\n",
      "Epoch 55/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8579 - accuracy: 0.4827 - val_loss: 2.4889 - val_accuracy: 0.3884\n",
      "Epoch 56/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8411 - accuracy: 0.4855 - val_loss: 2.4683 - val_accuracy: 0.3891\n",
      "Epoch 57/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8315 - accuracy: 0.4857 - val_loss: 2.5341 - val_accuracy: 0.3805\n",
      "Epoch 58/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8153 - accuracy: 0.4935 - val_loss: 2.5219 - val_accuracy: 0.3886\n",
      "Epoch 59/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8039 - accuracy: 0.4923 - val_loss: 2.4813 - val_accuracy: 0.3937\n",
      "Epoch 60/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7854 - accuracy: 0.4974 - val_loss: 2.5557 - val_accuracy: 0.3856\n",
      "Epoch 61/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7656 - accuracy: 0.5012 - val_loss: 2.4766 - val_accuracy: 0.3934\n",
      "Epoch 62/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7570 - accuracy: 0.5024 - val_loss: 2.5375 - val_accuracy: 0.3863\n",
      "Epoch 63/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7428 - accuracy: 0.5042 - val_loss: 2.4500 - val_accuracy: 0.3926\n",
      "Epoch 64/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7258 - accuracy: 0.5088 - val_loss: 2.4786 - val_accuracy: 0.3918\n",
      "Epoch 65/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7109 - accuracy: 0.5132 - val_loss: 2.4780 - val_accuracy: 0.3957\n",
      "Epoch 66/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7085 - accuracy: 0.5151 - val_loss: 2.5098 - val_accuracy: 0.3929\n",
      "Epoch 67/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6874 - accuracy: 0.5166 - val_loss: 2.4613 - val_accuracy: 0.3996\n",
      "Epoch 68/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6743 - accuracy: 0.5204 - val_loss: 2.4952 - val_accuracy: 0.3967\n",
      "Epoch 69/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6687 - accuracy: 0.5210 - val_loss: 2.4889 - val_accuracy: 0.3962\n",
      "Epoch 70/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6461 - accuracy: 0.5310 - val_loss: 2.4850 - val_accuracy: 0.3990\n",
      "Epoch 71/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6417 - accuracy: 0.5268 - val_loss: 2.5168 - val_accuracy: 0.3923\n",
      "Epoch 72/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6252 - accuracy: 0.5343 - val_loss: 2.5651 - val_accuracy: 0.3886\n",
      "Epoch 73/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6188 - accuracy: 0.5356 - val_loss: 2.5325 - val_accuracy: 0.3918\n",
      "Epoch 74/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6071 - accuracy: 0.5363 - val_loss: 2.5262 - val_accuracy: 0.3920\n",
      "Epoch 75/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6013 - accuracy: 0.5397 - val_loss: 2.5599 - val_accuracy: 0.3918\n",
      "Epoch 76/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5832 - accuracy: 0.5422 - val_loss: 2.5438 - val_accuracy: 0.3908\n",
      "Epoch 77/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5673 - accuracy: 0.5454 - val_loss: 2.5874 - val_accuracy: 0.3901\n",
      "Epoch 78/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5570 - accuracy: 0.5478 - val_loss: 2.6537 - val_accuracy: 0.3828\n",
      "Epoch 79/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5466 - accuracy: 0.5497 - val_loss: 2.5883 - val_accuracy: 0.3907\n",
      "Epoch 80/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5346 - accuracy: 0.5508 - val_loss: 2.5708 - val_accuracy: 0.3923\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train, y_train,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy = 39.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdr_blend import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = len(x_train)\n",
    "\n",
    "x_dataAug = []\n",
    "y_dataAug = []\n",
    "for image in range(0,end):\n",
    "    x_dataAug.append(hdr(x_train[image]))\n",
    "    y_dataAug.append(y_train[image])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataAug = np.asarray(y_dataAug)\n",
    "x_dataAug = np.asarray(x_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlAAAAEyCAYAAAA1Nu6JAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA+U0lEQVR4nO3deXhkZ3km/PupRbtaW++LLbu92+A2bptgG8fsDjAfZk0YkjEZiMlF+AKTZBiGTMDMyscHhAxhyJhAbBLCEgzYLBMwxsaxje1u2+321rZ739RSt9TaVartmT/OES3LOvfRkVSq6vb9u66+pK6nTtWrU+c89dapU3eZu0NERERE5i5V7QGIiIiInGw0gRIRERFJSBMoERERkYQ0gRIRERFJSBMoERERkYQ0gRIRERFJSBOoU5CZuZndvQi3c7eZLXnOhZl1h3/DzUt93yICmNl7w33wvdUey3RmdmM4rmsqeB81+bdL7dEEqorMbLOZ/Z2Z7TazCTMbNrPHzez/N7N11R7fyWLahGvvHK/vM/5NmtlRM3vEzP7WzH7LzNIRy948y/LjZvaUmX3OzFYs6h8nskCzbK9T2/xeM7vFzM6v9hhPZmbWbmb/3sy+EfaBYriOXxtx/dVhvxk1s7MirvPO8DZ+FdWLZlx/amJ5I7nO1MTw5ojLp/8bNbODZvZzM/vPZnY2ud2Zy5bMbCB8Af5eM7O48Z+sMtUewItRuEF9GsBHARQB3AHgnwDUAbgCwJ8B+KCZXe/u353HXZwPYHwRhvpvADQtwu3Uqk+FP9MA2gFcCOD3ALwPwFYze4+7Pxux7G0AtoW/rwLwRgB/AuDtZnapu/dXatAi8/Spab+3AbgcwT7+djO7yt23VWVUJ79uAJ8Jfz8I4BiCnjArdz9iZh8AcCuAvw/XfWmqHr54/hsAYwB+d3qtwh4D8IPw90YAKwG8HMBfAPhzM/sigD9z92LE8lPbVxbAWQDeCuA3AWwG8KEKjbmqNIGqjr9AMHnaC+DN7v7k9KKZvR3APwD4lpm9zt3vSnLj7r5jMQbp7vsX43ZqlbvfOPMyM1sF4IsA3gng52a22d37Zln8B+5+87TlGgA8AOBiBM3iU7MsI1I1Edv7FxFsrx8B8N6lHdEpYx+A1wJ41N0HwiM817MF3P17ZnZLeL2PA/gvwK9fXP8dgE4AN7j7rkoOfIZtEdvIqwDcDODDABoA/OFsC89c1syuBHAPgoMBn3P3PYs83qrTW3hLzMy6EUygCgD+n5mTJwBw91sB/DsER0a+bGapacv/+v15M7s2PEw6NP1cJYs4B8rM1oRvGfaFbxluM7Przeya2Q7/2iznQE2/rpltMrMfm9lg+DbWL83silnud62ZfcLM7jOzI2aWN7PDZvaPtfb2gbv3AvgdAHcD2ICguc1luRyAb4T/vawigxNZfD8Lf875rWczW29mf23BqQeTZtZvZreb2Qu2+2lvLV1jZu8ws4fCXjFgZt+yiFMVzOxSM/tnMxux4NSGn5vZK2LGdZ4Fb7EfCMfVG/aYcyOuf5aZ/ZOZHTezMTO738zeNNf1MMXdj7v7ne4+kHDRP0Yw+fqEmW2edtnrAPzQ3b+SdCyVEL6AfwOAPIAbzOxlc1zuPgA7ABiASys3wurRBGrp/T6CI3/fd/fHyfX+FsBhAOciOAw60zsA/AjACILDvd9hd2pmKwHcj+BV5tMAvgDgUQD/C8Eri6Q2h7fXEI71RwCuAnDnLA3ragAfAzCI4LD1XyI4WvMOAFvM7OJ53H/FuHsZwH8N//vuBO/hT12vsPijEqmIqfN0ts7lyuGT5zYAHwTwDIKjtT9EsI/fa2ZvjFj0gwiOqu8F8CUATwD4bQRHeetn3McVAP4lHNv/AfDXCJ6870bwltJs47oWwCMA3gNgC4C/AnAngLcBeGjmk354Ts9UD/pVeP2DCN7CelvsilgE7j6M4AhUCsA/hJOoTwPoA/D+pRjDXIXvanwHQY97d4JFT+meqLfwlt5V4c+fsyu5ezE8ivSvAVwJYObbeG8E8EZ3/+c53u//QPhevbv/h6kLzewLAB6a421M9yYAvz/jbawPIJjMfRhBw5zyCwCr3H1k+g2EE6f7EDSN35rHGCrpXgTnp61EsN7o4WczawTwu9OWFakpM44wL0NwpPRKBC9+PjuH5TMInkRbALzK3X85rbYWwcTlq2bW7e6TMxa/FsBl0180mtk/Ingyfkt4u1NvYX0NwTk417n7bdOu/2EEL/xmjqsDwDcRnPd5tbs/Na12IYAHEbzImz6J+hKALgAfcfe/mnb9t+DEeUAV5+6/NLPPIzjv9V4A9QDeFXHawFxcM/OdhGk2zfM2p9yNoMddPpcrm9nVCA4A5DG/55iapwnU0lsT/jwwh+tOXWftLLXb5jp5MrM6BI1qCCeOrAAA3P0xM/s6kr/iuW/65Cn0NQSvFp+3g0U1g/C+fwHg9WaWdfeaeZXi7pNm1o/gZNAVeOEE6rrw7VggmGS9GcFbfvcA+PJSjVMkgU/OctlTAL4588VNhDcB2Ajgs9MnTwDg7ofN7DMIJjivAfCTGcv+z1mOuH8FQV+6HCeOoF+B4En3numTp9BfA/h/wzFM928QfAjkQ9MnT+G4njSzrwD4iJld4O5Pmdl6BG+T7Qlvc/r1bzOzX2L2o/6VciOCF5xNAL7t7j9cwG39Jio39kPhz1nf7p02cZt+ErkhOPG8p0JjqipNoJbe1CHNueQrsesmmdGfi+AV3daIRnkvkk+gXnDI390LZtYLoGNmLTy34A8RvPW3HC/c9pYDqLWdjK3/t4T/prsDwJtqaSIoMsXdf/1WtJk1I/jU6acBfMPMLnT3P4+5ialzkE6POMox9VH38/HCCdRsbxFOvUCc3i+mjhL9csZ14e4lM7sXL5xATY3r4ohxnTNtXE8BuCT8/70Rn3C7G0s7gfooTnza+RozW+7ux+Z5W5+a7URwIDh/FsEJ6vMV99w1c4LuAN7n7gu5z5qmCdTS6wFwHoDT5nDd9dOWmelIgvtsC3/2RtSjLmcGIy4vIjj5/dfM7I8RnGNwHMEkYz+Cw+0O4DoEn1x73nkQ1WbBp+o6w/8eneUqv+/uN1uQ0XImgk/R/DaCo081df6CyEzuPobg3KC3ITj356Nm9jfuzo6Md4U/3xlz8y2zXDY4y2VTH4ef3i/ietVsfW9qXH8wx3HN5z4qwsxejuCDKnsA/D2ATyA4DeIdSzWGBKbeCZmtH/56gh5Ozl8B4KsA/sbM9rn7L5ZmiEtLE6ildy+AVyE4QTLyUxbhE/M14X/vm+UqSRLCh8OfUdkkkZklCxWeN/EpBE3pZTMP5cZ9sqaKrkKwf/S6+96oK4WvYJ8zs3+N4Fyp95nZ7e5++5KMUmQB3H3QzJ5BcOTnZeCnFgyFP99Swe176j6ietJqsszF7r69Qvex6MysCcGkKYXgbcj7ETw3vN3Mftfd/2EpxpHAq8KfD7IrhZPzn5vZv0JwYv8tZnauuy9GNmFN0afwlt7NAEoA3hqe4Bjl3yKY8T+DWQ5nJ7QDwASAl5pZ6yz1q2a5bLEsR3B+wv2zTJ5a8PwTO2uCBbERU29n/ONclgk/uTf1acbP2BzSg0VqxNRbaHHPBw+EP19ZwbE8Ev58wVto4T41W69KOq5Hw59XReyn18zxdhbq8wje9vyMu98b9pDrAYwC+GJ4rlZNMLPzEBx5dMy9J25HcJBgPYJYnlOOJlBLzN13A/jvCE60u93MLph5HTO7DsFbXiUAHwx3rIXcZx7AtxEcuv5PM+7rYgSvfiqlD8HbdZeGE6ap+80i+BuXV/C+EwvjHr6FoInuR/BYzYm7P4jgE03norLrVGRRhL3mDAQfM78/5uq3AdgF4I+i4grM7BXhkZX5uh/Bi8arw0/ETfchvPD8JyA4r2cQwCfN7AWfEDOzlE377jx3P4jgVIIzMCMhO7zPip//FK6/DyCIhPj1uUNh2OSfIHjR+bUEESoVY2a/CeCfEXxTxpfd/bEEi/9XADkAfxZ+WvKUorfwquNGAM0IdpTHzOynAJ5EMKm6AkHWyQSAdy/ie8cfA/BqBOc6vBxBo1oD4F0ITvi8DsCCJmqzcfeymf3P8P4fN7PbEOyIr0JwjtFdOHFoeKGWW/QXEI+7+/RohemfGknhxFe5XBWO7yEA75nHyZyfQPBppU+a2TfCyatI1c04wboZwAU4ER/y8TBENlL4IZG3AfgpgB+b2f0IJgDjCD6BehmC8wHXYJ5fJeXubmbvQzDBudXMvgdgJ4LzJF+L4In82hnL9JvZOwB8H8ADZnYngn5aRnCu6SsQnCfVMG2xP0KQ//QFM3s9gq8xmfrk2A8B/Ksk4zazz+LEi8Gpo2T/3symok1+4O4/CK+7HMH5QTkEX9XyvB7h7l8JJ3JvCsf5vE8KVtCmadtIPYK3OF+OYDspIzhi9tEkN+juh8zsfyM4Ov9RAP9x0UZbAzSBqoLwiNKfmtm3EewgVyP46G8JQdDc5wB8IXyltFj32RsG1P13BBlSL0fwSu+DCL5z6TqcOFdqsf0FghMP34/gVdcQggb5n7C4X3nSjOivUBjC87OpgBOv/PIIAkn3Afg6grDPn83nyJ+7P2pm30cQxvcBBEGDIrVg+qekSgj2yR8C+Gt3v2MuN+Du28Oj1n+CILrj9xE8ufYgeGvskwi+C27e3P0+M3slgP+GExO8BxEcFX4DZkygwmXuNLOXIshTegOCt/PyCMKIf4Fgn55+/efM7DcQfArxteFtb0fQB1cg4QQKwUnfp8+47PXTft+LE/lSNyE4z+pPZvsmitD7ATwO4P8zs5959HdyLqaLw39AMAE+juD0j+8C+Ht33znP2/0fCE7w/2Mz+0LcRP1kYu5JzkWWU5GZ/TcEnwS51t1/Wu3xiIiI1DpNoF5EzGytux+ecdlLELydlwewLvxONxERESH0Ft6Ly1Yz24nge6jGEHwC5E0IzgH6Q02eRERE5kZHoF5EzOyTCN7j7wbQiuCTKw8g+GqGu6s1LhERkZONJlAiIiIiCSkHSkRERCShBZ0DZWbXIghDTAP4W3f/NLv+8uXLvbu7eyF3KacsfiS0MDlJ62PjPHampXUZrWcyJ+/pgHFZC6VSkdYnJ/mpb+kMf52Vz/Pldz6z55i7z/oN7tWWpIepf0mkWb+T+ITcxASvT/K4uKamZlqvq6+prxJ9nrj3uIox74IVi3zdTObi+hfPIi0W+He/735ub2T/mvezRhiB/yUAr0PwZZRbwu8Aeypqme7ubmzdOtuXcsuLXolPkI7s30XrDz70CK2/8rUviI55ns6umgpEfx7emoHxEr/GyOgAre/e9TStd3Tx5r1//3O0/sar372PXqFKkvYw9S+JlOcRes8+/jitP71zD61ffPmVtN59xhm0Xk1xScLH8vwa/Ud5HOKe556h9dbOLK0f7TlE67997Xsj+9dC3sK7HMBOd98dJql+C8DM6H0RkVqlHiYi87aQCdQ6PP+buw+Gl4mInAzUw0Rk3hYygZrtjcUXvJlpZjeY2VYz23r06NEF3J2IyKKK7WHqXyISZSETqIMIvkByynoE3zv0PO5+k7tvdvfNK1bU5HmkIvLiFNvD1L9EJMpCJlBbAJxtZmeYWR2A3wFw++IMS0Sk4tTDRGTe5v0pPHcvmtmHAPwUwUeAv0a+WVpEpKaoh4nIQiwo/MbdfwLgJ4s0FigV/dRVjskissJxWh/p203rd93+Pb78CM8K+d33v5/WEbNtlsukHnOc12c9FeeEArttAId79tP6wCD/GHDPAT5n2P3cMVofGuaPXS1bzB5WLsclcsnJqhiTdZYaP0Dro8d4faH9qzuuf8Vsm8UiqcfMEsoxDW6c3TaAXc/xGJXBQR4zMHF8L1++t5/WF9K/lEQuIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIiktCCcqCWmhnPy5HqiUvwSlmJX6E0wm9/gn8PWXM5T+v9PUdovfdIL62njb/WaGtvi6xl67J02XJMDpQ7z1HJ8JtHoTRB612rumi99yjPgerZ9YJvcJJZpFJ6vVqr4hK8MineX1DgWUOpPK8vtH89s2MXrWditr01q6O/pqippYkuW4w5DlMu8wzATF1MRlWZZ2DVtcaMb6xy/Ut7tIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCJ1UOVK1jaRZenqTLFo/zrIqJoVFa97pmWl+2bi2tIybnyGKyiFIxWR/DPQdofe8TD9D6nqd38PtP1cXc/35av/snt9J6x9oNtH7Fla+MLmaW0WX7B4dofXKUZ8Dkcn207kWesdU3sJvWjw/ybdPLeh12KmB7cLk4TJed3M/3r5H+MVovNbTS+qoLz6F1pPhTWVx/yhR51tCRZ56h9f3PPUvrPfv4+llo/9p2/09pvbFrNa23LHt5dLGhgS6771BMfxriYy9M8v5lMRmBoxM8x2l4mGdwLaR/qfOJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQcqMVULkWWju3kOUZ9D99L6+MDPCvoSJ7Phc955TW0fvbFm2k9leWbyuNPPk7rj951F62PxOREDff10no2U0/ruX6eFXLXj/fR+vm/+QZaf8XVr4m+78k8XfZ4H7/v3Vt+Quu9h3fRetfpp9H6eJln9BTG+WNfl1pJ63KSKEZvp/u3PEoXHdu1ndZzQzzLJ65/req9jNY3XXElrWeaeJbRLx94mNb3PbqF1k/2/nXZVddE1oZGeH8Y6OE5coM7/4XWF9q/8lXsXzoCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpLQgnKgzGwvgBEAJQBFd+dhQqc4z01G1vqf4VkXGBym5c50kS+f4llDu++5g9YzbrTesJZncXz9uz+k9Se3bqP1Mzuaab0zxf/+5picqlI6S+u7n+U5K/c++11aX7P+wsjaKy8/ny57dMf9tP7Yz75P65ODx2l97NAFtN50waW83ric1lvP6KD1WqYedkKZZDWNHDhCl82MjNL6QvvXse18H3myzPtXZt0GWv/Rz3lO3c7tPOduXTPPmap0/xo6PEDr//Sdn9L6qrXRPeLi89bTZScPbaP1uP6VHzp5+9diBGm+yt2PLcLtiIhUg3qYiCSmt/BEREREElroBMoB/MzMHjazGxZjQCIiS0g9TETmZaFv4V3p7ofNbCWAO8xsh7vfM/0KYVO6AQBOO42fRyMissRoD1P/EpEoCzoC5e6Hw599AL4P4PJZrnOTu292980rVqxYyN2JiCyquB6m/iUiUeY9gTKzZjNrnfodwOsBPLFYAxMRqST1MBFZiIW8hbcKwPfNbOp2/tHd/3lRRiUiUnnqYSIyb/OeQLn7bgAXL+JYAB7lUfNSdXWRtZaVa+myRw/uofXc0YO03lxXpvXhHF+5Ox64l9bHO06n9Z/97D6+/Eh0xgwAtKbW8HoHz1kZm+Q5Kzv28xybI2NO6wf7eVbJN27+u+hlt62ky44f2ErrzaUxWq9vrKf1ybFxWj+9heekpFadRes5i97ua9mi97CT/DPNmeboLLb6Vr6NTPTz/lTp/nX4mUdpfbyP5yT96gG+fG5sgtabePtA6zK+j8T1r0NH+2n9SI4/le/u5eP/p298PbJ2+CX8sV9w/2qobP+abOPPXbkUv3/mJN/lRURERJaeJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpLQQr8Lb3HFZGksOCeqwrfvmejVufolPG6mMDpI67v2P0Pr4wNHaT1f30jrzz77NK2PtfAckUyBr9zhfp7DMtQVnUEDAA2n85yo4eM8p2n7Pp4DdTTPc1pa29poff/OxyJrDw7k6LJnL8/Sel2Wr9vBSV5vXckf+57DB2h9WVMnrdd1dtH6iwaPMlr4y9UK3365ITpr7bTLX0aX3ZvjOW+Ha7x/1RVoGb19Q7Q+2Zqm9YZlq2g9rn89e5jX9+VbaL2jaxmtDx2JfnyeHnuILruqiW94cf1rKKZ/tSywf7U2dtC6N/PezugIlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJFRTMQYWEzPgC4wZMI/LMYi7AT4AK0fffrY++iPCALDu8iv5ffNPuqPnkftoff3aDbTef6xE69sffJTWGzP8Y8LLW3lMwDWv5H//yy++gNa/+KUv0frIRJ7W4x4fL/KPaY+PjUfW6jfwj/mXnccc9PYN03qmg39E2ppX0PpjT+6i9aGHd9D6mjPPpPUXi1RMzEB5gS9XU+W4HIO4G+ADSBWjb7+plX/Ue8MrXkHr5Qwf+/Au3l/WrFxL6/2DtBzfv9Kj/P7b+LrbfOlltH7ZS86j9Vv+7m9p/fgYz1mob4p5cizyHjIxGt2/raudLlv26N4HACODfN0WY/pTLfcvHYESERERSUgTKBEREZGENIESERERSUgTKBEREZGENIESERERSUgTKBEREZGENIESERERSWjJc6DKJCspbjZXjslxyuUnab0uw//ctMXkpCAma4PkRBXBx75r4BitH4/JKZo85yJav/DSK2i9sH+A1r/z45/z5SfGaP2t115D62978+tp/bmdu2m9b4znWOU9TetZ58vXZfjyrQ3Rj09zO88xGSrwdde8ag2te+MyWj94lGdYlSZ4hld+kGfI3HX7E7R+qnAHiiQrKa6ZFmNynIZG+ePU3FBP63UpPoJMXIclOVE58LE/0ttL62ONjfyuT+c5Sadf+FJaX7Of70MjP7uL1ss5vg9c/YpLaf11V11F60eOHaH1gTx/bhkv8yDA+nKR1+v57beQbSvdzHPshgr8uam5g/cvVLh/pcf5trH9l3fQOqMjUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJxeZAmdnXALwZQJ+7XxRe1gng2wC6AewF8C53Px53W2V3TBbykfWGujq6/PD4KK3ft+VBWl/W0kLrl8RkjbQ2NtF6qRSdxXHo6GG67N338pylPfv30/rkRPR6BYD6td20XhzJ0Xrfvn20PjrCH5uN3RtoPQOewzQ4xLOI8mWe01Qs8Ryb8jjPGkk5z2FJN0Rvu/0DfNfo7eMZYI11zbTe3Mbzz1ra+fKtMRlXjRmeYbZheTutP0KrlbdYPaxULmOYZMq0t/D13NN/lNZ/tfVhWm+MyYK7/JJNtL6mo5PW8/noHvDU3p102aee4L13of2r0MWzhNI53j/G+3hO1fgIzxKK619Z8Bym2P7l/Kk4X+T7YGl4iNbT5ZiMsProYynjY3zsI0ODtF6f4ftFuok/dulGnn8W179KKd77Vy3j8wJmLkegbgZw7YzLPgbgTnc/G8Cd4f9FRGrRzVAPE5FFFjuBcvd7AMyMqX4LgFvC328BcN3iDktEZHGoh4lIJcz3HKhV7t4DAOHPlYs3JBGRilMPE5EFqfhJ5GZ2g5ltNbOtx47ycwBERGrJ9P7V38/PVRORF5f5TqB6zWwNAIQ/+6Ku6O43uftmd9+8fAX/UlURkSUypx42vX91dS1f0gGKSG2b7wTqdgDXh79fD+C2xRmOiMiSUA8TkQWJnUCZ2TcB/ArAuWZ20MzeB+DTAF5nZs8BeF34fxGRmqMeJiKVEJsD5e7vjii9JumdmQFGMhuGR3mW0JZtPFFmf88hWq+v43kSKzr5IfpzuzfS+tBwf2Rt27Z76bI9e5+i9SP7+fkXfcf5utv2+P20fvn682j9zNX87dfjnTxjpm05z3E5cPgIrff08BytsRGetdTe0siXH+U5UMPHZ36I6/nOXLk+stbSwHez8UZeLxV5xkxpjP/tpVRMBk1HF60jw3Na2tr4uq22xephljKkGqLzwA738XM8t2x7jNYHBvnycf3ruT08q21FazutH+49EFnb9SzvvQvtX8eGovO1AODcs1fT+gXt3bS+un0ZrY8sa6X1uP51NCaHKa5/5Ub5PrqynT/2I4M8C+5YD8/BOpv0r/oMP85SquN1LxVoPZ0bpPWs0TLyzR20Xk7z/lnXyDP+GCWRi4iIiCSkCZSIiIhIQppAiYiIiCSkCZSIiIhIQppAiYiIiCSkCZSIiIhIQppAiYiIiCQUmwO1mLwMlCajM2Xue/AhuvzDT26n9Y3nRWdZAMDhAzyr4wc/upPW3/xGnmexa+/T0bUDe+iyqXQDrQ/08RyVQwf30npD6TJaf0l3N63/4b/9PVofHOI5Jhvb22j98GGe4fXc4zxnZqSfZ+i0dfGso1KRr//mMi1jXUd0joyn8nRZK/MbT6ec19M8KKVY4Nvt+Oggv/1MHa2Xyjxn5VThJUdhJHpdPrDlUbr807uj+wMArN/Is4b6j/Cst/sO8f5ZLPPtoLc3ukcdPhKdEQXE96/hfp6jNjTIc5IaSptp/dI1/Lug7T2/w+9/mOfAbWhoovVjA5HfZgYA2PlETP86wpdvWx3Tvwo8i60jZhftIjl5ZeP9KxXTv9z4dpeKjoYMlHj/K+b48zpSvH+Vff79S0egRERERBLSBEpEREQkIU2gRERERBLSBEpEREQkIU2gRERERBLSBEpEREQkIU2gRERERBJa0hyoUrmEkdHozIZf3PNzunzX2uW0PpnL0fq+3Udo3WLydh7afh+tP0FyqixmVafjHorMJC1f85pNtL6yo5PWi+M86+Oic8+l9dTx47R+8Kc8Y6vx2CCtv66V57ysPueltL71aA+t72jM0nr3ep7Rs6Ih+vHL5XjGTLHEc1TKMfk96Qwfe32GZ8Tkx/n46hp5Bk4qW0/rp4piqUDzfh7Y8ku6fFz/8iLfBxfav/bs5zl6lexf5TTvzS/bfB6tdy1rp/X8WHS+IABsXLGO1uvqeY5d34O/ovW4/vXaFt6/zjjjHFp/bpjnaG1r5uv/tHU8I3FZOnr9FQr8uWeizLc7d758KsPHnkrH9Jf8GK/XxYT4pXlOFKMjUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJLWkOlKUM2eboTIe2zha6/KFDu2h9+2NP0Pq+naO0vmY9z8vpWs2zQsrlYmTt+AC/72xMhkv3mTE5SGtbaX1ikmcJ5XM8g6Y0wesTew/R+vhensM0NMRzpBrb22j9stN4zsmaer5+lvUfpvVMRzOtl7PRj72XeE6TxeQ8lQo8Q8fiYpjK6Zj75xk6xUl+/3UpfvunCkunUNfREFmvdP/qO8T3wfYu/jhUs3+d1r2C1uP6V77It9HJyeixA0A5pp4/znOWKt2/Llyzmtbb0/y5qWmC3/9EE886Kqajs5pKcRlgZb5dFou8f8TlyFnZeN35tuFFnkNlNv/jSDoCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpJQbA6UmX0NwJsB9Ln7ReFlNwL4AwBHw6t93N1/EndbY+M5PPjo05H1kvMck3SaD3fP7j20fugQzzJp6eBZJaVSB62PjIxH1uJyVM6IyTFauYLnQB08+Cytd2QGaT17Ic8ZyQxN0PqBbU/S+pPDY7T+46f48kNlniXS3tBE668/dzOtX1G3gdYP9O6l9XRbdNZTsYnnmBRicpY8JmfFy3y/iMtxKpV4DlXay7RezixpnFxii9XDxsZz2PLwzsj6QvvXkZ6DtN7bz/Nusk3RGVXAqd2/6lbxLKH6Eb4P7XvsKVqP61/b9+2l9cE9vH8ua+D996L1F9L62dnltD56jOfcFVqij6Uca+THWUp53l/gPIcJMfsNijxjzGNy9BDTv2wBOXZzOQJ1M4BrZ7n8L919U/gvdvIkIlIlN0M9TEQWWewEyt3vAcBjWkVEapR6mIhUwkLOgfqQmW03s6+ZGT82LCJSe9TDRGTe5juB+jKAjQA2AegB8LmoK5rZDWa21cy2Dg0OzvPuREQW1Zx62PT+NTI0tITDE5FaN68JlLv3unvJ3csAvgLgcnLdm9x9s7tvbmtvn+cwRUQWz1x72PT+1drGvxBWRF5c5jWBMrM10/77VgD8a8RFRGqIepiILNRcYgy+CeAaAMvN7CCATwK4xsw2AXAAewF8oHJDFBGZP/UwEamE2AmUu797lou/Op87m8xPYM/ex6MHk+F5Dyu7eNaFgec9NDTyvIfXvvoNtH7eBWfSemnykcjayk7+t21Ycxqtr+hspfUzN5xL66etWEvr6ZhjkUOH99F6/3Afre8Gz+pofelLab04MUzrgwP8/JTb9vGclwtXrqH1M4znzOBIdM7LRBvP7/Eiz0kpFnmGTbkQnUEFACXwbW88xzN+Gpr5+OsaY9ZNlS1WDysWJ9F/NDqvqOL9i8c84dXXvJ7W15+xjtZLk9FZbAvtX51tzbQe1786GjtpPcOj1jA+fITW4/rXXivS+ug5Z9N6vshzpHpJBhcAjPTvpfX1y/j6ObvMe0RdX3SPKbXz/X+gyDOuSjH9DfV8GmLO64X8CK2nGvn406ijdXrb815SRERE5EVKEygRERGRhDSBEhEREUlIEygRERGRhDSBEhEREUlIEygRERGRhDSBEhEREUkoNgdqMdXVlbG2OzozomN5E12+UOB5OG9402W03t/P8yoyDTwvIp/n93/JJRdG1nJjPAvj8P5jtL7p/OjbBoCN3afT+uAxnqPUc+QwrQ8cOEjrqbP4/b/yVdfQei7Fc0qGR/ljV+QPHZ58Jjp/DAD2P7OT1lemeQ7OslR0ho+X+bIp4/k/VuYZNB7zxxf53SNf4BldmRIP2SnG5MCcKtKZIpatHoyst7TzPCwv8xy6q1+9idYX2r9KRb4dnX9hdBZTOc+XPXKgn9a71/MMvY2n8Yyq4308q+zgAM95yh3pofW4/nXlq87gt1/l/nVgz15aXxeTUVafIgPwhfWvdJk/b1opJicvpn8VYnLysmXev1Ll+efY6QiUiIiISEKaQImIiIgkpAmUiIiISEKaQImIiIgkpAmUiIiISEKaQImIiIgkpAmUiIiISEJLmgM1MjaEe7b8n8h6MSYM47TuFbS+6YoLaH3fLp4VkjKedTQwyrNOyqXonJeRIZ6j0j/Mc5oeemyI1nfsaqX1Q4f47TdM5mj9vPouWk81r6X1I0M8B+W+Lf9C60UeNYJsfSOtD40epfV8lmf0DDXwnJdMOnr5cfB1Wyrz7T6d4btpJqZeiMn/SRl/HZXO8HWTm+QZZ6eK8YkRPPL4LyLrC+1fZ2zqpvWOHp53E9e/JgrV61/P7ub960if+hczPMb7F7J8Hz5ez3tEiiw+nubbXTkmp86ydfy+07y3lko8py5lvD8Z6c0AUCzOv3/pCJSIiIhIQppAiYiIiCSkCZSIiIhIQppAiYiIiCSkCZSIiIhIQppAiYiIiCSkCZSIiIhIQkuaA1XfkMHGs6LzOApFnjexcjXPixge3UfrI2MDtJ7J1NN6odRA60Mj0VklhaLTZTvX84yYbD3PUUk3jNH66efxuXK5xOutGZ7T8i/3Pk3rTz53iN9+azutW4pvqrk8z/LoH+SPfdn57XtHJ62PHD8eWZvIj9NlzYzW6+p4jkpcfSLHM3IydXy/SrGQGADFmByrU8VC+1dLB7/9Qp7n1I2M8R6g/hWtwXnO0gMP8v711E7ev1pa2ml9of1rPDdC62Xn+/BYSxutZ0ej+9ek88cmrn9lsny7TGf52POTPIMrFdP/LKZ/lRfQv3QESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEtIESkRERCSh2BwoM9sA4OsAVgMoA7jJ3f/KzDoBfBtAN4C9AN7l7tFhEgCaGxuwedO5kfXRUZ738NRTj9H6wCC9e5x3wUW03tqyjNYBnnfRdzQ6K6WQ58uODPKcj+Gxo7Te1bk6ps5DaEZzMTkq6XZazzTxnKhSgT+2ddZC600tzbSeismpGjx6gNbb13TTekcd31WGBp6NrJWN5wPV1/Mck1RMzkqxWKD1QoHff3NjE62XimW+fEzGDMAzuCppMftXXX0aG85qj6znRnlWUu/+vbQe179O23gOrdc18JynuP7VMhK9HXgxTZcdHVpY/2prW0nrjc08xylX4PuQl/k2Hte/UOY5TXVWpPWF9q/J4R5ar+9aS+tx/av47K7IWjnF//ZMPc95iutf5RLvX6Uiz7Gra+DPHV7i/SvbENe/jkVW5nIEqgjgT939fAC/AeCPzOwCAB8DcKe7nw3gzvD/IiK1RP1LRCoidgLl7j3u/kj4+wiApwGsA/AWALeEV7sFwHUVGqOIyLyof4lIpSQ6B8rMugFcAuBBAKvcvQcImhQAfgxWRKSK1L9EZDHNeQJlZi0AbgXwEXeP/tKkFy53g5ltNbOtgwP8O3VERCphMfrX8AA/j09EXlzmNIEysyyC5vMNd/9eeHGvma0J62sA9M22rLvf5O6b3X1zeyc/kU5EZLEtVv9a1slPZBaRF5fYCZQFX7X8VQBPu/vnp5VuB3B9+Pv1AG5b/OGJiMyf+peIVEpsjAGAKwH8HoDHzWxbeNnHAXwawHfM7H0A9gN4Z0VGKCIyf+pfIlIRsRMod78X0QEir0lyZ6VyEUOjLFOB50kMD/E8hx07eNbIzt2/pPX1py2n9Zdu2kjrp5HlG1M8Y8pLPCujVCzRel2Wv71gWVpG0wTPsFnTxP/2SzbxnJXlbZ20ft8999H60PFBWi/GrJ+jh2Z9h+bXvLmL1kvn8L8f5PHLNPCx1Wf4gzMxNk7r5RLPoKlr4Aea0+D7VX6Cjx9x8UNVtJj9q1wuY2ws+jzOFHgW0cL7Vz+tx/WvjedvoPVlK6J7SKPx/RureI5Rucj7SzbL9wHPxmT5TPB9YEU9z0m66EKek7fQ/jUyNETrXuZ/39gxvvxkfTutl1afSeupMukRjXzdpmL6V3GCn/tsZZ4DlamP61+8PxVyvJ7K8udeuuy8lxQRERF5kdIESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEppLkOaiSRnQVBc9Z4vLwrjyNy6l9Y0bz6f13fv20nrf0YO0Ptg/SusN2egcq94JnvHS3s5zolpbec6Kx2RZjAzzHJHO5vW0vmLlCn77G3gO1ZZf/YrW+wej88GAIINnISwmq6izk1+hc107rY+RlyJZ469T6hrTtA7jGToTE/w72jzFly+Wec5L3Kofj7n/U0XKgAaWRxSTZ3PeRWfR+roNvH645xCtx/Wvwmie1nPp6KeDsTzfP+ubeQZWQ0tMzlMd3wcmh/i6bY/Jcduwnuc8jR7jOU/btj5E67lcTM6TL6x/lRv48m1tPEMxrn8NkB6TiTnOYjGPXdr4c0NhMqZ/pHn/KjnvX3GrvpCff//SESgRERGRhDSBEhEREUlIEygRERGRhDSBEhEREUlIEygRERGRhDSBEhEREUlIEygRERGRhJY0BwrmSKWjMxtSWZ73sKyNZ4ksX72O1s+/aC2t53I8D6JcLtF6z7GeyFrfEM9R6RvupfXVa3gOU1sbzzEqp3iG1WiBz6X7czwH5dDAMK0/8dR9tD6Z4+unoSEmyClGcxvftjZ08l1haGQ/rafao8fXnl1Oly2D5/OkUvyxKTrfLkdH+GOfTsXkUKX5/Zd4BNkppAxLRz9WqSxfT81tfD0vW8u38dXn8v42ObGG1uNy9o4fH4usjUzwnKORHN8Gm8o8x66+he+flsnR+liR/20HRu+i9f5xnsO345n7ab1UHKT1DMkInIu6Zl5faP+abI5+bs1meUZWKs0zuhCTg5cC33Ymx/hjnyL5ZQDgFexfOgIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJaQIlIiIikpAmUCIiIiIJLWmMQS4/iWcP74yst7XHfNQ1zz8qv6yBf9azo5XffkND3Mct62h9ZUdXZC2baaTLDo8cpfW0889aDg8O0nrv0X5aH+rdR+s7lz9G6+vbLqH197zralp/fAu//Xyef9S/vaOD1iezfP37IP+Y9hNPbaf17hUtkbWuZv4x4OLYAK33l6KjPwBgWbad1t34tjM6NELrDU18v2paFv23B/i2fbIoFIvo7Y+O28g28/6QKfCPYzc38Hbc1sb7kxmPeUk5r5dL0R+1T6d5hEJ+gvfmVEyEQmGEr5vxkXFa39f7DK0PLT9I6+2N59P6b7/9lbR+0vev5dH7cEdM/8rH9K/RYd6/spk2Wi/H9K/cCP/bM438eT/bGJMRQegIlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCmkCJiIiIJKQJlIiIiEhCsTlQZrYBwNcBrAZQBnCTu/+Vmd0I4A9wIuTl4+7+E3ZbpXIJg6PReSG5Is8Cqa/neQ+FVp4nMTI6SuvBnxetKSYvoqVpTWStoY5n5axoW0brhcIErQ+N8ByWgzsP03omxTeF7b0HaP1AAy3jnDqes9IZ89itXbmW1lNlnjWSa+JZIv3ZPlpfB54l0kiyTBqb+bKlcb7yCqUCredzk3z5PF8346N826qv5+Pv6FhN68CemHrlLGb/KnsZE5PReUSFMs/6SWV4fysX+HYwMcqzgAC+HdQ38Byoxub2yFpXHc+4Krby/lEq8HUzMcb/tmd38qyhhfavDvWv6FqF+1dxkm+32Sx/3i2keUZYJsOfW+ubVtI6sCv6tmOWBIAigD9190fMrBXAw2Z2R1j7S3f/7BxuQ0SkGtS/RKQiYidQ7t4DoCf8fcTMngbAY2lFRGqA+peIVEqic6DMrBvAJQAeDC/6kJltN7OvmRnPohcRqSL1LxFZTHOeQJlZC4BbAXzE3YcBfBnARgCbELzC+1zEcjeY2VYz2zo2xN8LFRGpBPUvEVlsc5pAWfAtlbcC+Ia7fw8A3L3X3UvuXgbwFQCXz7asu9/k7pvdfXNzGz+JUURksal/iUglxE6gzMwAfBXA0+7++WmXT//I2VsBPLH4wxMRmT/1LxGplLl8Cu9KAL8H4HEz2xZe9nEA7zazTQAcwF4AH6jA+EREFkL9S0QqYi6fwrsXwGwhFDQzZTZ12QasX3VWZL1Y5DlMqTQ/YDYxwbNG+gbHaH145Citbzid592M10dnpeRG+H23tPCcqK6uLlrPZpto/czTeY5KUwvP8ti9K03r9Rme1ZFawx/b9lU8q2N0dITW0yWeJbLxwujtDgDKO0q0Xijy9dNQH73+Syn+t3e18Mcuk+Xr/vixflq3cj2tj0/wc3sy9Xz5VHour8OqYzH7Vzpdh9Zl0R/gK/NNKPZ4f3EyJguIx91gbOw4rbetaqf1/Hj0/ZfHeVZYXSN/e7O1k29D6Syvn3n6elrPxGRcHdy3sP5VWsH3kUr3r+WnraL1cjN/fjiZ+9dQP89/zDby/QYx29ZC+peSyEVEREQS0gRKREREJCFNoEREREQS0gRKREREJCFNoEREREQS0gRKREREJCFNoEREREQSWtIAF/cS8sXoPKT6+ka6fHNjO62XijwPYnyIB6k0N/G8ilIhOucJAAbGo3NYGur4qraYb4kop3jIzHh+lNZXruY5JU1NPMtj9epOWi+W+PgmyzxHpqtzOa1PDPHlG7I8RyvdFLP8UZ6T0niEr79UOTrHpQSeAZZK8+2+sbmd1sfHeP5ZtoHnuJSc55+VjWfgTBSHaf1U4V5GiWzHqTreH+rqedaQF53Wy+P89W5TY0w7L84Wh3XCRC76b8tk+LKeiemd6ZiMqzzvHw1tfN1mG/jffs7FZ9B6uczXfd55FlF9B+8fRb44sjHrr9DE99H0KP8u7MbWNbRey/0rXRfTv8Dzz2D89hfSv3QESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEtIESkRERCQhTaBEREREEtIESkRERCShJc2BKpVLGBsfiKwXY7I4RkZ7aT1tPMvIjGcZtbXy+vg4v/9sJjrMyWJyPsZyPMdp5DDPqhgdHaF1xKxbL/Ocl3SW18vlmKwQ8OVL4zwoJZPmWSBj49E5JgAwku+ndWvjGT3WzHOkxo5FZ40UnGfcFMHHPjnBH/uC85ymgz2HaP1IX/Q+CQAr1vKcFx/nGT+nCvcSivno/SzlfD0UczzLyMDXs4FnpdXVt/L7j9kHUinSo9L8qWIyz7fh3JEcrRdzfHnj7QvmvPcjE9f/+P6dielfNsZvPx1zqCKf40GA4wWeZZRp5ndgzXz5Wu5fQ0NHaH2g/xitL1vF96tUkd8/XXbeS4qIiIi8SGkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCS1pDpSXUyhMLIusj4320eXLJZ6zks/zLKG6FM+zOL5nnNaHx3iezkUvOSeyNnQkJoPF+ENRLvMcJMTkOO3ZxcdeX8dzVNo7eZZGWwefi7e18wwc5HmOVEMTH9/QKM+ZGR/nOSg+wbetXJbntBQQvV2XCw182TTf7goZnqMyXuA5Trv3H6D1kSG+X7Svr6f1Yoqv21OGp4F8e2Q5NxHXv3jeTD5mH6hL8X3s+MDC+td5F5wVWcsN8cf4VO9f2caYHLsiXz7dxNdPLsdvv1Dg+2imxJ/7Clnef2u5fx3u2Ufrw8d4725Yw/tXZgH9S0egRERERBLSBEpEREQkIU2gRERERBLSBEpEREQkIU2gRERERBLSBEpEREQkIU2gRERERBKKzYEyswYA9wCoD6//XXf/pJl1Avg2gG4AewG8y92Ps9sq5Ms4fHAksl6OyQKpyzbT+qEenrWUz/O8iUyGZ4W0d0RnZQT33xtZS6f435YCv++mbAutN9TxeqaeZ9Ds2LmD1tfm+N+eOTZJ69ksz4FpaWql9ebmNlqfmOA5UOk6fv8l51klLQ3r+fIpkhM1MUGXPV6M3m4AwFZG7zMAMDDKt/uRUf6355y/jup+2fm0ftElp9P6j374bVqvJPWvE+L6V2/f0cia+pf6V5RK96+hmJy6MU/Teuc559J6XP+69dbo/jWXI1CTAF7t7hcD2ATgWjP7DQAfA3Cnu58N4M7w/yIitUT9S0QqInYC5YHR8L/Z8J8DeAuAW8LLbwFwXSUGKCIyX+pfIlIpczoHyszSZrYNQB+AO9z9QQCr3L0HAMKfKys2ShGReVL/EpFKmNMEyt1L7r4JwHoAl5vZRXO9AzO7wcy2mtnW8dEXyXdmiUjNUP8SkUpI9Ck8dx8EcDeAawH0mtkaAAh/zvpNmu5+k7tvdvfNTS0xXygrIlIh6l8isphiJ1BmtsLM2sPfGwG8FsAOALcDuD682vUAbqvQGEVE5kX9S0QqJTbGAMAaALeYWRrBhOs77v4jM/sVgO+Y2fsA7AfwzgqOU0RkPtS/RKQiYidQ7r4dwCWzXN4P4DVJ7mxysoBdu3oi6waeddHawuvDx/kBtZERfg7DBRetpfXu07to/eDhvZG11tYOuqwXnNabmnmOSX1Mzkr3aTzHpbOzgdZzuXFaHxwcovWh4/yxS3W207oXeNZHKsXHPzR2jNbzpTFaHxyKzsgBgGVjTZG1+picpVyK33d9HV9+aISv27ExvnzbOv7WVMMKvu5LLTzDpprUv06o5f6VTfOMLPWvU7d/5Sb5tjN4nE9TOs+up/VK9i8lkYuIiIgkpAmUiIiISEKaQImIiIgkpAmUiIiISEKaQImIiIgkpAmUiIiISEKaQImIiIgkZO48g2FR78zsKIB90y5aDoAHXFRXLY+vlscG1Pb4anlswKk3vtPdfUWlBrNU1L8WXS2Pr5bHBtT2+Gp5bMAi9q8lnUC94M7Ntrr75qoNIEYtj6+WxwbU9vhqeWyAxneyqPX1oPHNXy2PDajt8dXy2IDFHZ/ewhMRERFJSBMoERERkYSqPYG6qcr3H6eWx1fLYwNqe3y1PDZA4ztZ1Pp60Pjmr5bHBtT2+Gp5bMAijq+q50CJiIiInIyqfQRKRERE5KRTlQmUmV1rZs+Y2U4z+1g1xsCY2V4ze9zMtpnZ1hoYz9fMrM/Mnph2WaeZ3WFmz4U/O2psfDea2aFwHW4zszdWaWwbzOwuM3vazJ40sw+Hl1d9/ZGx1cq6azCzh8zssXB8nwovr/q6qzb1sERjUf+a/9hqtn/FjK/q628p+teSv4VnZmkAzwJ4HYCDALYAeLe7P7WkAyHMbC+Aze5eE1kWZnY1gFEAX3f3i8LLPgNgwN0/HTbwDnf/DzU0vhsBjLr7Z6sxpmljWwNgjbs/YmatAB4GcB2A96LK64+M7V2ojXVnAJrdfdTMsgDuBfBhAG9DjWx71aAelngs6l/zH1vN9q+Y8VW9hy1F/6rGEajLAex0993ungfwLQBvqcI4Thrufg+AgRkXvwXALeHvtyDYaKsiYnw1wd173P2R8PcRAE8DWIcaWH9kbDXBA6Phf7PhP0cNrLsqUw9LQP1r/mq5f8WMr+qWon9VYwK1DsCBaf8/iBpZ4dM4gJ+Z2cNmdkO1BxNhlbv3AMFGDGBllcczmw+Z2fbwEHnV3+Yxs24AlwB4EDW2/maMDaiRdWdmaTPbBqAPwB3uXnPrrgrUwxbuZNiGamIfnFLL/QuozR5W6f5VjQmUzXJZrX0U8Ep3fxmA3wLwR+EhXknmywA2AtgEoAfA56o5GDNrAXArgI+4+3A1xzLTLGOrmXXn7iV33wRgPYDLzeyiao2lhqiHnfpqZh8Eart/AbXbwyrdv6oxgToIYMO0/68HcLgK44jk7ofDn30Avo/gkH2t6Q3ff556H7qvyuN5HnfvDTfeMoCvoIrrMHz/+1YA33D374UX18T6m21stbTuprj7IIC7AVyLGll3VaQetnA1vQ3V0j5Yy/0rany1tP7C8QyiAv2rGhOoLQDONrMzzKwOwO8AuL0K45iVmTWHJ8PBzJoBvB7AE3ypqrgdwPXh79cDuK2KY3mBqQ009FZUaR2GJxJ+FcDT7v75aaWqr7+osdXQulthZu3h740AXgtgB2pg3VWZetjC1fQ2VEP7YM32L6C2e9iS9C93X/J/AN6I4FMsuwD8eTXGQMZ2JoDHwn9P1sL4AHwTwWHQAoJXv+8D0AXgTgDPhT87a2x8fw/gcQDbww12TZXGdhWCt1e2A9gW/ntjLaw/MrZaWXcvBfBoOI4nAHwivLzq667a/9TDEo1H/Wv+Y6vZ/hUzvqqvv6XoX0oiFxEREUlISeQiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpKQJlAiIiIiCWkCJSIiIpLQ/wVG8/TjqvOLsQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show images. \n",
    "fig = plt.figure(figsize = (10,20))\n",
    "columns = 2\n",
    "rows = 1\n",
    "fig.add_subplot(rows,columns, 1)\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('Original LDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 2)\n",
    "plt.imshow(x_dataAug[0])\n",
    "plt.title('Blended 1X HDR', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_dataAug)\n",
    "len(y_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_11 (Rescaling)       (None, 32, 32, 3)    0           ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_11[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 28, 28, 32)  128         ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_94 (Separable  (None, 28, 28, 32)  1312        ['activation_94[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 28, 28, 32)  128         ['separable_conv2d_94[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_95 (Separable  (None, 28, 28, 32)  1312        ['activation_95[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_53 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_95[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_66[0][0]']              \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_53[0][0]',       \n",
      "                                                                  'conv2d_67[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 14, 14, 32)  128         ['add_47[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 14, 14, 32)   0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_96 (Separable  (None, 14, 14, 64)  2336        ['activation_96[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 14, 14, 64)  256         ['separable_conv2d_96[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 14, 14, 64)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_97 (Separable  (None, 14, 14, 64)  4672        ['activation_97[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_54 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_97[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 7, 7, 64)     2048        ['add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_54[0][0]',       \n",
      "                                                                  'conv2d_68[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 7, 7, 64)    256         ['add_48[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 7, 7, 64)     0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_98 (Separable  (None, 7, 7, 128)   8768        ['activation_98[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 7, 7, 128)   512         ['separable_conv2d_98[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_99 (Separable  (None, 7, 7, 128)   17536       ['activation_99[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_55 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_99[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 4, 4, 128)    8192        ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_55[0][0]',       \n",
      "                                                                  'conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 4, 4, 128)   512         ['add_49[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_100 (Separabl  (None, 4, 4, 256)   33920       ['activation_100[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_100[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_101 (Separabl  (None, 4, 4, 256)   67840       ['activation_101[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_56 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_101[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 2, 2, 256)    32768       ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_56[0][0]',       \n",
      "                                                                  'conv2d_70[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 1024)         0           ['add_50[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 1024)         0           ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 256)          262400      ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 256)          0           ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 128)          32896       ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 128)          0           ['dense_40[0][0]']               \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 100)          12900       ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 4.2082 - accuracy: 0.0627 - val_loss: 3.8395 - val_accuracy: 0.1264\n",
      "Epoch 2/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8474 - accuracy: 0.1094 - val_loss: 3.5755 - val_accuracy: 0.1692\n",
      "Epoch 3/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.6768 - accuracy: 0.1318 - val_loss: 3.4109 - val_accuracy: 0.1936\n",
      "Epoch 4/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.5466 - accuracy: 0.1536 - val_loss: 3.3862 - val_accuracy: 0.1891\n",
      "Epoch 5/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4371 - accuracy: 0.1708 - val_loss: 3.2195 - val_accuracy: 0.2220\n",
      "Epoch 6/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.3379 - accuracy: 0.1884 - val_loss: 3.1177 - val_accuracy: 0.2392\n",
      "Epoch 7/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.2639 - accuracy: 0.1992 - val_loss: 3.0523 - val_accuracy: 0.2516\n",
      "Epoch 8/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1854 - accuracy: 0.2134 - val_loss: 3.0137 - val_accuracy: 0.2598\n",
      "Epoch 9/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1174 - accuracy: 0.2276 - val_loss: 2.9720 - val_accuracy: 0.2618\n",
      "Epoch 10/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.0528 - accuracy: 0.2393 - val_loss: 2.9739 - val_accuracy: 0.2653\n",
      "Epoch 11/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0006 - accuracy: 0.2461 - val_loss: 2.8734 - val_accuracy: 0.2854\n",
      "Epoch 12/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9455 - accuracy: 0.2588 - val_loss: 2.7946 - val_accuracy: 0.3011\n",
      "Epoch 13/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8973 - accuracy: 0.2662 - val_loss: 2.8099 - val_accuracy: 0.2972\n",
      "Epoch 14/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8425 - accuracy: 0.2776 - val_loss: 2.7715 - val_accuracy: 0.3092\n",
      "Epoch 15/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.8022 - accuracy: 0.2855 - val_loss: 2.7406 - val_accuracy: 0.3136\n",
      "Epoch 16/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7571 - accuracy: 0.2923 - val_loss: 2.7117 - val_accuracy: 0.3187\n",
      "Epoch 17/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7139 - accuracy: 0.3024 - val_loss: 2.7115 - val_accuracy: 0.3129\n",
      "Epoch 18/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6770 - accuracy: 0.3096 - val_loss: 2.7782 - val_accuracy: 0.3057\n",
      "Epoch 19/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6398 - accuracy: 0.3145 - val_loss: 2.6224 - val_accuracy: 0.3331\n",
      "Epoch 20/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5982 - accuracy: 0.3263 - val_loss: 2.6036 - val_accuracy: 0.3355\n",
      "Epoch 21/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5650 - accuracy: 0.3332 - val_loss: 2.6115 - val_accuracy: 0.3330\n",
      "Epoch 22/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5301 - accuracy: 0.3380 - val_loss: 2.6342 - val_accuracy: 0.3348\n",
      "Epoch 23/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5100 - accuracy: 0.3425 - val_loss: 2.5600 - val_accuracy: 0.3490\n",
      "Epoch 24/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4673 - accuracy: 0.3515 - val_loss: 2.5738 - val_accuracy: 0.3411\n",
      "Epoch 25/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4359 - accuracy: 0.3581 - val_loss: 2.6310 - val_accuracy: 0.3395\n",
      "Epoch 26/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.4087 - accuracy: 0.3648 - val_loss: 2.5100 - val_accuracy: 0.3574\n",
      "Epoch 27/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3822 - accuracy: 0.3674 - val_loss: 2.5120 - val_accuracy: 0.3561\n",
      "Epoch 28/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3562 - accuracy: 0.3744 - val_loss: 2.5472 - val_accuracy: 0.3506\n",
      "Epoch 29/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3281 - accuracy: 0.3818 - val_loss: 2.4880 - val_accuracy: 0.3649\n",
      "Epoch 30/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3109 - accuracy: 0.3854 - val_loss: 2.5420 - val_accuracy: 0.3541\n",
      "Epoch 31/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2807 - accuracy: 0.3905 - val_loss: 2.5430 - val_accuracy: 0.3531\n",
      "Epoch 32/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2625 - accuracy: 0.3936 - val_loss: 2.5454 - val_accuracy: 0.3549\n",
      "Epoch 33/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2265 - accuracy: 0.4011 - val_loss: 2.5214 - val_accuracy: 0.3590\n",
      "Epoch 34/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2120 - accuracy: 0.4053 - val_loss: 2.4766 - val_accuracy: 0.3665\n",
      "Epoch 35/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1836 - accuracy: 0.4097 - val_loss: 2.4540 - val_accuracy: 0.3711\n",
      "Epoch 36/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1614 - accuracy: 0.4186 - val_loss: 2.4609 - val_accuracy: 0.3716\n",
      "Epoch 37/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1459 - accuracy: 0.4194 - val_loss: 2.4755 - val_accuracy: 0.3700\n",
      "Epoch 38/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1266 - accuracy: 0.4216 - val_loss: 2.4296 - val_accuracy: 0.3782\n",
      "Epoch 39/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1064 - accuracy: 0.4280 - val_loss: 2.4599 - val_accuracy: 0.3780\n",
      "Epoch 40/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0744 - accuracy: 0.4333 - val_loss: 2.5178 - val_accuracy: 0.3683\n",
      "Epoch 41/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0614 - accuracy: 0.4361 - val_loss: 2.5181 - val_accuracy: 0.3666\n",
      "Epoch 42/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0507 - accuracy: 0.4398 - val_loss: 2.4428 - val_accuracy: 0.3808\n",
      "Epoch 43/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0240 - accuracy: 0.4449 - val_loss: 2.4573 - val_accuracy: 0.3803\n",
      "Epoch 44/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0049 - accuracy: 0.4504 - val_loss: 2.4420 - val_accuracy: 0.3849\n",
      "Epoch 45/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9830 - accuracy: 0.4527 - val_loss: 2.4497 - val_accuracy: 0.3864\n",
      "Epoch 46/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9681 - accuracy: 0.4592 - val_loss: 2.5216 - val_accuracy: 0.3754\n",
      "Epoch 47/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9561 - accuracy: 0.4581 - val_loss: 2.4494 - val_accuracy: 0.3832\n",
      "Epoch 48/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9308 - accuracy: 0.4645 - val_loss: 2.4149 - val_accuracy: 0.3889\n",
      "Epoch 49/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9137 - accuracy: 0.4695 - val_loss: 2.4589 - val_accuracy: 0.3882\n",
      "Epoch 50/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9029 - accuracy: 0.4708 - val_loss: 2.4243 - val_accuracy: 0.3877\n",
      "Epoch 51/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8887 - accuracy: 0.4724 - val_loss: 2.4418 - val_accuracy: 0.3871\n",
      "Epoch 52/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8696 - accuracy: 0.4757 - val_loss: 2.4872 - val_accuracy: 0.3801\n",
      "Epoch 53/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8617 - accuracy: 0.4803 - val_loss: 2.5149 - val_accuracy: 0.3845\n",
      "Epoch 54/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8404 - accuracy: 0.4843 - val_loss: 2.4265 - val_accuracy: 0.3935\n",
      "Epoch 55/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8216 - accuracy: 0.4894 - val_loss: 2.4659 - val_accuracy: 0.3871\n",
      "Epoch 56/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8074 - accuracy: 0.4928 - val_loss: 2.5037 - val_accuracy: 0.3873\n",
      "Epoch 57/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7888 - accuracy: 0.4972 - val_loss: 2.5193 - val_accuracy: 0.3849\n",
      "Epoch 58/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7806 - accuracy: 0.4985 - val_loss: 2.4920 - val_accuracy: 0.3875\n",
      "Epoch 59/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7676 - accuracy: 0.5018 - val_loss: 2.5876 - val_accuracy: 0.3736\n",
      "Epoch 60/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7482 - accuracy: 0.5050 - val_loss: 2.4927 - val_accuracy: 0.3938\n",
      "Epoch 61/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7399 - accuracy: 0.5074 - val_loss: 2.4636 - val_accuracy: 0.3938\n",
      "Epoch 62/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7220 - accuracy: 0.5109 - val_loss: 2.4884 - val_accuracy: 0.3933\n",
      "Epoch 63/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7201 - accuracy: 0.5129 - val_loss: 2.4935 - val_accuracy: 0.3880\n",
      "Epoch 64/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6990 - accuracy: 0.5155 - val_loss: 2.5346 - val_accuracy: 0.3882\n",
      "Epoch 65/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6925 - accuracy: 0.5164 - val_loss: 2.6273 - val_accuracy: 0.3785\n",
      "Epoch 66/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6802 - accuracy: 0.5213 - val_loss: 2.5438 - val_accuracy: 0.3880\n",
      "Epoch 67/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6601 - accuracy: 0.5238 - val_loss: 2.5107 - val_accuracy: 0.3983\n",
      "Epoch 68/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6544 - accuracy: 0.5264 - val_loss: 2.5242 - val_accuracy: 0.3919\n",
      "Epoch 69/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6355 - accuracy: 0.5307 - val_loss: 2.4882 - val_accuracy: 0.3944\n",
      "Epoch 70/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6304 - accuracy: 0.5323 - val_loss: 2.6037 - val_accuracy: 0.3867\n",
      "Epoch 71/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6190 - accuracy: 0.5338 - val_loss: 2.5296 - val_accuracy: 0.3906\n",
      "Epoch 72/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6063 - accuracy: 0.5379 - val_loss: 2.5476 - val_accuracy: 0.3943\n",
      "Epoch 73/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5856 - accuracy: 0.5402 - val_loss: 2.6705 - val_accuracy: 0.3784\n",
      "Epoch 74/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5791 - accuracy: 0.5450 - val_loss: 2.6172 - val_accuracy: 0.3872\n",
      "Epoch 75/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5686 - accuracy: 0.5462 - val_loss: 2.5821 - val_accuracy: 0.3908\n",
      "Epoch 76/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5634 - accuracy: 0.5474 - val_loss: 2.5443 - val_accuracy: 0.3942\n",
      "Epoch 77/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5522 - accuracy: 0.5491 - val_loss: 2.5200 - val_accuracy: 0.3966\n",
      "Epoch 78/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5453 - accuracy: 0.5530 - val_loss: 2.6209 - val_accuracy: 0.3884\n",
      "Epoch 79/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5338 - accuracy: 0.5553 - val_loss: 2.7477 - val_accuracy: 0.3773\n",
      "Epoch 80/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5320 - accuracy: 0.5546 - val_loss: 2.5715 - val_accuracy: 0.3940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_dataAug, y_dataAug,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Validation Accuracy: 39.44%.  One pass through HDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = len(x_dataAug)\n",
    "\n",
    "x_dataAug_2 = []\n",
    "y_dataAug_2 = []\n",
    "for image in range(0,end):\n",
    "    x_dataAug_2.append(hdr(x_dataAug[image]))\n",
    "    y_dataAug_2.append(y_train[image])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataAug_2 = np.asarray(y_dataAug)\n",
    "x_dataAug_2 = np.asarray(x_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show images. \n",
    "fig = plt.figure(figsize = (10,30))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows,columns, 1)\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('Original LDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 2)\n",
    "plt.imshow(x_dataAug[0])\n",
    "plt.title('Blended 1X HDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 3)\n",
    "plt.imshow(x_dataAug_2[0])\n",
    "plt.title('Blended 2X HDR', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_12 (Rescaling)       (None, 32, 32, 3)    0           ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_12[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 28, 28, 32)  128         ['conv2d_71[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_102 (Separabl  (None, 28, 28, 32)  1312        ['activation_102[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_102[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_103 (Separabl  (None, 28, 28, 32)  1312        ['activation_103[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_57 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_103[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_71[0][0]']              \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_57[0][0]',       \n",
      "                                                                  'conv2d_72[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 14, 14, 32)  128         ['add_51[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_104 (Separabl  (None, 14, 14, 64)  2336        ['activation_104[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_104[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_105 (Separabl  (None, 14, 14, 64)  4672        ['activation_105[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_58 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_105[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 7, 7, 64)     2048        ['add_51[0][0]']                 \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_58[0][0]',       \n",
      "                                                                  'conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 7, 7, 64)    256         ['add_52[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_106 (Separabl  (None, 7, 7, 128)   8768        ['activation_106[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_106[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_107 (Separabl  (None, 7, 7, 128)   17536       ['activation_107[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_59 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_107[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 4, 4, 128)    8192        ['add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_59[0][0]',       \n",
      "                                                                  'conv2d_74[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 4, 4, 128)   512         ['add_53[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_108 (Separabl  (None, 4, 4, 256)   33920       ['activation_108[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_108[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_109 (Separabl  (None, 4, 4, 256)   67840       ['activation_109[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_60 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_109[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 2, 2, 256)    32768       ['add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_60[0][0]',       \n",
      "                                                                  'conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 1024)         0           ['add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 1024)         0           ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 256)          262400      ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 256)          0           ['dense_42[0][0]']               \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 128)          32896       ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 128)          0           ['dense_43[0][0]']               \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 100)          12900       ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation2X.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 4.2247 - accuracy: 0.0595 - val_loss: 3.8486 - val_accuracy: 0.1183\n",
      "Epoch 2/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8677 - accuracy: 0.1067 - val_loss: 3.5953 - val_accuracy: 0.1564\n",
      "Epoch 3/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.6994 - accuracy: 0.1284 - val_loss: 3.4667 - val_accuracy: 0.1783\n",
      "Epoch 4/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.5652 - accuracy: 0.1497 - val_loss: 3.3374 - val_accuracy: 0.2012\n",
      "Epoch 5/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4604 - accuracy: 0.1685 - val_loss: 3.2483 - val_accuracy: 0.2162\n",
      "Epoch 6/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.3706 - accuracy: 0.1822 - val_loss: 3.1881 - val_accuracy: 0.2296\n",
      "Epoch 7/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2939 - accuracy: 0.1975 - val_loss: 3.1154 - val_accuracy: 0.2413\n",
      "Epoch 8/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2174 - accuracy: 0.2113 - val_loss: 3.0457 - val_accuracy: 0.2506\n",
      "Epoch 9/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 3.1498 - accuracy: 0.2226 - val_loss: 2.9857 - val_accuracy: 0.2629\n",
      "Epoch 10/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0886 - accuracy: 0.2317 - val_loss: 2.9425 - val_accuracy: 0.2745\n",
      "Epoch 11/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0336 - accuracy: 0.2448 - val_loss: 2.9400 - val_accuracy: 0.2742\n",
      "Epoch 12/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9753 - accuracy: 0.2556 - val_loss: 2.8544 - val_accuracy: 0.2860\n",
      "Epoch 13/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.9212 - accuracy: 0.2676 - val_loss: 2.8242 - val_accuracy: 0.2933\n",
      "Epoch 14/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.8757 - accuracy: 0.2743 - val_loss: 2.7722 - val_accuracy: 0.3031\n",
      "Epoch 15/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8274 - accuracy: 0.2852 - val_loss: 2.7362 - val_accuracy: 0.3123\n",
      "Epoch 16/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.7767 - accuracy: 0.2944 - val_loss: 2.7310 - val_accuracy: 0.3106\n",
      "Epoch 17/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.7319 - accuracy: 0.3034 - val_loss: 2.6945 - val_accuracy: 0.3222\n",
      "Epoch 18/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6992 - accuracy: 0.3068 - val_loss: 2.7017 - val_accuracy: 0.3172\n",
      "Epoch 19/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6461 - accuracy: 0.3187 - val_loss: 2.6456 - val_accuracy: 0.3319\n",
      "Epoch 20/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6199 - accuracy: 0.3246 - val_loss: 2.6188 - val_accuracy: 0.3380\n",
      "Epoch 21/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5730 - accuracy: 0.3342 - val_loss: 2.5853 - val_accuracy: 0.3379\n",
      "Epoch 22/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5438 - accuracy: 0.3384 - val_loss: 2.5872 - val_accuracy: 0.3446\n",
      "Epoch 23/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5085 - accuracy: 0.3473 - val_loss: 2.6299 - val_accuracy: 0.3336\n",
      "Epoch 24/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4778 - accuracy: 0.3531 - val_loss: 2.5674 - val_accuracy: 0.3445\n",
      "Epoch 25/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4500 - accuracy: 0.3569 - val_loss: 2.5702 - val_accuracy: 0.3489\n",
      "Epoch 26/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4137 - accuracy: 0.3660 - val_loss: 2.5562 - val_accuracy: 0.3489\n",
      "Epoch 27/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.3887 - accuracy: 0.3694 - val_loss: 2.5075 - val_accuracy: 0.3607\n",
      "Epoch 28/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3549 - accuracy: 0.3755 - val_loss: 2.5117 - val_accuracy: 0.3568\n",
      "Epoch 29/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.3362 - accuracy: 0.3802 - val_loss: 2.5397 - val_accuracy: 0.3555\n",
      "Epoch 30/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3101 - accuracy: 0.3890 - val_loss: 2.5367 - val_accuracy: 0.3548\n",
      "Epoch 31/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2849 - accuracy: 0.3918 - val_loss: 2.4647 - val_accuracy: 0.3657\n",
      "Epoch 32/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2565 - accuracy: 0.3977 - val_loss: 2.4813 - val_accuracy: 0.3671\n",
      "Epoch 33/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2366 - accuracy: 0.4032 - val_loss: 2.4656 - val_accuracy: 0.3687\n",
      "Epoch 34/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2070 - accuracy: 0.4063 - val_loss: 2.4790 - val_accuracy: 0.3665\n",
      "Epoch 35/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1850 - accuracy: 0.4098 - val_loss: 2.4752 - val_accuracy: 0.3670\n",
      "Epoch 36/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1612 - accuracy: 0.4166 - val_loss: 2.4698 - val_accuracy: 0.3713\n",
      "Epoch 37/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1325 - accuracy: 0.4237 - val_loss: 2.5279 - val_accuracy: 0.3616\n",
      "Epoch 38/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1172 - accuracy: 0.4260 - val_loss: 2.4594 - val_accuracy: 0.3750\n",
      "Epoch 39/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0955 - accuracy: 0.4326 - val_loss: 2.4229 - val_accuracy: 0.3812\n",
      "Epoch 40/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0747 - accuracy: 0.4345 - val_loss: 2.4836 - val_accuracy: 0.3713\n",
      "Epoch 41/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0546 - accuracy: 0.4375 - val_loss: 2.4349 - val_accuracy: 0.3799\n",
      "Epoch 42/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0293 - accuracy: 0.4435 - val_loss: 2.4421 - val_accuracy: 0.3789\n",
      "Epoch 43/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0155 - accuracy: 0.4454 - val_loss: 2.4661 - val_accuracy: 0.3810\n",
      "Epoch 44/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9960 - accuracy: 0.4507 - val_loss: 2.4396 - val_accuracy: 0.3791\n",
      "Epoch 45/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9819 - accuracy: 0.4538 - val_loss: 2.4243 - val_accuracy: 0.3833\n",
      "Epoch 46/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9566 - accuracy: 0.4601 - val_loss: 2.4996 - val_accuracy: 0.3685\n",
      "Epoch 47/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9475 - accuracy: 0.4631 - val_loss: 2.4509 - val_accuracy: 0.3826\n",
      "Epoch 48/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9249 - accuracy: 0.4661 - val_loss: 2.4396 - val_accuracy: 0.3875\n",
      "Epoch 49/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9025 - accuracy: 0.4717 - val_loss: 2.4525 - val_accuracy: 0.3817\n",
      "Epoch 50/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8923 - accuracy: 0.4733 - val_loss: 2.4356 - val_accuracy: 0.3849\n",
      "Epoch 51/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8704 - accuracy: 0.4772 - val_loss: 2.4103 - val_accuracy: 0.3900\n",
      "Epoch 52/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8513 - accuracy: 0.4820 - val_loss: 2.5502 - val_accuracy: 0.3661\n",
      "Epoch 53/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8407 - accuracy: 0.4856 - val_loss: 2.4513 - val_accuracy: 0.3845\n",
      "Epoch 54/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8270 - accuracy: 0.4898 - val_loss: 2.4654 - val_accuracy: 0.3809\n",
      "Epoch 55/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8040 - accuracy: 0.4942 - val_loss: 2.5131 - val_accuracy: 0.3797\n",
      "Epoch 56/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7942 - accuracy: 0.4941 - val_loss: 2.4559 - val_accuracy: 0.3875\n",
      "Epoch 57/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7796 - accuracy: 0.4978 - val_loss: 2.4625 - val_accuracy: 0.3869\n",
      "Epoch 58/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7610 - accuracy: 0.5041 - val_loss: 2.4859 - val_accuracy: 0.3892\n",
      "Epoch 59/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7518 - accuracy: 0.5060 - val_loss: 2.4834 - val_accuracy: 0.3852\n",
      "Epoch 60/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7363 - accuracy: 0.5084 - val_loss: 2.5227 - val_accuracy: 0.3805\n",
      "Epoch 61/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7188 - accuracy: 0.5134 - val_loss: 2.4622 - val_accuracy: 0.3926\n",
      "Epoch 62/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7117 - accuracy: 0.5128 - val_loss: 2.5279 - val_accuracy: 0.3861\n",
      "Epoch 63/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6912 - accuracy: 0.5195 - val_loss: 2.4755 - val_accuracy: 0.3913\n",
      "Epoch 64/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6797 - accuracy: 0.5205 - val_loss: 2.5624 - val_accuracy: 0.3827\n",
      "Epoch 65/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6647 - accuracy: 0.5250 - val_loss: 2.5186 - val_accuracy: 0.3888\n",
      "Epoch 66/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6521 - accuracy: 0.5276 - val_loss: 2.4817 - val_accuracy: 0.3900\n",
      "Epoch 67/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6394 - accuracy: 0.5296 - val_loss: 2.4883 - val_accuracy: 0.3929\n",
      "Epoch 68/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6355 - accuracy: 0.5306 - val_loss: 2.4824 - val_accuracy: 0.3919\n",
      "Epoch 69/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6124 - accuracy: 0.5364 - val_loss: 2.5112 - val_accuracy: 0.3883\n",
      "Epoch 70/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6038 - accuracy: 0.5364 - val_loss: 2.5568 - val_accuracy: 0.3815\n",
      "Epoch 71/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5936 - accuracy: 0.5399 - val_loss: 2.5420 - val_accuracy: 0.3829\n",
      "Epoch 72/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5799 - accuracy: 0.5440 - val_loss: 2.5629 - val_accuracy: 0.3892\n",
      "Epoch 73/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5688 - accuracy: 0.5451 - val_loss: 2.5225 - val_accuracy: 0.3918\n",
      "Epoch 74/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5554 - accuracy: 0.5511 - val_loss: 2.5116 - val_accuracy: 0.3924\n",
      "Epoch 75/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5486 - accuracy: 0.5520 - val_loss: 2.5232 - val_accuracy: 0.3923\n",
      "Epoch 76/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5369 - accuracy: 0.5546 - val_loss: 2.5569 - val_accuracy: 0.3918\n",
      "Epoch 77/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5216 - accuracy: 0.5581 - val_loss: 2.5748 - val_accuracy: 0.3835\n",
      "Epoch 78/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5205 - accuracy: 0.5587 - val_loss: 2.5618 - val_accuracy: 0.3940\n",
      "Epoch 79/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5035 - accuracy: 0.5604 - val_loss: 2.6117 - val_accuracy: 0.3826\n",
      "Epoch 80/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.4956 - accuracy: 0.5644 - val_loss: 2.5604 - val_accuracy: 0.3924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_dataAug_2, y_dataAug_2,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Validation Accuracy: 39.40%.  Two passes through HDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153600000\n",
      "(50000, 32, 32, 3)\n",
      "uint8\n",
      "153600000\n",
      "(50000, 32, 32, 3)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "#How many elements are in that data. \n",
    "print(x_train.size)\n",
    "\n",
    "#What is the shape of the array.\n",
    "print(x_train.shape)\n",
    "\n",
    "#What type are the elements.\n",
    "print(x_train.dtype)\n",
    "\n",
    "#How many elements are in that data. \n",
    "print(x_dataAug_2.size)\n",
    "\n",
    "#What is the shape of the array.\n",
    "print(x_dataAug_2.shape)\n",
    "\n",
    "#What type are the elements.\n",
    "print(x_dataAug_2.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.concatenate((x_train, x_dataAug_2))\n",
    "y_full = np.concatenate((y_train, y_dataAug_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_13 (Rescaling)       (None, 32, 32, 3)    0           ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_13[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 28, 28, 32)  128         ['conv2d_76[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_110 (Separabl  (None, 28, 28, 32)  1312        ['activation_110[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_110[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_111 (Separabl  (None, 28, 28, 32)  1312        ['activation_111[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_61 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_111[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_61[0][0]',       \n",
      "                                                                  'conv2d_77[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 14, 14, 32)  128         ['add_55[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_112 (Separabl  (None, 14, 14, 64)  2336        ['activation_112[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_112[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_113 (Separabl  (None, 14, 14, 64)  4672        ['activation_113[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_62 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_113[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 7, 7, 64)     2048        ['add_55[0][0]']                 \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_62[0][0]',       \n",
      "                                                                  'conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 7, 7, 64)    256         ['add_56[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_114 (Separabl  (None, 7, 7, 128)   8768        ['activation_114[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_114[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_115 (Separabl  (None, 7, 7, 128)   17536       ['activation_115[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_63 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_115[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 4, 4, 128)    8192        ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_63[0][0]',       \n",
      "                                                                  'conv2d_79[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 4, 4, 128)   512         ['add_57[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_116 (Separabl  (None, 4, 4, 256)   33920       ['activation_116[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_116[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_117 (Separabl  (None, 4, 4, 256)   67840       ['activation_117[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_64 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_117[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 2, 2, 256)    32768       ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_64[0][0]',       \n",
      "                                                                  'conv2d_80[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 1024)         0           ['add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 1024)         0           ['flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 256)          262400      ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 256)          0           ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 128)          32896       ['dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 128)          0           ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 100)          12900       ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation_fullDS.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 4.0324 - accuracy: 0.0831 - val_loss: 3.5463 - val_accuracy: 0.1706\n",
      "Epoch 2/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.5902 - accuracy: 0.1485 - val_loss: 3.2915 - val_accuracy: 0.2133\n",
      "Epoch 3/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.3778 - accuracy: 0.1827 - val_loss: 3.1721 - val_accuracy: 0.2281\n",
      "Epoch 4/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 3.2148 - accuracy: 0.2103 - val_loss: 3.0150 - val_accuracy: 0.2572\n",
      "Epoch 5/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.0813 - accuracy: 0.2361 - val_loss: 2.8813 - val_accuracy: 0.2815\n",
      "Epoch 6/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.9695 - accuracy: 0.2553 - val_loss: 2.9046 - val_accuracy: 0.2829\n",
      "Epoch 7/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.8628 - accuracy: 0.2745 - val_loss: 2.7153 - val_accuracy: 0.3109\n",
      "Epoch 8/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.7703 - accuracy: 0.2939 - val_loss: 2.6935 - val_accuracy: 0.3197\n",
      "Epoch 9/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6842 - accuracy: 0.3082 - val_loss: 2.6688 - val_accuracy: 0.3184\n",
      "Epoch 10/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6037 - accuracy: 0.3252 - val_loss: 2.5660 - val_accuracy: 0.3429\n",
      "Epoch 11/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5442 - accuracy: 0.3389 - val_loss: 2.5344 - val_accuracy: 0.3462\n",
      "Epoch 12/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4729 - accuracy: 0.3530 - val_loss: 2.4784 - val_accuracy: 0.3569\n",
      "Epoch 13/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4131 - accuracy: 0.3643 - val_loss: 2.5013 - val_accuracy: 0.3546\n",
      "Epoch 14/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3584 - accuracy: 0.3753 - val_loss: 2.4198 - val_accuracy: 0.3719\n",
      "Epoch 15/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3025 - accuracy: 0.3876 - val_loss: 2.4609 - val_accuracy: 0.3651\n",
      "Epoch 16/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2558 - accuracy: 0.3954 - val_loss: 2.3923 - val_accuracy: 0.3792\n",
      "Epoch 17/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2060 - accuracy: 0.4069 - val_loss: 2.4002 - val_accuracy: 0.3843\n",
      "Epoch 18/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.1653 - accuracy: 0.4162 - val_loss: 2.3571 - val_accuracy: 0.3901\n",
      "Epoch 19/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1244 - accuracy: 0.4246 - val_loss: 2.3570 - val_accuracy: 0.3918\n",
      "Epoch 20/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0761 - accuracy: 0.4347 - val_loss: 2.3977 - val_accuracy: 0.3865\n",
      "Epoch 21/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0392 - accuracy: 0.4422 - val_loss: 2.3558 - val_accuracy: 0.3953\n",
      "Epoch 22/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0079 - accuracy: 0.4489 - val_loss: 2.3444 - val_accuracy: 0.3962\n",
      "Epoch 23/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9649 - accuracy: 0.4586 - val_loss: 2.3414 - val_accuracy: 0.4006\n",
      "Epoch 24/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9331 - accuracy: 0.4647 - val_loss: 2.3326 - val_accuracy: 0.4031\n",
      "Epoch 25/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8961 - accuracy: 0.4743 - val_loss: 2.3473 - val_accuracy: 0.4065\n",
      "Epoch 26/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8653 - accuracy: 0.4801 - val_loss: 2.4232 - val_accuracy: 0.3916\n",
      "Epoch 27/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8357 - accuracy: 0.4859 - val_loss: 2.3478 - val_accuracy: 0.4097\n",
      "Epoch 28/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8089 - accuracy: 0.4928 - val_loss: 2.3501 - val_accuracy: 0.4098\n",
      "Epoch 29/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7705 - accuracy: 0.5005 - val_loss: 2.3518 - val_accuracy: 0.4103\n",
      "Epoch 30/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7418 - accuracy: 0.5073 - val_loss: 2.3688 - val_accuracy: 0.4073\n",
      "Epoch 31/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7255 - accuracy: 0.5114 - val_loss: 2.3455 - val_accuracy: 0.4146\n",
      "Epoch 32/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6988 - accuracy: 0.5167 - val_loss: 2.3800 - val_accuracy: 0.4075\n",
      "Epoch 33/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6726 - accuracy: 0.5208 - val_loss: 2.3753 - val_accuracy: 0.4128\n",
      "Epoch 34/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.6489 - accuracy: 0.5276 - val_loss: 2.3637 - val_accuracy: 0.4163\n",
      "Epoch 35/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6237 - accuracy: 0.5332 - val_loss: 2.3854 - val_accuracy: 0.4050\n",
      "Epoch 36/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6031 - accuracy: 0.5377 - val_loss: 2.4133 - val_accuracy: 0.4129\n",
      "Epoch 37/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5800 - accuracy: 0.5432 - val_loss: 2.4024 - val_accuracy: 0.4139\n",
      "Epoch 38/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5583 - accuracy: 0.5482 - val_loss: 2.4374 - val_accuracy: 0.4084\n",
      "Epoch 39/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5370 - accuracy: 0.5542 - val_loss: 2.4171 - val_accuracy: 0.4115\n",
      "Epoch 40/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5175 - accuracy: 0.5589 - val_loss: 2.4368 - val_accuracy: 0.4098\n",
      "Epoch 41/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4990 - accuracy: 0.5635 - val_loss: 2.4166 - val_accuracy: 0.4116\n",
      "Epoch 42/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4772 - accuracy: 0.5664 - val_loss: 2.4348 - val_accuracy: 0.4176\n",
      "Epoch 43/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4626 - accuracy: 0.5727 - val_loss: 2.4467 - val_accuracy: 0.4130\n",
      "Epoch 44/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4405 - accuracy: 0.5764 - val_loss: 2.4600 - val_accuracy: 0.4132\n",
      "Epoch 45/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4283 - accuracy: 0.5804 - val_loss: 2.4548 - val_accuracy: 0.4159\n",
      "Epoch 46/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4098 - accuracy: 0.5827 - val_loss: 2.4731 - val_accuracy: 0.4115\n",
      "Epoch 47/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3963 - accuracy: 0.5873 - val_loss: 2.4769 - val_accuracy: 0.4156\n",
      "Epoch 48/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3801 - accuracy: 0.5924 - val_loss: 2.4790 - val_accuracy: 0.4147\n",
      "Epoch 49/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3613 - accuracy: 0.5961 - val_loss: 2.5213 - val_accuracy: 0.4157\n",
      "Epoch 50/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3508 - accuracy: 0.5972 - val_loss: 2.4839 - val_accuracy: 0.4150\n",
      "Epoch 51/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3338 - accuracy: 0.6029 - val_loss: 2.5219 - val_accuracy: 0.4115\n",
      "Epoch 52/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3214 - accuracy: 0.6071 - val_loss: 2.4935 - val_accuracy: 0.4182\n",
      "Epoch 53/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3077 - accuracy: 0.6094 - val_loss: 2.5628 - val_accuracy: 0.4150\n",
      "Epoch 54/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2903 - accuracy: 0.6146 - val_loss: 2.5965 - val_accuracy: 0.4092\n",
      "Epoch 55/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2869 - accuracy: 0.6159 - val_loss: 2.5811 - val_accuracy: 0.4060\n",
      "Epoch 56/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2671 - accuracy: 0.6185 - val_loss: 2.5267 - val_accuracy: 0.4133\n",
      "Epoch 57/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2605 - accuracy: 0.6220 - val_loss: 2.5393 - val_accuracy: 0.4135\n",
      "Epoch 58/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2486 - accuracy: 0.6229 - val_loss: 2.5930 - val_accuracy: 0.4145\n",
      "Epoch 59/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2289 - accuracy: 0.6291 - val_loss: 2.5995 - val_accuracy: 0.4141\n",
      "Epoch 60/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2201 - accuracy: 0.6313 - val_loss: 2.6180 - val_accuracy: 0.4114\n",
      "Epoch 61/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2147 - accuracy: 0.6324 - val_loss: 2.5865 - val_accuracy: 0.4142\n",
      "Epoch 62/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2011 - accuracy: 0.6370 - val_loss: 2.5987 - val_accuracy: 0.4154\n",
      "Epoch 63/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1860 - accuracy: 0.6396 - val_loss: 2.6628 - val_accuracy: 0.4074\n",
      "Epoch 64/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1755 - accuracy: 0.6420 - val_loss: 2.6416 - val_accuracy: 0.4135\n",
      "Epoch 65/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1682 - accuracy: 0.6446 - val_loss: 2.6512 - val_accuracy: 0.4141\n",
      "Epoch 66/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1554 - accuracy: 0.6467 - val_loss: 2.6722 - val_accuracy: 0.4063\n",
      "Epoch 67/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1446 - accuracy: 0.6492 - val_loss: 2.6321 - val_accuracy: 0.4131\n",
      "Epoch 68/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1422 - accuracy: 0.6521 - val_loss: 2.6416 - val_accuracy: 0.4109\n",
      "Epoch 69/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1289 - accuracy: 0.6553 - val_loss: 2.7382 - val_accuracy: 0.4032\n",
      "Epoch 70/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1245 - accuracy: 0.6569 - val_loss: 2.6813 - val_accuracy: 0.4147\n",
      "Epoch 71/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1159 - accuracy: 0.6589 - val_loss: 2.7111 - val_accuracy: 0.4089\n",
      "Epoch 72/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1025 - accuracy: 0.6617 - val_loss: 2.6757 - val_accuracy: 0.4056\n",
      "Epoch 73/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0947 - accuracy: 0.6656 - val_loss: 2.6666 - val_accuracy: 0.4171\n",
      "Epoch 74/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0915 - accuracy: 0.6649 - val_loss: 2.8168 - val_accuracy: 0.3993\n",
      "Epoch 75/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0787 - accuracy: 0.6687 - val_loss: 2.7525 - val_accuracy: 0.4048\n",
      "Epoch 76/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0726 - accuracy: 0.6706 - val_loss: 2.7064 - val_accuracy: 0.4116\n",
      "Epoch 77/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0665 - accuracy: 0.6728 - val_loss: 2.8012 - val_accuracy: 0.4053\n",
      "Epoch 78/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0534 - accuracy: 0.6755 - val_loss: 2.7874 - val_accuracy: 0.4080\n",
      "Epoch 79/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0484 - accuracy: 0.6768 - val_loss: 2.7384 - val_accuracy: 0.4118\n",
      "Epoch 80/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0449 - accuracy: 0.6766 - val_loss: 2.7432 - val_accuracy: 0.4102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_full, y_full,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy: 41.76%.  Almost 2% better accuracy when combining original dataset with blended dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2X = np.concatenate((x_train, x_train))\n",
    "y_train_2X = np.concatenate((y_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_14 (Rescaling)       (None, 32, 32, 3)    0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_14[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 28, 28, 32)  128         ['conv2d_81[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_118 (Separabl  (None, 28, 28, 32)  1312        ['activation_118[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_118[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_119 (Separabl  (None, 28, 28, 32)  1312        ['activation_119[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_65 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_119[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_81[0][0]']              \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_65[0][0]',       \n",
      "                                                                  'conv2d_82[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 14, 14, 32)  128         ['add_59[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_120 (Separabl  (None, 14, 14, 64)  2336        ['activation_120[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_120[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_121 (Separabl  (None, 14, 14, 64)  4672        ['activation_121[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_66 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_121[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 7, 7, 64)     2048        ['add_59[0][0]']                 \n",
      "                                                                                                  \n",
      " add_60 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_66[0][0]',       \n",
      "                                                                  'conv2d_83[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 7, 7, 64)    256         ['add_60[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_122 (Separabl  (None, 7, 7, 128)   8768        ['activation_122[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_122[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_123 (Separabl  (None, 7, 7, 128)   17536       ['activation_123[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_67 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_123[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 4, 4, 128)    8192        ['add_60[0][0]']                 \n",
      "                                                                                                  \n",
      " add_61 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_67[0][0]',       \n",
      "                                                                  'conv2d_84[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 4, 4, 128)   512         ['add_61[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_124 (Separabl  (None, 4, 4, 256)   33920       ['activation_124[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_124[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_125 (Separabl  (None, 4, 4, 256)   67840       ['activation_125[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_68 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_125[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 2, 2, 256)    32768       ['add_61[0][0]']                 \n",
      "                                                                                                  \n",
      " add_62 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_68[0][0]',       \n",
      "                                                                  'conv2d_85[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 1024)         0           ['add_62[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 1024)         0           ['flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 256)          262400      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 256)          0           ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 128)          32896       ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 128)          0           ['dense_49[0][0]']               \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 100)          12900       ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_DS2X.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 4.0519 - accuracy: 0.0807 - val_loss: 3.6024 - val_accuracy: 0.1607\n",
      "Epoch 2/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.6292 - accuracy: 0.1392 - val_loss: 3.3436 - val_accuracy: 0.2066\n",
      "Epoch 3/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.4199 - accuracy: 0.1757 - val_loss: 3.1548 - val_accuracy: 0.2374\n",
      "Epoch 4/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.2517 - accuracy: 0.2035 - val_loss: 3.0417 - val_accuracy: 0.2553\n",
      "Epoch 5/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.1044 - accuracy: 0.2307 - val_loss: 2.8926 - val_accuracy: 0.2870\n",
      "Epoch 6/80\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 2.9737 - accuracy: 0.2528 - val_loss: 2.7940 - val_accuracy: 0.2963\n",
      "Epoch 7/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.8680 - accuracy: 0.2718 - val_loss: 2.7598 - val_accuracy: 0.3083\n",
      "Epoch 8/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.7706 - accuracy: 0.2902 - val_loss: 2.6581 - val_accuracy: 0.3254\n",
      "Epoch 9/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6872 - accuracy: 0.3075 - val_loss: 2.6256 - val_accuracy: 0.3314\n",
      "Epoch 10/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6062 - accuracy: 0.3244 - val_loss: 2.5702 - val_accuracy: 0.3437\n",
      "Epoch 11/80\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 2.5365 - accuracy: 0.3371 - val_loss: 2.4968 - val_accuracy: 0.3589\n",
      "Epoch 12/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.4724 - accuracy: 0.3516 - val_loss: 2.4708 - val_accuracy: 0.3641\n",
      "Epoch 13/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4111 - accuracy: 0.3631 - val_loss: 2.4777 - val_accuracy: 0.3659\n",
      "Epoch 14/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3594 - accuracy: 0.3718 - val_loss: 2.4729 - val_accuracy: 0.3664\n",
      "Epoch 15/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.3067 - accuracy: 0.3855 - val_loss: 2.4206 - val_accuracy: 0.3747\n",
      "Epoch 16/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.2541 - accuracy: 0.3971 - val_loss: 2.4055 - val_accuracy: 0.3824\n",
      "Epoch 17/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2096 - accuracy: 0.4062 - val_loss: 2.5261 - val_accuracy: 0.3590\n",
      "Epoch 18/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1549 - accuracy: 0.4179 - val_loss: 2.4012 - val_accuracy: 0.3837\n",
      "Epoch 19/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.1188 - accuracy: 0.4234 - val_loss: 2.3743 - val_accuracy: 0.3887\n",
      "Epoch 20/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0785 - accuracy: 0.4336 - val_loss: 2.4169 - val_accuracy: 0.3846\n",
      "Epoch 21/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0384 - accuracy: 0.4429 - val_loss: 2.4228 - val_accuracy: 0.3843\n",
      "Epoch 22/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9938 - accuracy: 0.4493 - val_loss: 2.4274 - val_accuracy: 0.3823\n",
      "Epoch 23/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9600 - accuracy: 0.4570 - val_loss: 2.3777 - val_accuracy: 0.3940\n",
      "Epoch 24/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.9232 - accuracy: 0.4666 - val_loss: 2.3634 - val_accuracy: 0.3981\n",
      "Epoch 25/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8922 - accuracy: 0.4746 - val_loss: 2.3868 - val_accuracy: 0.3979\n",
      "Epoch 26/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8547 - accuracy: 0.4802 - val_loss: 2.4034 - val_accuracy: 0.3956\n",
      "Epoch 27/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8200 - accuracy: 0.4889 - val_loss: 2.4260 - val_accuracy: 0.3926\n",
      "Epoch 28/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7903 - accuracy: 0.4938 - val_loss: 2.4354 - val_accuracy: 0.3945\n",
      "Epoch 29/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7551 - accuracy: 0.5041 - val_loss: 2.4136 - val_accuracy: 0.3978\n",
      "Epoch 30/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7326 - accuracy: 0.5083 - val_loss: 2.3973 - val_accuracy: 0.4015\n",
      "Epoch 31/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7074 - accuracy: 0.5137 - val_loss: 2.4227 - val_accuracy: 0.4021\n",
      "Epoch 32/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6814 - accuracy: 0.5190 - val_loss: 2.4918 - val_accuracy: 0.3909\n",
      "Epoch 33/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6562 - accuracy: 0.5272 - val_loss: 2.5164 - val_accuracy: 0.3913\n",
      "Epoch 34/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6314 - accuracy: 0.5309 - val_loss: 2.4778 - val_accuracy: 0.4058\n",
      "Epoch 35/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6051 - accuracy: 0.5361 - val_loss: 2.4959 - val_accuracy: 0.3975\n",
      "Epoch 36/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5770 - accuracy: 0.5437 - val_loss: 2.4679 - val_accuracy: 0.4032\n",
      "Epoch 37/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5580 - accuracy: 0.5480 - val_loss: 2.5143 - val_accuracy: 0.4005\n",
      "Epoch 38/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5356 - accuracy: 0.5531 - val_loss: 2.5120 - val_accuracy: 0.4007\n",
      "Epoch 39/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5117 - accuracy: 0.5583 - val_loss: 2.5260 - val_accuracy: 0.4030\n",
      "Epoch 40/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4878 - accuracy: 0.5654 - val_loss: 2.5304 - val_accuracy: 0.4026\n",
      "Epoch 41/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4758 - accuracy: 0.5673 - val_loss: 2.5782 - val_accuracy: 0.3953\n",
      "Epoch 42/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4580 - accuracy: 0.5703 - val_loss: 2.5667 - val_accuracy: 0.3989\n",
      "Epoch 43/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4366 - accuracy: 0.5772 - val_loss: 2.5275 - val_accuracy: 0.4045\n",
      "Epoch 44/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4194 - accuracy: 0.5806 - val_loss: 2.5911 - val_accuracy: 0.4033\n",
      "Epoch 45/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3978 - accuracy: 0.5855 - val_loss: 2.6059 - val_accuracy: 0.3973\n",
      "Epoch 46/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3817 - accuracy: 0.5910 - val_loss: 2.6220 - val_accuracy: 0.4011\n",
      "Epoch 47/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3655 - accuracy: 0.5939 - val_loss: 2.6465 - val_accuracy: 0.3966\n",
      "Epoch 48/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3413 - accuracy: 0.5981 - val_loss: 2.6673 - val_accuracy: 0.3985\n",
      "Epoch 49/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3373 - accuracy: 0.6009 - val_loss: 2.6288 - val_accuracy: 0.4006\n",
      "Epoch 50/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3189 - accuracy: 0.6064 - val_loss: 2.6534 - val_accuracy: 0.3986\n",
      "Epoch 51/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3024 - accuracy: 0.6107 - val_loss: 2.7158 - val_accuracy: 0.3957\n",
      "Epoch 52/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2876 - accuracy: 0.6136 - val_loss: 2.6842 - val_accuracy: 0.3996\n",
      "Epoch 53/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2789 - accuracy: 0.6162 - val_loss: 2.6930 - val_accuracy: 0.3992\n",
      "Epoch 54/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2600 - accuracy: 0.6186 - val_loss: 2.7388 - val_accuracy: 0.3937\n",
      "Epoch 55/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2483 - accuracy: 0.6232 - val_loss: 2.7948 - val_accuracy: 0.3891\n",
      "Epoch 56/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2375 - accuracy: 0.6274 - val_loss: 2.7469 - val_accuracy: 0.3968\n",
      "Epoch 57/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2218 - accuracy: 0.6323 - val_loss: 2.7633 - val_accuracy: 0.3980\n",
      "Epoch 58/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2067 - accuracy: 0.6330 - val_loss: 2.7352 - val_accuracy: 0.4008\n",
      "Epoch 59/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1986 - accuracy: 0.6369 - val_loss: 2.8109 - val_accuracy: 0.3976\n",
      "Epoch 60/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1848 - accuracy: 0.6392 - val_loss: 2.7425 - val_accuracy: 0.3962\n",
      "Epoch 61/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1691 - accuracy: 0.6432 - val_loss: 2.8553 - val_accuracy: 0.3934\n",
      "Epoch 62/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1637 - accuracy: 0.6442 - val_loss: 2.8282 - val_accuracy: 0.3958\n",
      "Epoch 63/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1590 - accuracy: 0.6455 - val_loss: 2.8294 - val_accuracy: 0.3942\n",
      "Epoch 64/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1403 - accuracy: 0.6507 - val_loss: 2.8305 - val_accuracy: 0.3974\n",
      "Epoch 65/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1399 - accuracy: 0.6522 - val_loss: 2.9124 - val_accuracy: 0.3859\n",
      "Epoch 66/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1253 - accuracy: 0.6549 - val_loss: 2.8450 - val_accuracy: 0.3966\n",
      "Epoch 67/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1141 - accuracy: 0.6581 - val_loss: 2.9016 - val_accuracy: 0.3943\n",
      "Epoch 68/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0937 - accuracy: 0.6634 - val_loss: 2.9253 - val_accuracy: 0.3942\n",
      "Epoch 69/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0959 - accuracy: 0.6637 - val_loss: 2.9144 - val_accuracy: 0.3973\n",
      "Epoch 70/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0877 - accuracy: 0.6664 - val_loss: 2.8787 - val_accuracy: 0.3932\n",
      "Epoch 71/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0778 - accuracy: 0.6687 - val_loss: 2.9490 - val_accuracy: 0.3876\n",
      "Epoch 72/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0726 - accuracy: 0.6694 - val_loss: 2.8882 - val_accuracy: 0.3975\n",
      "Epoch 73/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0520 - accuracy: 0.6757 - val_loss: 2.9504 - val_accuracy: 0.3944\n",
      "Epoch 74/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0507 - accuracy: 0.6730 - val_loss: 2.9647 - val_accuracy: 0.3929\n",
      "Epoch 75/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0398 - accuracy: 0.6787 - val_loss: 2.9121 - val_accuracy: 0.3921\n",
      "Epoch 76/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0438 - accuracy: 0.6785 - val_loss: 2.9967 - val_accuracy: 0.3899\n",
      "Epoch 77/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0302 - accuracy: 0.6818 - val_loss: 3.0093 - val_accuracy: 0.3920\n",
      "Epoch 78/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0223 - accuracy: 0.6823 - val_loss: 2.9714 - val_accuracy: 0.3947\n",
      "Epoch 79/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0106 - accuracy: 0.6870 - val_loss: 2.9364 - val_accuracy: 0.3987\n",
      "Epoch 80/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0065 - accuracy: 0.6887 - val_loss: 2.9553 - val_accuracy: 0.3978\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train_2X, y_train_2X,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy: 40.40%. The combonation of the blended and origanal dataset was 1.25% better than the original dataset doubled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "[\n",
    "layers.RandomFlip(\"horizontal\"),\n",
    "layers.RandomRotation(0.1),\n",
    "layers.RandomZoom(0.1),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 32, 32, 3)    0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " rescaling_19 (Rescaling)       (None, 32, 32, 3)    0           ['sequential_6[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 28, 28, 32)   2400        ['rescaling_19[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 28, 28, 32)  128         ['conv2d_106[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_158 (Separabl  (None, 28, 28, 32)  1312        ['activation_158[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_158[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_159 (Separabl  (None, 28, 28, 32)  1312        ['activation_159[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_85 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_159[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 14, 14, 32)   1024        ['conv2d_106[0][0]']             \n",
      "                                                                                                  \n",
      " add_79 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_85[0][0]',       \n",
      "                                                                  'conv2d_107[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 14, 14, 32)  128         ['add_79[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_160 (Separabl  (None, 14, 14, 64)  2336        ['activation_160[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_160[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_161 (Separabl  (None, 14, 14, 64)  4672        ['activation_161[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_86 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_161[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 7, 7, 64)     2048        ['add_79[0][0]']                 \n",
      "                                                                                                  \n",
      " add_80 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_86[0][0]',       \n",
      "                                                                  'conv2d_108[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 7, 7, 64)    256         ['add_80[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_162 (Separabl  (None, 7, 7, 128)   8768        ['activation_162[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_162[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_163 (Separabl  (None, 7, 7, 128)   17536       ['activation_163[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_87 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_163[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 4, 4, 128)    8192        ['add_80[0][0]']                 \n",
      "                                                                                                  \n",
      " add_81 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_87[0][0]',       \n",
      "                                                                  'conv2d_109[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 4, 4, 128)   512         ['add_81[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_164 (Separabl  (None, 4, 4, 256)   33920       ['activation_164[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_164[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_165 (Separabl  (None, 4, 4, 256)   67840       ['activation_165[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_88 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_165[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 2, 2, 256)    32768       ['add_81[0][0]']                 \n",
      "                                                                                                  \n",
      " add_82 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_88[0][0]',       \n",
      "                                                                  'conv2d_110[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_16 (Flatten)           (None, 1024)         0           ['add_82[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 1024)         0           ['flatten_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 256)          262400      ['dropout_65[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 256)          0           ['dense_63[0][0]']               \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 128)          32896       ['dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 128)          0           ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 100)          12900       ['dropout_67[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_fullds_classic.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 30s 18ms/step - loss: 4.0968 - accuracy: 0.0734 - val_loss: 3.6703 - val_accuracy: 0.1474\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.7326 - accuracy: 0.1221 - val_loss: 3.5050 - val_accuracy: 0.1681\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.5729 - accuracy: 0.1486 - val_loss: 3.3526 - val_accuracy: 0.1881\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.4479 - accuracy: 0.1704 - val_loss: 3.2298 - val_accuracy: 0.2100\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 3.3486 - accuracy: 0.1870 - val_loss: 3.0836 - val_accuracy: 0.2400\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.2641 - accuracy: 0.2037 - val_loss: 3.0809 - val_accuracy: 0.2425\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 3.1849 - accuracy: 0.2185 - val_loss: 3.0721 - val_accuracy: 0.2451\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 3.1269 - accuracy: 0.2293 - val_loss: 3.0372 - val_accuracy: 0.2485\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 3.0655 - accuracy: 0.2403 - val_loss: 3.0187 - val_accuracy: 0.2568\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 3.0147 - accuracy: 0.2509 - val_loss: 2.8048 - val_accuracy: 0.2982\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.9655 - accuracy: 0.2592 - val_loss: 3.0530 - val_accuracy: 0.2464\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.9284 - accuracy: 0.2680 - val_loss: 2.8262 - val_accuracy: 0.2887\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.8808 - accuracy: 0.2765 - val_loss: 2.9001 - val_accuracy: 0.2817\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.8463 - accuracy: 0.2838 - val_loss: 2.7031 - val_accuracy: 0.3130\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 2.8145 - accuracy: 0.2919 - val_loss: 2.6723 - val_accuracy: 0.3230\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.7764 - accuracy: 0.2973 - val_loss: 2.6767 - val_accuracy: 0.3230\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.7472 - accuracy: 0.3034 - val_loss: 2.6953 - val_accuracy: 0.3167\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.7169 - accuracy: 0.3111 - val_loss: 2.5986 - val_accuracy: 0.3376\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.6923 - accuracy: 0.3151 - val_loss: 2.6282 - val_accuracy: 0.3325\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 2.6597 - accuracy: 0.3213 - val_loss: 2.5297 - val_accuracy: 0.3526\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.6333 - accuracy: 0.3272 - val_loss: 2.5873 - val_accuracy: 0.3423\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.6136 - accuracy: 0.3313 - val_loss: 2.5905 - val_accuracy: 0.3413\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.5901 - accuracy: 0.3342 - val_loss: 2.6107 - val_accuracy: 0.3409\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.5709 - accuracy: 0.3413 - val_loss: 2.4540 - val_accuracy: 0.3685\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 2.5493 - accuracy: 0.3449 - val_loss: 2.4896 - val_accuracy: 0.3605\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.5299 - accuracy: 0.3491 - val_loss: 2.4104 - val_accuracy: 0.3833\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.5089 - accuracy: 0.3537 - val_loss: 2.4479 - val_accuracy: 0.3723\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.4871 - accuracy: 0.3581 - val_loss: 2.3996 - val_accuracy: 0.3859\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.4676 - accuracy: 0.3611 - val_loss: 2.3253 - val_accuracy: 0.3983\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.4463 - accuracy: 0.3671 - val_loss: 2.4735 - val_accuracy: 0.3674\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4318 - accuracy: 0.3688 - val_loss: 2.3298 - val_accuracy: 0.3989\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.4146 - accuracy: 0.3729 - val_loss: 2.3950 - val_accuracy: 0.3838\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4092 - accuracy: 0.3751 - val_loss: 2.4302 - val_accuracy: 0.3768\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.3826 - accuracy: 0.3799 - val_loss: 2.3365 - val_accuracy: 0.3966\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.3696 - accuracy: 0.3825 - val_loss: 2.2807 - val_accuracy: 0.4067\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.3567 - accuracy: 0.3862 - val_loss: 2.3795 - val_accuracy: 0.3890\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3394 - accuracy: 0.3903 - val_loss: 2.3674 - val_accuracy: 0.3887\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3231 - accuracy: 0.3926 - val_loss: 2.3206 - val_accuracy: 0.4010\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3190 - accuracy: 0.3941 - val_loss: 2.2767 - val_accuracy: 0.4121\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.3021 - accuracy: 0.3990 - val_loss: 2.3147 - val_accuracy: 0.3988\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.2883 - accuracy: 0.4020 - val_loss: 2.2890 - val_accuracy: 0.4045\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2733 - accuracy: 0.4054 - val_loss: 2.2689 - val_accuracy: 0.4123\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.2575 - accuracy: 0.4052 - val_loss: 2.2396 - val_accuracy: 0.4153\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.2495 - accuracy: 0.4093 - val_loss: 2.2333 - val_accuracy: 0.4181\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2373 - accuracy: 0.4114 - val_loss: 2.2835 - val_accuracy: 0.4097\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2240 - accuracy: 0.4144 - val_loss: 2.2491 - val_accuracy: 0.4120\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.2220 - accuracy: 0.4135 - val_loss: 2.2680 - val_accuracy: 0.4105\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.2031 - accuracy: 0.4187 - val_loss: 2.2243 - val_accuracy: 0.4212\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2008 - accuracy: 0.4187 - val_loss: 2.2003 - val_accuracy: 0.4267\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.1826 - accuracy: 0.4238 - val_loss: 2.2332 - val_accuracy: 0.4175\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.1721 - accuracy: 0.4266 - val_loss: 2.1879 - val_accuracy: 0.4243\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1682 - accuracy: 0.4262 - val_loss: 2.2449 - val_accuracy: 0.4208\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.1511 - accuracy: 0.4313 - val_loss: 2.1887 - val_accuracy: 0.4297\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.1451 - accuracy: 0.4332 - val_loss: 2.2506 - val_accuracy: 0.4178\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.1368 - accuracy: 0.4342 - val_loss: 2.2459 - val_accuracy: 0.4199\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.1242 - accuracy: 0.4378 - val_loss: 2.1675 - val_accuracy: 0.4363\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1144 - accuracy: 0.4388 - val_loss: 2.2364 - val_accuracy: 0.4192\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.1065 - accuracy: 0.4420 - val_loss: 2.1302 - val_accuracy: 0.4465\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.0987 - accuracy: 0.4425 - val_loss: 2.1545 - val_accuracy: 0.4393\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0919 - accuracy: 0.4447 - val_loss: 2.2177 - val_accuracy: 0.4281\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.0800 - accuracy: 0.4482 - val_loss: 2.1819 - val_accuracy: 0.4313\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.0719 - accuracy: 0.4477 - val_loss: 2.1068 - val_accuracy: 0.4526\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0628 - accuracy: 0.4504 - val_loss: 2.1298 - val_accuracy: 0.4449\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.0519 - accuracy: 0.4518 - val_loss: 2.1478 - val_accuracy: 0.4422\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.0442 - accuracy: 0.4541 - val_loss: 2.1437 - val_accuracy: 0.4393\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0389 - accuracy: 0.4561 - val_loss: 2.1444 - val_accuracy: 0.4411\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0302 - accuracy: 0.4564 - val_loss: 2.1873 - val_accuracy: 0.4336\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0168 - accuracy: 0.4595 - val_loss: 2.1501 - val_accuracy: 0.4414\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0241 - accuracy: 0.4602 - val_loss: 2.1045 - val_accuracy: 0.4517\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0031 - accuracy: 0.4641 - val_loss: 2.0819 - val_accuracy: 0.4574\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9953 - accuracy: 0.4643 - val_loss: 2.1283 - val_accuracy: 0.4498\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9905 - accuracy: 0.4679 - val_loss: 2.0669 - val_accuracy: 0.4558\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9849 - accuracy: 0.4687 - val_loss: 2.0954 - val_accuracy: 0.4534\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9812 - accuracy: 0.4684 - val_loss: 2.1203 - val_accuracy: 0.4475\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9716 - accuracy: 0.4709 - val_loss: 2.1266 - val_accuracy: 0.4492\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.9631 - accuracy: 0.4710 - val_loss: 2.0753 - val_accuracy: 0.4532\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9607 - accuracy: 0.4745 - val_loss: 2.1630 - val_accuracy: 0.4362\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.9513 - accuracy: 0.4755 - val_loss: 2.0717 - val_accuracy: 0.4633\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9430 - accuracy: 0.4778 - val_loss: 2.0841 - val_accuracy: 0.4571\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9384 - accuracy: 0.4784 - val_loss: 2.0799 - val_accuracy: 0.4573\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9307 - accuracy: 0.4798 - val_loss: 2.0724 - val_accuracy: 0.4598\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9289 - accuracy: 0.4814 - val_loss: 2.1044 - val_accuracy: 0.4545\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9203 - accuracy: 0.4803 - val_loss: 2.0465 - val_accuracy: 0.4636\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9178 - accuracy: 0.4830 - val_loss: 2.0554 - val_accuracy: 0.4670\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9116 - accuracy: 0.4869 - val_loss: 2.1023 - val_accuracy: 0.4570\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9014 - accuracy: 0.4867 - val_loss: 2.0578 - val_accuracy: 0.4607\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8890 - accuracy: 0.4880 - val_loss: 2.0347 - val_accuracy: 0.4647\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8923 - accuracy: 0.4890 - val_loss: 2.0332 - val_accuracy: 0.4689\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8873 - accuracy: 0.4898 - val_loss: 2.0160 - val_accuracy: 0.4699\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8809 - accuracy: 0.4912 - val_loss: 2.1024 - val_accuracy: 0.4565\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8813 - accuracy: 0.4923 - val_loss: 2.0197 - val_accuracy: 0.4699\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.8723 - accuracy: 0.4946 - val_loss: 2.0021 - val_accuracy: 0.4765\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.8600 - accuracy: 0.4953 - val_loss: 2.0722 - val_accuracy: 0.4591\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8601 - accuracy: 0.4953 - val_loss: 2.0911 - val_accuracy: 0.4633\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8579 - accuracy: 0.4967 - val_loss: 2.0878 - val_accuracy: 0.4575\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8450 - accuracy: 0.4981 - val_loss: 1.9921 - val_accuracy: 0.4775\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.8444 - accuracy: 0.4997 - val_loss: 2.0093 - val_accuracy: 0.4752\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8365 - accuracy: 0.5017 - val_loss: 2.1230 - val_accuracy: 0.4569\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8317 - accuracy: 0.5031 - val_loss: 2.0136 - val_accuracy: 0.4704\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8277 - accuracy: 0.5045 - val_loss: 1.9986 - val_accuracy: 0.4791\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8249 - accuracy: 0.5053 - val_loss: 1.9499 - val_accuracy: 0.4883\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8203 - accuracy: 0.5057 - val_loss: 2.0018 - val_accuracy: 0.4735\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8131 - accuracy: 0.5062 - val_loss: 2.0589 - val_accuracy: 0.4655\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.8148 - accuracy: 0.5063 - val_loss: 1.9687 - val_accuracy: 0.4787\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8056 - accuracy: 0.5082 - val_loss: 1.9817 - val_accuracy: 0.4797\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8010 - accuracy: 0.5097 - val_loss: 2.0568 - val_accuracy: 0.4673\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7945 - accuracy: 0.5121 - val_loss: 1.9711 - val_accuracy: 0.4794\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7859 - accuracy: 0.5128 - val_loss: 2.0466 - val_accuracy: 0.4721\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7899 - accuracy: 0.5139 - val_loss: 2.0325 - val_accuracy: 0.4741\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7796 - accuracy: 0.5145 - val_loss: 2.0246 - val_accuracy: 0.4754\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7702 - accuracy: 0.5137 - val_loss: 2.0079 - val_accuracy: 0.4769\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7734 - accuracy: 0.5151 - val_loss: 1.9803 - val_accuracy: 0.4843\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7678 - accuracy: 0.5181 - val_loss: 1.9622 - val_accuracy: 0.4856\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7625 - accuracy: 0.5201 - val_loss: 1.9852 - val_accuracy: 0.4794\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7581 - accuracy: 0.5197 - val_loss: 1.9928 - val_accuracy: 0.4847\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7540 - accuracy: 0.5195 - val_loss: 2.0388 - val_accuracy: 0.4719\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7524 - accuracy: 0.5214 - val_loss: 1.9942 - val_accuracy: 0.4829\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7448 - accuracy: 0.5235 - val_loss: 1.9716 - val_accuracy: 0.4814\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7495 - accuracy: 0.5210 - val_loss: 2.0196 - val_accuracy: 0.4747\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7460 - accuracy: 0.5220 - val_loss: 2.0208 - val_accuracy: 0.4784\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7320 - accuracy: 0.5256 - val_loss: 1.9596 - val_accuracy: 0.4856\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7350 - accuracy: 0.5259 - val_loss: 2.0516 - val_accuracy: 0.4738\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7296 - accuracy: 0.5282 - val_loss: 1.9574 - val_accuracy: 0.4916\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7281 - accuracy: 0.5287 - val_loss: 2.0536 - val_accuracy: 0.4738\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7197 - accuracy: 0.5292 - val_loss: 2.0181 - val_accuracy: 0.4803\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7165 - accuracy: 0.5305 - val_loss: 1.9965 - val_accuracy: 0.4824\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7139 - accuracy: 0.5297 - val_loss: 1.9863 - val_accuracy: 0.4900\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7096 - accuracy: 0.5307 - val_loss: 2.0043 - val_accuracy: 0.4828\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7052 - accuracy: 0.5322 - val_loss: 1.9530 - val_accuracy: 0.4882\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7056 - accuracy: 0.5325 - val_loss: 1.9287 - val_accuracy: 0.4977\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7029 - accuracy: 0.5329 - val_loss: 1.9762 - val_accuracy: 0.4882\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6979 - accuracy: 0.5319 - val_loss: 1.9953 - val_accuracy: 0.4835\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6918 - accuracy: 0.5333 - val_loss: 1.9785 - val_accuracy: 0.4862\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6938 - accuracy: 0.5361 - val_loss: 1.9460 - val_accuracy: 0.4911\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.6865 - accuracy: 0.5358 - val_loss: 1.9878 - val_accuracy: 0.4864\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6864 - accuracy: 0.5363 - val_loss: 1.9199 - val_accuracy: 0.4999\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6804 - accuracy: 0.5390 - val_loss: 1.9303 - val_accuracy: 0.4938\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6733 - accuracy: 0.5406 - val_loss: 1.9486 - val_accuracy: 0.4942\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6602 - accuracy: 0.5415 - val_loss: 1.9727 - val_accuracy: 0.4936\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6676 - accuracy: 0.5379 - val_loss: 1.9258 - val_accuracy: 0.4979\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6637 - accuracy: 0.5420 - val_loss: 1.9407 - val_accuracy: 0.4933\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6573 - accuracy: 0.5412 - val_loss: 1.9633 - val_accuracy: 0.4875\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6624 - accuracy: 0.5430 - val_loss: 1.9105 - val_accuracy: 0.5032\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6503 - accuracy: 0.5426 - val_loss: 1.9851 - val_accuracy: 0.4856\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6499 - accuracy: 0.5449 - val_loss: 1.9783 - val_accuracy: 0.4905\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6455 - accuracy: 0.5444 - val_loss: 1.9549 - val_accuracy: 0.4938\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.6444 - accuracy: 0.5463 - val_loss: 1.9680 - val_accuracy: 0.4895\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6409 - accuracy: 0.5472 - val_loss: 1.9517 - val_accuracy: 0.4927\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6392 - accuracy: 0.5478 - val_loss: 2.0036 - val_accuracy: 0.4818\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6338 - accuracy: 0.5481 - val_loss: 1.9434 - val_accuracy: 0.4925\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6273 - accuracy: 0.5519 - val_loss: 1.9115 - val_accuracy: 0.5042\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6314 - accuracy: 0.5490 - val_loss: 1.9240 - val_accuracy: 0.5020\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6268 - accuracy: 0.5493 - val_loss: 1.9084 - val_accuracy: 0.5005\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6250 - accuracy: 0.5504 - val_loss: 1.9891 - val_accuracy: 0.4915\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6133 - accuracy: 0.5507 - val_loss: 1.8957 - val_accuracy: 0.5039\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6159 - accuracy: 0.5519 - val_loss: 1.9527 - val_accuracy: 0.4989\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6227 - accuracy: 0.5515 - val_loss: 1.9286 - val_accuracy: 0.4999\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.6099 - accuracy: 0.5529 - val_loss: 1.9684 - val_accuracy: 0.4923\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.6082 - accuracy: 0.5549 - val_loss: 1.8969 - val_accuracy: 0.5032\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5991 - accuracy: 0.5568 - val_loss: 1.9441 - val_accuracy: 0.4956\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6021 - accuracy: 0.5552 - val_loss: 1.9680 - val_accuracy: 0.4964\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5989 - accuracy: 0.5573 - val_loss: 1.9096 - val_accuracy: 0.5058\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5984 - accuracy: 0.5571 - val_loss: 1.9394 - val_accuracy: 0.4986\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5873 - accuracy: 0.5581 - val_loss: 1.9732 - val_accuracy: 0.4930\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5923 - accuracy: 0.5586 - val_loss: 1.8654 - val_accuracy: 0.5131\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5849 - accuracy: 0.5592 - val_loss: 1.8833 - val_accuracy: 0.5087\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5854 - accuracy: 0.5609 - val_loss: 1.9340 - val_accuracy: 0.5037\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5887 - accuracy: 0.5587 - val_loss: 1.9175 - val_accuracy: 0.5025\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.5832 - accuracy: 0.5605 - val_loss: 1.9865 - val_accuracy: 0.4931\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5754 - accuracy: 0.5618 - val_loss: 1.9501 - val_accuracy: 0.4994\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 1.5748 - accuracy: 0.5628 - val_loss: 1.9532 - val_accuracy: 0.5035\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5718 - accuracy: 0.5636 - val_loss: 1.9495 - val_accuracy: 0.4972\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5674 - accuracy: 0.5644 - val_loss: 1.9213 - val_accuracy: 0.5044\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.5699 - accuracy: 0.5646 - val_loss: 1.8881 - val_accuracy: 0.5072\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5657 - accuracy: 0.5635 - val_loss: 1.8578 - val_accuracy: 0.5146\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5604 - accuracy: 0.5647 - val_loss: 1.9393 - val_accuracy: 0.5003\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5575 - accuracy: 0.5671 - val_loss: 2.0165 - val_accuracy: 0.4848\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5560 - accuracy: 0.5656 - val_loss: 1.9085 - val_accuracy: 0.5047\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.5585 - accuracy: 0.5670 - val_loss: 1.9235 - val_accuracy: 0.5063\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5491 - accuracy: 0.5681 - val_loss: 1.9170 - val_accuracy: 0.5039\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5495 - accuracy: 0.5688 - val_loss: 1.9216 - val_accuracy: 0.5029\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.5430 - accuracy: 0.5687 - val_loss: 1.9001 - val_accuracy: 0.5100\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5427 - accuracy: 0.5711 - val_loss: 1.9444 - val_accuracy: 0.5041\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5465 - accuracy: 0.5689 - val_loss: 1.8485 - val_accuracy: 0.5139\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5383 - accuracy: 0.5713 - val_loss: 1.9024 - val_accuracy: 0.5108\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5351 - accuracy: 0.5718 - val_loss: 1.9030 - val_accuracy: 0.5105\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5341 - accuracy: 0.5722 - val_loss: 1.8764 - val_accuracy: 0.5108\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5318 - accuracy: 0.5726 - val_loss: 1.9023 - val_accuracy: 0.5080\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5332 - accuracy: 0.5726 - val_loss: 1.8864 - val_accuracy: 0.5071\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5302 - accuracy: 0.5737 - val_loss: 1.9493 - val_accuracy: 0.5025\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5251 - accuracy: 0.5733 - val_loss: 1.8631 - val_accuracy: 0.5123\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5269 - accuracy: 0.5736 - val_loss: 1.9137 - val_accuracy: 0.5061\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5167 - accuracy: 0.5767 - val_loss: 1.9488 - val_accuracy: 0.5029\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5171 - accuracy: 0.5762 - val_loss: 1.8758 - val_accuracy: 0.5151\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 1.5178 - accuracy: 0.5760 - val_loss: 1.9301 - val_accuracy: 0.5046\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5099 - accuracy: 0.5775 - val_loss: 1.9036 - val_accuracy: 0.5115\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5158 - accuracy: 0.5770 - val_loss: 1.9253 - val_accuracy: 0.5078\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5098 - accuracy: 0.5775 - val_loss: 1.9006 - val_accuracy: 0.5152\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5053 - accuracy: 0.5795 - val_loss: 1.9151 - val_accuracy: 0.5106\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5097 - accuracy: 0.5786 - val_loss: 1.9033 - val_accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_full, y_full,\n",
    "epochs=200,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 32, 32, 3)    0           ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " rescaling_20 (Rescaling)       (None, 32, 32, 3)    0           ['sequential_6[2][0]']           \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 28, 28, 32)   2400        ['rescaling_20[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 28, 28, 32)  128         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_166 (Separabl  (None, 28, 28, 32)  1312        ['activation_166[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_166[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_167 (Separabl  (None, 28, 28, 32)  1312        ['activation_167[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_89 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_167[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 14, 14, 32)   1024        ['conv2d_111[0][0]']             \n",
      "                                                                                                  \n",
      " add_83 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_89[0][0]',       \n",
      "                                                                  'conv2d_112[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 14, 14, 32)  128         ['add_83[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_168 (Separabl  (None, 14, 14, 64)  2336        ['activation_168[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_168[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_169 (Separabl  (None, 14, 14, 64)  4672        ['activation_169[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_90 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_169[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 7, 7, 64)     2048        ['add_83[0][0]']                 \n",
      "                                                                                                  \n",
      " add_84 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_90[0][0]',       \n",
      "                                                                  'conv2d_113[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 7, 7, 64)    256         ['add_84[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_170 (Separabl  (None, 7, 7, 128)   8768        ['activation_170[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_170[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_171 (Separabl  (None, 7, 7, 128)   17536       ['activation_171[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_91 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_171[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 4, 4, 128)    8192        ['add_84[0][0]']                 \n",
      "                                                                                                  \n",
      " add_85 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_91[0][0]',       \n",
      "                                                                  'conv2d_114[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 4, 4, 128)   512         ['add_85[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_172 (Separabl  (None, 4, 4, 256)   33920       ['activation_172[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_172[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_173 (Separabl  (None, 4, 4, 256)   67840       ['activation_173[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_92 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_173[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 2, 2, 256)    32768       ['add_85[0][0]']                 \n",
      "                                                                                                  \n",
      " add_86 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_92[0][0]',       \n",
      "                                                                  'conv2d_115[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_17 (Flatten)           (None, 1024)         0           ['add_86[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)           (None, 1024)         0           ['flatten_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 256)          262400      ['dropout_68[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)           (None, 256)          0           ['dense_66[0][0]']               \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 128)          32896       ['dropout_69[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)           (None, 128)          0           ['dense_67[0][0]']               \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 100)          12900       ['dropout_70[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_DS2X_classic.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 4.0719 - accuracy: 0.0774 - val_loss: 3.6722 - val_accuracy: 0.1452\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 3.7140 - accuracy: 0.1235 - val_loss: 3.4796 - val_accuracy: 0.1701\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.5449 - accuracy: 0.1506 - val_loss: 3.3349 - val_accuracy: 0.1970\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 3.4206 - accuracy: 0.1728 - val_loss: 3.2317 - val_accuracy: 0.2146\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 3.3121 - accuracy: 0.1937 - val_loss: 3.1575 - val_accuracy: 0.2262\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 3.2266 - accuracy: 0.2086 - val_loss: 2.9662 - val_accuracy: 0.2660\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 3.1575 - accuracy: 0.2218 - val_loss: 2.9366 - val_accuracy: 0.2653\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.0904 - accuracy: 0.2349 - val_loss: 2.8772 - val_accuracy: 0.2826\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.0387 - accuracy: 0.2465 - val_loss: 2.9083 - val_accuracy: 0.2735\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.9808 - accuracy: 0.2551 - val_loss: 2.7865 - val_accuracy: 0.3049\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.9340 - accuracy: 0.2658 - val_loss: 2.8161 - val_accuracy: 0.2912\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.8880 - accuracy: 0.2748 - val_loss: 2.8024 - val_accuracy: 0.2984\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.8528 - accuracy: 0.2833 - val_loss: 2.7270 - val_accuracy: 0.3120\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.8165 - accuracy: 0.2921 - val_loss: 2.7329 - val_accuracy: 0.3084\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.7791 - accuracy: 0.2966 - val_loss: 2.6308 - val_accuracy: 0.3351\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.7428 - accuracy: 0.3051 - val_loss: 2.6158 - val_accuracy: 0.3353\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.7103 - accuracy: 0.3130 - val_loss: 2.5976 - val_accuracy: 0.3373\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.6842 - accuracy: 0.3191 - val_loss: 2.6260 - val_accuracy: 0.3324\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.6506 - accuracy: 0.3239 - val_loss: 2.5896 - val_accuracy: 0.3380\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.6295 - accuracy: 0.3297 - val_loss: 2.4632 - val_accuracy: 0.3645\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.6010 - accuracy: 0.3361 - val_loss: 2.5084 - val_accuracy: 0.3599\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.5742 - accuracy: 0.3401 - val_loss: 2.4737 - val_accuracy: 0.3695\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.5546 - accuracy: 0.3449 - val_loss: 2.4781 - val_accuracy: 0.3653\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.5304 - accuracy: 0.3489 - val_loss: 2.4176 - val_accuracy: 0.3766\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.5120 - accuracy: 0.3549 - val_loss: 2.4807 - val_accuracy: 0.3676\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4874 - accuracy: 0.3575 - val_loss: 2.4022 - val_accuracy: 0.3804\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.4676 - accuracy: 0.3637 - val_loss: 2.4640 - val_accuracy: 0.3698\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.4491 - accuracy: 0.3652 - val_loss: 2.4441 - val_accuracy: 0.3736\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.4294 - accuracy: 0.3699 - val_loss: 2.3887 - val_accuracy: 0.3849\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.4134 - accuracy: 0.3755 - val_loss: 2.3329 - val_accuracy: 0.3926\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.3970 - accuracy: 0.3772 - val_loss: 2.3659 - val_accuracy: 0.3919\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3760 - accuracy: 0.3814 - val_loss: 2.3220 - val_accuracy: 0.3982\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3601 - accuracy: 0.3827 - val_loss: 2.3027 - val_accuracy: 0.4004\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 2.3508 - accuracy: 0.3889 - val_loss: 2.2914 - val_accuracy: 0.4053\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3338 - accuracy: 0.3913 - val_loss: 2.2511 - val_accuracy: 0.4149\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3205 - accuracy: 0.3942 - val_loss: 2.2724 - val_accuracy: 0.4146\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3074 - accuracy: 0.3938 - val_loss: 2.2269 - val_accuracy: 0.4194\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2849 - accuracy: 0.4028 - val_loss: 2.3126 - val_accuracy: 0.4052\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2756 - accuracy: 0.4019 - val_loss: 2.3253 - val_accuracy: 0.4091\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.2645 - accuracy: 0.4049 - val_loss: 2.2420 - val_accuracy: 0.4166\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.2534 - accuracy: 0.4090 - val_loss: 2.3665 - val_accuracy: 0.3959\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.2367 - accuracy: 0.4117 - val_loss: 2.1969 - val_accuracy: 0.4310\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2141 - accuracy: 0.4163 - val_loss: 2.2462 - val_accuracy: 0.4258\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2091 - accuracy: 0.4190 - val_loss: 2.2315 - val_accuracy: 0.4208\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1912 - accuracy: 0.4238 - val_loss: 2.2285 - val_accuracy: 0.4239\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1824 - accuracy: 0.4235 - val_loss: 2.2388 - val_accuracy: 0.4220\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1764 - accuracy: 0.4242 - val_loss: 2.2226 - val_accuracy: 0.4257\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.1649 - accuracy: 0.4263 - val_loss: 2.2365 - val_accuracy: 0.4196\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1493 - accuracy: 0.4310 - val_loss: 2.1518 - val_accuracy: 0.4396\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1463 - accuracy: 0.4318 - val_loss: 2.1976 - val_accuracy: 0.4299\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.1343 - accuracy: 0.4349 - val_loss: 2.1354 - val_accuracy: 0.4444\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1257 - accuracy: 0.4361 - val_loss: 2.1655 - val_accuracy: 0.4425\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1116 - accuracy: 0.4391 - val_loss: 2.1222 - val_accuracy: 0.4514\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1000 - accuracy: 0.4427 - val_loss: 2.2432 - val_accuracy: 0.4210\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0897 - accuracy: 0.4434 - val_loss: 2.1584 - val_accuracy: 0.4398\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.0750 - accuracy: 0.4477 - val_loss: 2.1225 - val_accuracy: 0.4492\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 2.0741 - accuracy: 0.4489 - val_loss: 2.1496 - val_accuracy: 0.4380\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0624 - accuracy: 0.4503 - val_loss: 2.1253 - val_accuracy: 0.4487\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0548 - accuracy: 0.4527 - val_loss: 2.1555 - val_accuracy: 0.4403\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0475 - accuracy: 0.4528 - val_loss: 2.2124 - val_accuracy: 0.4306\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0420 - accuracy: 0.4538 - val_loss: 2.1279 - val_accuracy: 0.4452\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0316 - accuracy: 0.4577 - val_loss: 2.1386 - val_accuracy: 0.4468\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.0201 - accuracy: 0.4609 - val_loss: 2.2245 - val_accuracy: 0.4273\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.0129 - accuracy: 0.4628 - val_loss: 2.0960 - val_accuracy: 0.4570\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0074 - accuracy: 0.4617 - val_loss: 2.1089 - val_accuracy: 0.4530\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.9966 - accuracy: 0.4647 - val_loss: 2.1238 - val_accuracy: 0.4502\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9881 - accuracy: 0.4667 - val_loss: 2.1115 - val_accuracy: 0.4531\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9827 - accuracy: 0.4678 - val_loss: 2.0954 - val_accuracy: 0.4566\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9763 - accuracy: 0.4704 - val_loss: 2.0830 - val_accuracy: 0.4577\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9733 - accuracy: 0.4708 - val_loss: 2.1498 - val_accuracy: 0.4436\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 1.9629 - accuracy: 0.4721 - val_loss: 2.0961 - val_accuracy: 0.4523\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9478 - accuracy: 0.4775 - val_loss: 2.0550 - val_accuracy: 0.4627\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9478 - accuracy: 0.4749 - val_loss: 2.0409 - val_accuracy: 0.4646\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9377 - accuracy: 0.4776 - val_loss: 2.0425 - val_accuracy: 0.4672\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9248 - accuracy: 0.4799 - val_loss: 2.0553 - val_accuracy: 0.4656\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9254 - accuracy: 0.4818 - val_loss: 2.0248 - val_accuracy: 0.4720\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9209 - accuracy: 0.4829 - val_loss: 2.0552 - val_accuracy: 0.4629\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9126 - accuracy: 0.4843 - val_loss: 2.0761 - val_accuracy: 0.4596\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8993 - accuracy: 0.4864 - val_loss: 2.0152 - val_accuracy: 0.4742\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9048 - accuracy: 0.4845 - val_loss: 2.0488 - val_accuracy: 0.4622\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.8899 - accuracy: 0.4883 - val_loss: 2.0288 - val_accuracy: 0.4720\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8917 - accuracy: 0.4881 - val_loss: 2.0438 - val_accuracy: 0.4680\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8825 - accuracy: 0.4904 - val_loss: 2.0606 - val_accuracy: 0.4607\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8782 - accuracy: 0.4929 - val_loss: 2.0311 - val_accuracy: 0.4708\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.8788 - accuracy: 0.4927 - val_loss: 2.0484 - val_accuracy: 0.4669\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8655 - accuracy: 0.4952 - val_loss: 1.9801 - val_accuracy: 0.4817\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8548 - accuracy: 0.4986 - val_loss: 2.0107 - val_accuracy: 0.4729\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8513 - accuracy: 0.4977 - val_loss: 2.0011 - val_accuracy: 0.4779\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8485 - accuracy: 0.4983 - val_loss: 2.0273 - val_accuracy: 0.4741\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8471 - accuracy: 0.4977 - val_loss: 2.0388 - val_accuracy: 0.4669\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8295 - accuracy: 0.5032 - val_loss: 2.0229 - val_accuracy: 0.4731\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8348 - accuracy: 0.5014 - val_loss: 1.9891 - val_accuracy: 0.4790\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8294 - accuracy: 0.5036 - val_loss: 2.0220 - val_accuracy: 0.4730\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8120 - accuracy: 0.5088 - val_loss: 1.9700 - val_accuracy: 0.4855\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8089 - accuracy: 0.5080 - val_loss: 2.0135 - val_accuracy: 0.4783\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.8134 - accuracy: 0.5065 - val_loss: 1.9821 - val_accuracy: 0.4796\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8024 - accuracy: 0.5081 - val_loss: 1.9770 - val_accuracy: 0.4814\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7936 - accuracy: 0.5110 - val_loss: 2.0005 - val_accuracy: 0.4752\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7902 - accuracy: 0.5106 - val_loss: 2.0120 - val_accuracy: 0.4773\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7919 - accuracy: 0.5118 - val_loss: 1.9652 - val_accuracy: 0.4881\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.7821 - accuracy: 0.5150 - val_loss: 1.9881 - val_accuracy: 0.4840\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7769 - accuracy: 0.5154 - val_loss: 2.0146 - val_accuracy: 0.4753\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7724 - accuracy: 0.5166 - val_loss: 1.9726 - val_accuracy: 0.4886\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7680 - accuracy: 0.5175 - val_loss: 1.9895 - val_accuracy: 0.4816\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7688 - accuracy: 0.5165 - val_loss: 1.9769 - val_accuracy: 0.4844\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7594 - accuracy: 0.5177 - val_loss: 1.9292 - val_accuracy: 0.4948\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7570 - accuracy: 0.5187 - val_loss: 1.9864 - val_accuracy: 0.4797\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7503 - accuracy: 0.5198 - val_loss: 1.9265 - val_accuracy: 0.4931\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7448 - accuracy: 0.5207 - val_loss: 1.9681 - val_accuracy: 0.4840\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7424 - accuracy: 0.5239 - val_loss: 1.9888 - val_accuracy: 0.4835\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7354 - accuracy: 0.5248 - val_loss: 1.9105 - val_accuracy: 0.5008\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7372 - accuracy: 0.5243 - val_loss: 1.9974 - val_accuracy: 0.4812\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7278 - accuracy: 0.5284 - val_loss: 1.9786 - val_accuracy: 0.4839\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7225 - accuracy: 0.5281 - val_loss: 1.9517 - val_accuracy: 0.4915\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7209 - accuracy: 0.5276 - val_loss: 1.9401 - val_accuracy: 0.4910\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7182 - accuracy: 0.5296 - val_loss: 1.9434 - val_accuracy: 0.4946\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7143 - accuracy: 0.5304 - val_loss: 1.9181 - val_accuracy: 0.4941\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.7078 - accuracy: 0.5313 - val_loss: 1.9424 - val_accuracy: 0.4960\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7043 - accuracy: 0.5318 - val_loss: 1.9066 - val_accuracy: 0.5024\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6993 - accuracy: 0.5328 - val_loss: 1.9858 - val_accuracy: 0.4865\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6966 - accuracy: 0.5332 - val_loss: 1.9202 - val_accuracy: 0.5027\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.6896 - accuracy: 0.5362 - val_loss: 1.9543 - val_accuracy: 0.4933\n",
      "Epoch 123/200\n",
      " 965/1563 [=================>............] - ETA: 11s - loss: 1.6907 - accuracy: 0.5349"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train_2X, y_train_2X,\n",
    "epochs=200,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
