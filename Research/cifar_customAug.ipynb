{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D,Activation\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_datasets as tfds\n",
    "import keras_cv\n",
    "from keras_cv import utils\n",
    "from keras_cv.layers import BaseImageAugmentationLayer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "import cv2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n",
    "assert x_train.shape == (50000, 32, 32, 3)\n",
    "assert x_test.shape == (10000, 32, 32, 3)\n",
    "assert y_train.shape == (50000, 1)\n",
    "assert y_test.shape == (10000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomColorDistortion(tf.keras.layers.Layer):\n",
    "    def __init__(self, contrast_range=[0.5, 1.5], \n",
    "        brightness_delta=[-0.2, 0.2], **kwargs):\n",
    "        super(RandomColorDistortion, self).__init__(**kwargs)\n",
    "        self.contrast_range = contrast_range\n",
    "        self.brightness_delta = brightness_delta\n",
    "        self.auto_vectorize = False\n",
    "    \n",
    "    def call(self, images, training=None):\n",
    "        if not training:\n",
    "            return images\n",
    "        \n",
    "        contrast = np.random.uniform(\n",
    "            self.contrast_range[0], self.contrast_range[1])\n",
    "        brightness = np.random.uniform(\n",
    "            self.brightness_delta[0], self.brightness_delta[1])\n",
    "        \n",
    "        images = tf.image.adjust_contrast(images, contrast)\n",
    "        images = tf.image.adjust_brightness(images, brightness)\n",
    "        images = tf.clip_by_value(images, 0, 1)\n",
    "        return images\n",
    "\n",
    "class random_zoom_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, probability=0.5, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.probability = probability\n",
    "        self.layer = tf.keras.layers.RandomZoom(height_factor=(-0.1, 0.1), width_factor=(-0.1, 0.1), fill_mode='constant')\n",
    "\n",
    "    def call(self, x):\n",
    "        return tf.cond(tf.less(tf.random.uniform([]), self.probability), lambda: self.layer(x), lambda: x)\n",
    "\n",
    "class RandomMultiply(keras_cv.layers.BaseImageAugmentationLayer):\n",
    "    \"\"\"RandomBlueTint randomly applies a blue tint to images.\n",
    "\n",
    "    Args:\n",
    "      factor: A tuple of two floats, a single float or a\n",
    "        `keras_cv.FactorSampler`. `factor` controls the extent to which the\n",
    "        image is blue shifted. `factor=0.0` makes this layer perform a no-op\n",
    "        operation, while a value of 1.0 uses the degenerated result entirely.\n",
    "        Values between 0 and 1 result in linear interpolation between the original\n",
    "        image and a fully blue image.\n",
    "        Values should be between `0.0` and `1.0`.  If a tuple is used, a `factor` is\n",
    "        sampled between the two values for every image augmented.  If a single float\n",
    "        is used, a value between `0.0` and the passed float is sampled.  In order to\n",
    "        ensure the value is always the same, please pass a tuple with two identical\n",
    "        floats: `(0.5, 0.5)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, factor, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.factor = utils.parse_factor(factor)\n",
    "        # this disables BaseImageAugmentationLayer's Auto Vectorization\n",
    "        self.auto_vectorize = False\n",
    "\n",
    "    def get_random_transformation(self, **kwargs):\n",
    "        # kwargs holds {\"images\": image, \"labels\": label, etc...}\n",
    "        return self.factor()\n",
    "\n",
    "    def augment_image(self, image, transformation=None, **kwargs):\n",
    "        image = image / 255\n",
    "        image1 = image * image\n",
    "        image = image * 255\n",
    "        # [*others, blue] = tf.unstack(image, axis=-1)\n",
    "        tf.clip_by_value(image, 0.0, 255.0)\n",
    "        return image\n",
    "\n",
    "    def augment_bounding_boxes(self, bounding_boxes, transformation=None, **kwargs):\n",
    "        # you can also perform no-op augmentations on label types to support them in\n",
    "        # your pipeline.\n",
    "        return bounding_boxes\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        # RandomMultiply(factor=1),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_3 (Rescaling)        (None, 32, 32, 3)    0           ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_3[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_24 (BatchN  (None, 28, 28, 32)  128         ['conv2d_15[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_24 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_24[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_24 (Separable  (None, 28, 28, 32)  1312        ['activation_24[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_25 (BatchN  (None, 28, 28, 32)  128         ['separable_conv2d_24[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_25 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_25[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_25 (Separable  (None, 28, 28, 32)  1312        ['activation_25[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_12 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_25[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " add_12 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_12[0][0]',       \n",
      "                                                                  'conv2d_16[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_26 (BatchN  (None, 14, 14, 32)  128         ['add_12[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_26 (Activation)     (None, 14, 14, 32)   0           ['batch_normalization_26[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_26 (Separable  (None, 14, 14, 64)  2336        ['activation_26[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_27 (BatchN  (None, 14, 14, 64)  256         ['separable_conv2d_26[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_27 (Activation)     (None, 14, 14, 64)   0           ['batch_normalization_27[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_27 (Separable  (None, 14, 14, 64)  4672        ['activation_27[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_13 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_27[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 7, 7, 64)     2048        ['add_12[0][0]']                 \n",
      "                                                                                                  \n",
      " add_13 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_13[0][0]',       \n",
      "                                                                  'conv2d_17[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_28 (BatchN  (None, 7, 7, 64)    256         ['add_13[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_28 (Activation)     (None, 7, 7, 64)     0           ['batch_normalization_28[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_28 (Separable  (None, 7, 7, 128)   8768        ['activation_28[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_29 (BatchN  (None, 7, 7, 128)   512         ['separable_conv2d_28[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_29 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_29[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_29 (Separable  (None, 7, 7, 128)   17536       ['activation_29[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_14 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_29[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 4, 4, 128)    8192        ['add_13[0][0]']                 \n",
      "                                                                                                  \n",
      " add_14 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_14[0][0]',       \n",
      "                                                                  'conv2d_18[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_30 (BatchN  (None, 4, 4, 128)   512         ['add_14[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_30 (Activation)     (None, 4, 4, 128)    0           ['batch_normalization_30[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_30 (Separable  (None, 4, 4, 256)   33920       ['activation_30[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_31 (BatchN  (None, 4, 4, 256)   1024        ['separable_conv2d_30[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_31 (Activation)     (None, 4, 4, 256)    0           ['batch_normalization_31[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_31 (Separable  (None, 4, 4, 256)   67840       ['activation_31[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_15 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_31[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 2, 2, 256)    32768       ['add_14[0][0]']                 \n",
      "                                                                                                  \n",
      " add_15 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_15[0][0]',       \n",
      "                                                                  'conv2d_19[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 1024)         0           ['add_15[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 1024)         0           ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 256)          262400      ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 256)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 128)          32896       ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128)          0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 100)          12900       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"test.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-13 15:00:31.155425: W tensorflow/core/common_runtime/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 128.0KiB (rounded to 131072)requested by op Fill\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2022-10-13 15:00:31.155465: I tensorflow/core/common_runtime/bfc_allocator.cc:1027] BFCAllocator dump for GPU_0_bfc\n",
      "2022-10-13 15:00:31.155476: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (256): \tTotal Chunks: 239, Chunks in use: 239. 59.8KiB allocated for chunks. 59.8KiB in use in bin. 26.7KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155483: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (512): \tTotal Chunks: 80, Chunks in use: 80. 41.5KiB allocated for chunks. 41.5KiB in use in bin. 38.9KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155489: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (1024): \tTotal Chunks: 75, Chunks in use: 75. 86.8KiB allocated for chunks. 86.8KiB in use in bin. 79.1KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155495: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (2048): \tTotal Chunks: 21, Chunks in use: 21. 47.2KiB allocated for chunks. 47.2KiB in use in bin. 47.2KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155501: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (4096): \tTotal Chunks: 56, Chunks in use: 55. 260.8KiB allocated for chunks. 254.2KiB in use in bin. 228.8KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155506: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (8192): \tTotal Chunks: 45, Chunks in use: 45. 413.0KiB allocated for chunks. 413.0KiB in use in bin. 382.6KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155512: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (16384): \tTotal Chunks: 11, Chunks in use: 11. 176.0KiB allocated for chunks. 176.0KiB in use in bin. 176.0KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155518: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (32768): \tTotal Chunks: 29, Chunks in use: 28. 1.04MiB allocated for chunks. 1015.5KiB in use in bin. 1004.0KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155524: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (65536): \tTotal Chunks: 15, Chunks in use: 15. 1016.0KiB allocated for chunks. 1016.0KiB in use in bin. 904.0KiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155529: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (131072): \tTotal Chunks: 32, Chunks in use: 32. 4.00MiB allocated for chunks. 4.00MiB in use in bin. 4.00MiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155536: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (262144): \tTotal Chunks: 16, Chunks in use: 16. 4.66MiB allocated for chunks. 4.66MiB in use in bin. 4.66MiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155546: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (524288): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155558: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (1048576): \tTotal Chunks: 11, Chunks in use: 11. 11.47MiB allocated for chunks. 11.47MiB in use in bin. 11.00MiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155567: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (2097152): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155575: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (4194304): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155584: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (8388608): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155591: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (16777216): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155596: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (33554432): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155603: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (67108864): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155610: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (134217728): \tTotal Chunks: 5, Chunks in use: 5. 791.52MiB allocated for chunks. 791.52MiB in use in bin. 732.42MiB client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155615: I tensorflow/core/common_runtime/bfc_allocator.cc:1034] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2022-10-13 15:00:31.155632: I tensorflow/core/common_runtime/bfc_allocator.cc:1050] Bin for 128.0KiB was 128.0KiB, Chunk State: \n",
      "2022-10-13 15:00:31.155638: I tensorflow/core/common_runtime/bfc_allocator.cc:1063] Next region of size 854327296\n",
      "2022-10-13 15:00:31.155646: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000000 of size 1280 next 1\n",
      "2022-10-13 15:00:31.155651: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000500 of size 256 next 2\n",
      "2022-10-13 15:00:31.155656: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000600 of size 256 next 3\n",
      "2022-10-13 15:00:31.155660: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000700 of size 256 next 4\n",
      "2022-10-13 15:00:31.155665: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000800 of size 256 next 5\n",
      "2022-10-13 15:00:31.155669: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000900 of size 256 next 6\n",
      "2022-10-13 15:00:31.155673: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000a00 of size 256 next 7\n",
      "2022-10-13 15:00:31.155678: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000b00 of size 256 next 10\n",
      "2022-10-13 15:00:31.155682: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000c00 of size 256 next 11\n",
      "2022-10-13 15:00:31.155686: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000d00 of size 256 next 12\n",
      "2022-10-13 15:00:31.155691: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000e00 of size 256 next 13\n",
      "2022-10-13 15:00:31.155695: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08000f00 of size 256 next 14\n",
      "2022-10-13 15:00:31.155700: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001000 of size 256 next 15\n",
      "2022-10-13 15:00:31.155705: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001100 of size 256 next 16\n",
      "2022-10-13 15:00:31.155709: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001200 of size 256 next 17\n",
      "2022-10-13 15:00:31.155713: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001300 of size 256 next 20\n",
      "2022-10-13 15:00:31.155718: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001400 of size 256 next 21\n",
      "2022-10-13 15:00:31.155723: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001500 of size 256 next 23\n",
      "2022-10-13 15:00:31.155728: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001600 of size 256 next 24\n",
      "2022-10-13 15:00:31.155732: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001700 of size 256 next 30\n",
      "2022-10-13 15:00:31.155737: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001800 of size 256 next 31\n",
      "2022-10-13 15:00:31.155742: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001900 of size 256 next 32\n",
      "2022-10-13 15:00:31.155747: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001a00 of size 256 next 33\n",
      "2022-10-13 15:00:31.155752: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001b00 of size 256 next 18\n",
      "2022-10-13 15:00:31.155758: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08001c00 of size 1280 next 19\n",
      "2022-10-13 15:00:31.155762: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002100 of size 256 next 42\n",
      "2022-10-13 15:00:31.155766: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002200 of size 256 next 43\n",
      "2022-10-13 15:00:31.155768: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002300 of size 256 next 44\n",
      "2022-10-13 15:00:31.155771: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002400 of size 256 next 46\n",
      "2022-10-13 15:00:31.155773: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002500 of size 256 next 51\n",
      "2022-10-13 15:00:31.155775: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002600 of size 256 next 26\n",
      "2022-10-13 15:00:31.155778: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002700 of size 1280 next 27\n",
      "2022-10-13 15:00:31.155781: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002c00 of size 512 next 57\n",
      "2022-10-13 15:00:31.155786: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08002e00 of size 512 next 60\n",
      "2022-10-13 15:00:31.155791: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003000 of size 512 next 61\n",
      "2022-10-13 15:00:31.155795: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003200 of size 256 next 62\n",
      "2022-10-13 15:00:31.155801: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003300 of size 256 next 63\n",
      "2022-10-13 15:00:31.155805: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003400 of size 256 next 66\n",
      "2022-10-13 15:00:31.155810: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003500 of size 256 next 67\n",
      "2022-10-13 15:00:31.155815: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003600 of size 512 next 70\n",
      "2022-10-13 15:00:31.155820: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003800 of size 512 next 72\n",
      "2022-10-13 15:00:31.155825: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003a00 of size 512 next 73\n",
      "2022-10-13 15:00:31.155830: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003c00 of size 512 next 74\n",
      "2022-10-13 15:00:31.155834: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003e00 of size 256 next 75\n",
      "2022-10-13 15:00:31.155836: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08003f00 of size 256 next 76\n",
      "2022-10-13 15:00:31.155838: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08004000 of size 256 next 22\n",
      "2022-10-13 15:00:31.155841: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08004100 of size 5376 next 8\n",
      "2022-10-13 15:00:31.155843: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08005600 of size 9728 next 9\n",
      "2022-10-13 15:00:31.155846: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08007c00 of size 256 next 36\n",
      "2022-10-13 15:00:31.155848: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08007d00 of size 256 next 37\n",
      "2022-10-13 15:00:31.155850: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08007e00 of size 256 next 40\n",
      "2022-10-13 15:00:31.155853: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08007f00 of size 256 next 41\n",
      "2022-10-13 15:00:31.155855: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008000 of size 256 next 34\n",
      "2022-10-13 15:00:31.155858: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008100 of size 1280 next 35\n",
      "2022-10-13 15:00:31.155862: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008600 of size 256 next 52\n",
      "2022-10-13 15:00:31.155867: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008700 of size 256 next 53\n",
      "2022-10-13 15:00:31.155874: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008800 of size 256 next 54\n",
      "2022-10-13 15:00:31.155879: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008900 of size 256 next 56\n",
      "2022-10-13 15:00:31.155884: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008a00 of size 512 next 28\n",
      "2022-10-13 15:00:31.155890: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08008c00 of size 4096 next 25\n",
      "2022-10-13 15:00:31.155895: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08009c00 of size 4096 next 29\n",
      "2022-10-13 15:00:31.155900: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800ac00 of size 2304 next 45\n",
      "2022-10-13 15:00:31.155905: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800b500 of size 1024 next 77\n",
      "2022-10-13 15:00:31.155909: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800b900 of size 1024 next 80\n",
      "2022-10-13 15:00:31.155912: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800bd00 of size 1024 next 81\n",
      "2022-10-13 15:00:31.155914: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800c100 of size 1536 next 65\n",
      "2022-10-13 15:00:31.155917: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800c700 of size 4608 next 64\n",
      "2022-10-13 15:00:31.155919: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800d900 of size 256 next 82\n",
      "2022-10-13 15:00:31.155921: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800da00 of size 256 next 85\n",
      "2022-10-13 15:00:31.155924: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800db00 of size 256 next 86\n",
      "2022-10-13 15:00:31.155926: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800dc00 of size 256 next 89\n",
      "2022-10-13 15:00:31.155930: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800dd00 of size 256 next 91\n",
      "2022-10-13 15:00:31.155935: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800de00 of size 1024 next 92\n",
      "2022-10-13 15:00:31.155941: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e200 of size 512 next 95\n",
      "2022-10-13 15:00:31.155945: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e400 of size 256 next 97\n",
      "2022-10-13 15:00:31.155950: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e500 of size 256 next 98\n",
      "2022-10-13 15:00:31.155955: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e600 of size 512 next 99\n",
      "2022-10-13 15:00:31.155960: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e800 of size 256 next 101\n",
      "2022-10-13 15:00:31.155965: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800e900 of size 256 next 102\n",
      "2022-10-13 15:00:31.155970: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800ea00 of size 256 next 103\n",
      "2022-10-13 15:00:31.155972: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800eb00 of size 256 next 39\n",
      "2022-10-13 15:00:31.155975: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0800ec00 of size 8192 next 38\n",
      "2022-10-13 15:00:31.155977: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08010c00 of size 2304 next 55\n",
      "2022-10-13 15:00:31.155980: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08011500 of size 5888 next 50\n",
      "2022-10-13 15:00:31.155982: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08012c00 of size 8192 next 47\n",
      "2022-10-13 15:00:31.155984: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08014c00 of size 256 next 116\n",
      "2022-10-13 15:00:31.155987: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08014d00 of size 256 next 117\n",
      "2022-10-13 15:00:31.155991: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08014e00 of size 1280 next 118\n",
      "2022-10-13 15:00:31.155996: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08015300 of size 4096 next 119\n",
      "2022-10-13 15:00:31.156001: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08016300 of size 4096 next 120\n",
      "2022-10-13 15:00:31.156006: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017300 of size 256 next 121\n",
      "2022-10-13 15:00:31.156011: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017400 of size 256 next 122\n",
      "2022-10-13 15:00:31.156016: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017500 of size 1280 next 123\n",
      "2022-10-13 15:00:31.156021: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017a00 of size 256 next 125\n",
      "2022-10-13 15:00:31.156026: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017b00 of size 256 next 126\n",
      "2022-10-13 15:00:31.156030: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08017c00 of size 4096 next 49\n",
      "2022-10-13 15:00:31.156032: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08018c00 of size 16384 next 48\n",
      "2022-10-13 15:00:31.156035: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0801cc00 of size 32768 next 132\n",
      "2022-10-13 15:00:31.156037: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08024c00 of size 32768 next 59\n",
      "2022-10-13 15:00:31.156040: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0802cc00 of size 32768 next 58\n",
      "2022-10-13 15:00:31.156042: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08034c00 of size 32768 next 71\n",
      "2022-10-13 15:00:31.156044: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803cc00 of size 256 next 106\n",
      "2022-10-13 15:00:31.156047: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803cd00 of size 256 next 107\n",
      "2022-10-13 15:00:31.156049: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803ce00 of size 256 next 108\n",
      "2022-10-13 15:00:31.156053: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803cf00 of size 256 next 109\n",
      "2022-10-13 15:00:31.156058: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d000 of size 256 next 110\n",
      "2022-10-13 15:00:31.156063: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d100 of size 256 next 111\n",
      "2022-10-13 15:00:31.156068: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d200 of size 256 next 112\n",
      "2022-10-13 15:00:31.156072: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d300 of size 256 next 113\n",
      "2022-10-13 15:00:31.156078: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d400 of size 256 next 114\n",
      "2022-10-13 15:00:31.156083: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803d500 of size 1280 next 115\n",
      "2022-10-13 15:00:31.156088: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803da00 of size 5632 next 84\n",
      "2022-10-13 15:00:31.156092: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0803f000 of size 9216 next 83\n",
      "2022-10-13 15:00:31.156094: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08041400 of size 14336 next 69\n",
      "2022-10-13 15:00:31.156097: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08044c00 of size 65536 next 68\n",
      "2022-10-13 15:00:31.156099: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08054c00 of size 8192 next 124\n",
      "2022-10-13 15:00:31.156101: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08056c00 of size 16384 next 127\n",
      "2022-10-13 15:00:31.156104: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805ac00 of size 8192 next 128\n",
      "2022-10-13 15:00:31.156107: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805cc00 of size 256 next 129\n",
      "2022-10-13 15:00:31.156111: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805cd00 of size 256 next 130\n",
      "2022-10-13 15:00:31.156117: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805ce00 of size 2304 next 131\n",
      "2022-10-13 15:00:31.156122: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805d700 of size 512 next 133\n",
      "2022-10-13 15:00:31.156127: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805d900 of size 512 next 134\n",
      "2022-10-13 15:00:31.156132: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805db00 of size 4608 next 135\n",
      "2022-10-13 15:00:31.156137: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805ed00 of size 512 next 137\n",
      "2022-10-13 15:00:31.156142: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805ef00 of size 512 next 138\n",
      "2022-10-13 15:00:31.156147: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd0805f100 of size 8960 next 100\n",
      "2022-10-13 15:00:31.156150: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08061400 of size 79872 next 96\n",
      "2022-10-13 15:00:31.156153: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08074c00 of size 131072 next 78\n",
      "2022-10-13 15:00:31.156155: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08094c00 of size 131072 next 79\n",
      "2022-10-13 15:00:31.156157: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080b4c00 of size 131072 next 90\n",
      "2022-10-13 15:00:31.156160: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080d4c00 of size 65536 next 136\n",
      "2022-10-13 15:00:31.156162: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e4c00 of size 1024 next 140\n",
      "2022-10-13 15:00:31.156166: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e5000 of size 1024 next 141\n",
      "2022-10-13 15:00:31.156171: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e5400 of size 9216 next 142\n",
      "2022-10-13 15:00:31.156176: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e7800 of size 1024 next 145\n",
      "2022-10-13 15:00:31.156181: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e7c00 of size 512 next 147\n",
      "2022-10-13 15:00:31.156186: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080e7e00 of size 52736 next 88\n",
      "2022-10-13 15:00:31.156191: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd080f4c00 of size 262144 next 87\n",
      "2022-10-13 15:00:31.156197: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08134c00 of size 400128 next 105\n",
      "2022-10-13 15:00:31.156202: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08196700 of size 131072 next 139\n",
      "2022-10-13 15:00:31.156207: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd081b6700 of size 262144 next 143\n",
      "2022-10-13 15:00:31.156210: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd081f6700 of size 131072 next 144\n",
      "2022-10-13 15:00:31.156213: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08216700 of size 1172736 next 94\n",
      "2022-10-13 15:00:31.156215: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08334c00 of size 1048576 next 93\n",
      "2022-10-13 15:00:31.156218: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd08434c00 of size 153600000 next 104\n",
      "2022-10-13 15:00:31.156220: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116b0c00 of size 131072 next 146\n",
      "2022-10-13 15:00:31.156222: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d0c00 of size 512 next 148\n",
      "2022-10-13 15:00:31.156225: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d0e00 of size 9728 next 149\n",
      "2022-10-13 15:00:31.156227: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d3400 of size 256 next 150\n",
      "2022-10-13 15:00:31.156229: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d3500 of size 256 next 151\n",
      "2022-10-13 15:00:31.156234: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d3600 of size 1280 next 152\n",
      "2022-10-13 15:00:31.156239: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d3b00 of size 4096 next 153\n",
      "2022-10-13 15:00:31.156244: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d4b00 of size 256 next 154\n",
      "2022-10-13 15:00:31.156249: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d4c00 of size 256 next 155\n",
      "2022-10-13 15:00:31.156254: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d4d00 of size 1280 next 156\n",
      "2022-10-13 15:00:31.156259: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d5200 of size 4096 next 157\n",
      "2022-10-13 15:00:31.156264: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d6200 of size 4096 next 158\n",
      "2022-10-13 15:00:31.156273: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d7200 of size 256 next 159\n",
      "2022-10-13 15:00:31.156278: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d7300 of size 256 next 160\n",
      "2022-10-13 15:00:31.156281: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d7400 of size 1280 next 161\n",
      "2022-10-13 15:00:31.156283: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d7900 of size 8192 next 162\n",
      "2022-10-13 15:00:31.156286: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d9900 of size 256 next 163\n",
      "2022-10-13 15:00:31.156288: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d9a00 of size 256 next 164\n",
      "2022-10-13 15:00:31.156290: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116d9b00 of size 2304 next 165\n",
      "2022-10-13 15:00:31.156293: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116da400 of size 16384 next 166\n",
      "2022-10-13 15:00:31.156295: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116de400 of size 8192 next 167\n",
      "2022-10-13 15:00:31.156299: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e0400 of size 256 next 168\n",
      "2022-10-13 15:00:31.156304: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e0500 of size 256 next 169\n",
      "2022-10-13 15:00:31.156309: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e0600 of size 2304 next 170\n",
      "2022-10-13 15:00:31.156314: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e0f00 of size 32768 next 171\n",
      "2022-10-13 15:00:31.156319: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e8f00 of size 512 next 172\n",
      "2022-10-13 15:00:31.156324: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e9100 of size 512 next 173\n",
      "2022-10-13 15:00:31.156329: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116e9300 of size 4608 next 174\n",
      "2022-10-13 15:00:31.156334: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116ea500 of size 65536 next 175\n",
      "2022-10-13 15:00:31.156339: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd116fa500 of size 32768 next 176\n",
      "2022-10-13 15:00:31.156343: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11702500 of size 512 next 177\n",
      "2022-10-13 15:00:31.156345: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11702700 of size 512 next 178\n",
      "2022-10-13 15:00:31.156347: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11702900 of size 4608 next 179\n",
      "2022-10-13 15:00:31.156350: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11703b00 of size 131072 next 180\n",
      "2022-10-13 15:00:31.156352: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11723b00 of size 1024 next 181\n",
      "2022-10-13 15:00:31.156354: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11723f00 of size 1024 next 182\n",
      "2022-10-13 15:00:31.156357: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11724300 of size 9216 next 183\n",
      "2022-10-13 15:00:31.156362: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11726700 of size 262144 next 184\n",
      "2022-10-13 15:00:31.156367: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11766700 of size 131072 next 185\n",
      "2022-10-13 15:00:31.156372: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11786700 of size 1048576 next 186\n",
      "2022-10-13 15:00:31.156377: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11886700 of size 1024 next 187\n",
      "2022-10-13 15:00:31.156383: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11886b00 of size 131072 next 188\n",
      "2022-10-13 15:00:31.156388: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118a6b00 of size 512 next 189\n",
      "2022-10-13 15:00:31.156393: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118a6d00 of size 51200 next 190\n",
      "2022-10-13 15:00:31.156399: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3500 of size 512 next 191\n",
      "2022-10-13 15:00:31.156402: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3700 of size 256 next 192\n",
      "2022-10-13 15:00:31.156405: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3800 of size 256 next 193\n",
      "2022-10-13 15:00:31.156407: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3900 of size 256 next 194\n",
      "2022-10-13 15:00:31.156409: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3a00 of size 256 next 195\n",
      "2022-10-13 15:00:31.156412: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3b00 of size 256 next 196\n",
      "2022-10-13 15:00:31.156414: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3c00 of size 256 next 197\n",
      "2022-10-13 15:00:31.156416: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3d00 of size 256 next 198\n",
      "2022-10-13 15:00:31.156418: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3e00 of size 256 next 199\n",
      "2022-10-13 15:00:31.156422: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b3f00 of size 256 next 200\n",
      "2022-10-13 15:00:31.156427: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4000 of size 256 next 201\n",
      "2022-10-13 15:00:31.156431: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4100 of size 256 next 202\n",
      "2022-10-13 15:00:31.156436: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4200 of size 256 next 203\n",
      "2022-10-13 15:00:31.156441: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4300 of size 256 next 204\n",
      "2022-10-13 15:00:31.156446: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4400 of size 256 next 205\n",
      "2022-10-13 15:00:31.156451: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4500 of size 256 next 206\n",
      "2022-10-13 15:00:31.156456: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4600 of size 256 next 240\n",
      "2022-10-13 15:00:31.156461: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4700 of size 256 next 243\n",
      "2022-10-13 15:00:31.156464: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4800 of size 256 next 214\n",
      "2022-10-13 15:00:31.156467: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4900 of size 256 next 215\n",
      "2022-10-13 15:00:31.156469: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4a00 of size 256 next 275\n",
      "2022-10-13 15:00:31.156471: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4b00 of size 256 next 309\n",
      "2022-10-13 15:00:31.156474: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4c00 of size 256 next 307\n",
      "2022-10-13 15:00:31.156476: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4d00 of size 512 next 247\n",
      "2022-10-13 15:00:31.156478: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b4f00 of size 512 next 248\n",
      "2022-10-13 15:00:31.156481: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5100 of size 768 next 273\n",
      "2022-10-13 15:00:31.156485: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5400 of size 1280 next 274\n",
      "2022-10-13 15:00:31.156489: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5900 of size 256 next 267\n",
      "2022-10-13 15:00:31.156494: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5a00 of size 256 next 268\n",
      "2022-10-13 15:00:31.156499: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5b00 of size 256 next 302\n",
      "2022-10-13 15:00:31.156504: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5c00 of size 256 next 301\n",
      "2022-10-13 15:00:31.156509: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5d00 of size 256 next 263\n",
      "2022-10-13 15:00:31.156514: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5e00 of size 256 next 270\n",
      "2022-10-13 15:00:31.156519: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b5f00 of size 1280 next 271\n",
      "2022-10-13 15:00:31.156524: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b6400 of size 512 next 295\n",
      "2022-10-13 15:00:31.156527: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b6600 of size 512 next 291\n",
      "2022-10-13 15:00:31.156530: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b6800 of size 512 next 239\n",
      "2022-10-13 15:00:31.156532: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b6a00 of size 1024 next 238\n",
      "2022-10-13 15:00:31.156535: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b6e00 of size 1024 next 234\n",
      "2022-10-13 15:00:31.156537: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b7200 of size 1792 next 308\n",
      "2022-10-13 15:00:31.156540: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b7900 of size 6400 next 241\n",
      "2022-10-13 15:00:31.156543: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118b9200 of size 9728 next 242\n",
      "2022-10-13 15:00:31.156548: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bb800 of size 256 next 260\n",
      "2022-10-13 15:00:31.156552: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bb900 of size 256 next 300\n",
      "2022-10-13 15:00:31.156557: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bba00 of size 256 next 254\n",
      "2022-10-13 15:00:31.156562: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bbb00 of size 256 next 253\n",
      "2022-10-13 15:00:31.156568: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bbc00 of size 256 next 265\n",
      "2022-10-13 15:00:31.156573: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bbd00 of size 1280 next 264\n",
      "2022-10-13 15:00:31.156578: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bc200 of size 256 next 252\n",
      "2022-10-13 15:00:31.156583: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bc300 of size 512 next 251\n",
      "2022-10-13 15:00:31.156586: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bc500 of size 768 next 305\n",
      "2022-10-13 15:00:31.156589: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bc800 of size 4096 next 306\n",
      "2022-10-13 15:00:31.156591: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bd800 of size 4096 next 304\n",
      "2022-10-13 15:00:31.156593: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118be800 of size 2304 next 257\n",
      "2022-10-13 15:00:31.156596: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118bf100 of size 4608 next 296\n",
      "2022-10-13 15:00:31.156598: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c0300 of size 1024 next 235\n",
      "2022-10-13 15:00:31.156600: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c0700 of size 1024 next 290\n",
      "2022-10-13 15:00:31.156606: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c0b00 of size 512 next 286\n",
      "2022-10-13 15:00:31.156611: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c0d00 of size 512 next 226\n",
      "2022-10-13 15:00:31.156616: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c0f00 of size 256 next 225\n",
      "2022-10-13 15:00:31.156621: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1000 of size 256 next 285\n",
      "2022-10-13 15:00:31.156626: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1100 of size 256 next 283\n",
      "2022-10-13 15:00:31.156631: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1200 of size 256 next 221\n",
      "2022-10-13 15:00:31.156635: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1300 of size 256 next 220\n",
      "2022-10-13 15:00:31.156639: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1400 of size 256 next 237\n",
      "2022-10-13 15:00:31.156642: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c1500 of size 4864 next 261\n",
      "2022-10-13 15:00:31.156644: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c2800 of size 8192 next 262\n",
      "2022-10-13 15:00:31.156646: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c4800 of size 2304 next 294\n",
      "2022-10-13 15:00:31.156647: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c5100 of size 1280 next 222\n",
      "2022-10-13 15:00:31.156649: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c5600 of size 4608 next 255\n",
      "2022-10-13 15:00:31.156651: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c6800 of size 8192 next 299\n",
      "2022-10-13 15:00:31.156653: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c8800 of size 4096 next 218\n",
      "2022-10-13 15:00:31.156656: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c9800 of size 256 next 219\n",
      "2022-10-13 15:00:31.156659: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c9900 of size 256 next 217\n",
      "2022-10-13 15:00:31.156663: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c9a00 of size 1280 next 209\n",
      "2022-10-13 15:00:31.156667: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118c9f00 of size 10496 next 256\n",
      "2022-10-13 15:00:31.156671: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118cc800 of size 16384 next 258\n",
      "2022-10-13 15:00:31.156674: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118d0800 of size 32768 next 279\n",
      "2022-10-13 15:00:31.156678: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118d8800 of size 32768 next 297\n",
      "2022-10-13 15:00:31.156682: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118e0800 of size 32768 next 298\n",
      "2022-10-13 15:00:31.156685: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118e8800 of size 32768 next 244\n",
      "2022-10-13 15:00:31.156687: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118f0800 of size 1280 next 282\n",
      "2022-10-13 15:00:31.156689: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118f0d00 of size 7936 next 232\n",
      "2022-10-13 15:00:31.156691: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118f2c00 of size 9216 next 288\n",
      "2022-10-13 15:00:31.156692: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118f5000 of size 14336 next 246\n",
      "2022-10-13 15:00:31.156694: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd118f8800 of size 65536 next 245\n",
      "2022-10-13 15:00:31.156696: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11908800 of size 256 next 281\n",
      "2022-10-13 15:00:31.156698: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11908900 of size 256 next 216\n",
      "2022-10-13 15:00:31.156699: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11908a00 of size 2304 next 213\n",
      "2022-10-13 15:00:31.156702: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11909300 of size 16384 next 276\n",
      "2022-10-13 15:00:31.156706: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1190d300 of size 8192 next 211\n",
      "2022-10-13 15:00:31.156710: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1190f300 of size 256 next 212\n",
      "2022-10-13 15:00:31.156713: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1190f400 of size 256 next 278\n",
      "2022-10-13 15:00:31.156716: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1190f500 of size 2304 next 280\n",
      "2022-10-13 15:00:31.156718: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1190fe00 of size 512 next 272\n",
      "2022-10-13 15:00:31.156721: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11910000 of size 512 next 269\n",
      "2022-10-13 15:00:31.156725: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11910200 of size 4608 next 266\n",
      "2022-10-13 15:00:31.156729: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11911400 of size 512 next 250\n",
      "2022-10-13 15:00:31.156732: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11911600 of size 512 next 249\n",
      "2022-10-13 15:00:31.156736: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11911800 of size 4608 next 236\n",
      "2022-10-13 15:00:31.156740: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11912a00 of size 1024 next 208\n",
      "2022-10-13 15:00:31.156744: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11912e00 of size 1024 next 224\n",
      "2022-10-13 15:00:31.156747: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913200 of size 1024 next 303\n",
      "2022-10-13 15:00:31.156751: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913600 of size 512 next 311\n",
      "2022-10-13 15:00:31.156755: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913800 of size 512 next 312\n",
      "2022-10-13 15:00:31.156759: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913a00 of size 256 next 314\n",
      "2022-10-13 15:00:31.156762: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913b00 of size 256 next 315\n",
      "2022-10-13 15:00:31.156766: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11913c00 of size 1280 next 316\n",
      "2022-10-13 15:00:31.156769: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914100 of size 256 next 318\n",
      "2022-10-13 15:00:31.156772: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914200 of size 256 next 319\n",
      "2022-10-13 15:00:31.156774: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914300 of size 1280 next 320\n",
      "2022-10-13 15:00:31.156775: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914800 of size 256 next 323\n",
      "2022-10-13 15:00:31.156777: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914900 of size 256 next 324\n",
      "2022-10-13 15:00:31.156779: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11914a00 of size 1536 next 227\n",
      "2022-10-13 15:00:31.156781: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11915000 of size 79872 next 287\n",
      "2022-10-13 15:00:31.156782: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11928800 of size 131072 next 292\n",
      "2022-10-13 15:00:31.156786: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11948800 of size 131072 next 293\n",
      "2022-10-13 15:00:31.156789: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11968800 of size 131072 next 229\n",
      "2022-10-13 15:00:31.156793: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11988800 of size 65536 next 259\n",
      "2022-10-13 15:00:31.156796: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11998800 of size 9216 next 207\n",
      "2022-10-13 15:00:31.156800: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1199ac00 of size 56320 next 231\n",
      "2022-10-13 15:00:31.156804: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd119a8800 of size 262144 next 233\n",
      "2022-10-13 15:00:31.156808: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd119e8800 of size 400128 next 284\n",
      "2022-10-13 15:00:31.156812: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11a4a300 of size 131072 next 228\n",
      "2022-10-13 15:00:31.156815: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11a6a300 of size 262144 next 223\n",
      "2022-10-13 15:00:31.156817: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11aaa300 of size 131072 next 210\n",
      "2022-10-13 15:00:31.156819: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11aca300 of size 1172736 next 289\n",
      "2022-10-13 15:00:31.156821: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11be8800 of size 1048576 next 230\n",
      "2022-10-13 15:00:31.156823: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd11ce8800 of size 153600000 next 277\n",
      "2022-10-13 15:00:31.156824: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af64800 of size 131072 next 310\n",
      "2022-10-13 15:00:31.156826: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af84800 of size 9728 next 313\n",
      "2022-10-13 15:00:31.156829: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af86e00 of size 4096 next 317\n",
      "2022-10-13 15:00:31.156832: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af87e00 of size 4096 next 321\n",
      "2022-10-13 15:00:31.156836: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af88e00 of size 4096 next 322\n",
      "2022-10-13 15:00:31.156839: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af89e00 of size 8192 next 325\n",
      "2022-10-13 15:00:31.156843: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af8be00 of size 256 next 326\n",
      "2022-10-13 15:00:31.156847: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af8bf00 of size 256 next 327\n",
      "2022-10-13 15:00:31.156851: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af8c000 of size 2304 next 328\n",
      "2022-10-13 15:00:31.156854: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af8c900 of size 16384 next 329\n",
      "2022-10-13 15:00:31.156857: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af90900 of size 8192 next 330\n",
      "2022-10-13 15:00:31.156859: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af92900 of size 256 next 331\n",
      "2022-10-13 15:00:31.156861: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af92a00 of size 256 next 332\n",
      "2022-10-13 15:00:31.156863: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af92b00 of size 2304 next 333\n",
      "2022-10-13 15:00:31.156865: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af93400 of size 32768 next 334\n",
      "2022-10-13 15:00:31.156866: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af9b400 of size 512 next 335\n",
      "2022-10-13 15:00:31.156868: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af9b600 of size 512 next 336\n",
      "2022-10-13 15:00:31.156870: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af9b800 of size 4608 next 337\n",
      "2022-10-13 15:00:31.156872: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1af9ca00 of size 65536 next 338\n",
      "2022-10-13 15:00:31.156875: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afaca00 of size 32768 next 339\n",
      "2022-10-13 15:00:31.156879: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afb4a00 of size 512 next 340\n",
      "2022-10-13 15:00:31.156882: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afb4c00 of size 512 next 341\n",
      "2022-10-13 15:00:31.156886: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afb4e00 of size 4608 next 342\n",
      "2022-10-13 15:00:31.156891: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afb6000 of size 131072 next 343\n",
      "2022-10-13 15:00:31.156894: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afd6000 of size 1024 next 344\n",
      "2022-10-13 15:00:31.156898: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afd6400 of size 1024 next 345\n",
      "2022-10-13 15:00:31.156902: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afd6800 of size 9216 next 346\n",
      "2022-10-13 15:00:31.156905: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1afd8c00 of size 262144 next 347\n",
      "2022-10-13 15:00:31.156907: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b018c00 of size 131072 next 348\n",
      "2022-10-13 15:00:31.156909: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b038c00 of size 1048576 next 349\n",
      "2022-10-13 15:00:31.156911: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b138c00 of size 1024 next 350\n",
      "2022-10-13 15:00:31.156913: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b139000 of size 131072 next 351\n",
      "2022-10-13 15:00:31.156914: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b159000 of size 512 next 352\n",
      "2022-10-13 15:00:31.156916: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b159200 of size 51200 next 353\n",
      "2022-10-13 15:00:31.156919: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b165a00 of size 512 next 354\n",
      "2022-10-13 15:00:31.156922: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b165c00 of size 256 next 355\n",
      "2022-10-13 15:00:31.156926: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b165d00 of size 256 next 356\n",
      "2022-10-13 15:00:31.156929: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b165e00 of size 256 next 357\n",
      "2022-10-13 15:00:31.156933: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b165f00 of size 256 next 358\n",
      "2022-10-13 15:00:31.156937: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166000 of size 256 next 359\n",
      "2022-10-13 15:00:31.156941: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166100 of size 256 next 360\n",
      "2022-10-13 15:00:31.156944: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166200 of size 256 next 361\n",
      "2022-10-13 15:00:31.156948: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166300 of size 256 next 362\n",
      "2022-10-13 15:00:31.156951: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166400 of size 256 next 363\n",
      "2022-10-13 15:00:31.156952: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166500 of size 256 next 364\n",
      "2022-10-13 15:00:31.156954: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166600 of size 256 next 365\n",
      "2022-10-13 15:00:31.156956: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166700 of size 256 next 366\n",
      "2022-10-13 15:00:31.156958: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166800 of size 256 next 367\n",
      "2022-10-13 15:00:31.156959: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166900 of size 256 next 370\n",
      "2022-10-13 15:00:31.156961: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166a00 of size 256 next 371\n",
      "2022-10-13 15:00:31.156965: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166b00 of size 256 next 368\n",
      "2022-10-13 15:00:31.156968: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166c00 of size 256 next 425\n",
      "2022-10-13 15:00:31.156972: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166d00 of size 256 next 435\n",
      "2022-10-13 15:00:31.156976: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166e00 of size 256 next 436\n",
      "2022-10-13 15:00:31.156979: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b166f00 of size 256 next 433\n",
      "2022-10-13 15:00:31.156983: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167000 of size 256 next 437\n",
      "2022-10-13 15:00:31.156987: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167100 of size 256 next 469\n",
      "2022-10-13 15:00:31.156991: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167200 of size 512 next 450\n",
      "2022-10-13 15:00:31.156992: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167400 of size 512 next 407\n",
      "2022-10-13 15:00:31.156994: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167600 of size 768 next 434\n",
      "2022-10-13 15:00:31.156996: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167900 of size 1280 next 438\n",
      "2022-10-13 15:00:31.156998: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167e00 of size 256 next 430\n",
      "2022-10-13 15:00:31.156999: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b167f00 of size 256 next 466\n",
      "2022-10-13 15:00:31.157001: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168000 of size 256 next 447\n",
      "2022-10-13 15:00:31.157005: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168100 of size 256 next 462\n",
      "2022-10-13 15:00:31.157008: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168200 of size 256 next 421\n",
      "2022-10-13 15:00:31.157012: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168300 of size 256 next 432\n",
      "2022-10-13 15:00:31.157016: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168400 of size 1280 next 464\n",
      "2022-10-13 15:00:31.157020: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168900 of size 512 next 406\n",
      "2022-10-13 15:00:31.157023: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168b00 of size 512 next 403\n",
      "2022-10-13 15:00:31.157027: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168d00 of size 512 next 404\n",
      "2022-10-13 15:00:31.157030: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b168f00 of size 1024 next 402\n",
      "2022-10-13 15:00:31.157032: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b169300 of size 1024 next 399\n",
      "2022-10-13 15:00:31.157034: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b169700 of size 1792 next 470\n",
      "2022-10-13 15:00:31.157036: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b169e00 of size 6400 next 369\n",
      "2022-10-13 15:00:31.157037: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16b700 of size 9728 next 424\n",
      "2022-10-13 15:00:31.157039: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16dd00 of size 256 next 461\n",
      "2022-10-13 15:00:31.157041: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16de00 of size 256 next 418\n",
      "2022-10-13 15:00:31.157043: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16df00 of size 256 next 415\n",
      "2022-10-13 15:00:31.157046: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16e000 of size 256 next 413\n",
      "2022-10-13 15:00:31.157049: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16e100 of size 256 next 455\n",
      "2022-10-13 15:00:31.157053: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16e200 of size 1280 next 463\n",
      "2022-10-13 15:00:31.157055: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16e700 of size 256 next 458\n",
      "2022-10-13 15:00:31.157057: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16e800 of size 512 next 411\n",
      "2022-10-13 15:00:31.157061: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16ea00 of size 768 next 467\n",
      "2022-10-13 15:00:31.157064: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16ed00 of size 4096 next 465\n",
      "2022-10-13 15:00:31.157068: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b16fd00 of size 4096 next 429\n",
      "2022-10-13 15:00:31.157072: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b170d00 of size 2304 next 417\n",
      "2022-10-13 15:00:31.157075: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b171600 of size 4608 next 408\n",
      "2022-10-13 15:00:31.157079: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b172800 of size 1024 next 398\n",
      "2022-10-13 15:00:31.157083: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b172c00 of size 1024 next 395\n",
      "2022-10-13 15:00:31.157087: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173000 of size 512 next 443\n",
      "2022-10-13 15:00:31.157090: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173200 of size 512 next 390\n",
      "2022-10-13 15:00:31.157094: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173400 of size 256 next 387\n",
      "2022-10-13 15:00:31.157098: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173500 of size 256 next 442\n",
      "2022-10-13 15:00:31.157102: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173600 of size 256 next 384\n",
      "2022-10-13 15:00:31.157106: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173700 of size 256 next 445\n",
      "2022-10-13 15:00:31.157109: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173800 of size 256 next 382\n",
      "2022-10-13 15:00:31.157113: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173900 of size 256 next 452\n",
      "2022-10-13 15:00:31.157115: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b173a00 of size 4864 next 420\n",
      "2022-10-13 15:00:31.157116: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b174d00 of size 8192 next 422\n",
      "2022-10-13 15:00:31.157118: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b176d00 of size 2304 next 412\n",
      "2022-10-13 15:00:31.157120: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b177600 of size 1280 next 383\n",
      "2022-10-13 15:00:31.157122: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b177b00 of size 4608 next 414\n",
      "2022-10-13 15:00:31.157123: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b178d00 of size 8192 next 419\n",
      "2022-10-13 15:00:31.157126: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17ad00 of size 4096 next 440\n",
      "2022-10-13 15:00:31.157130: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17bd00 of size 256 next 441\n",
      "2022-10-13 15:00:31.157134: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17be00 of size 256 next 378\n",
      "2022-10-13 15:00:31.157138: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17bf00 of size 1280 next 379\n",
      "2022-10-13 15:00:31.157141: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17c400 of size 10496 next 459\n",
      "2022-10-13 15:00:31.157145: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b17ed00 of size 16384 next 460\n",
      "2022-10-13 15:00:31.157149: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b182d00 of size 32768 next 426\n",
      "2022-10-13 15:00:31.157153: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b18ad00 of size 32768 next 456\n",
      "2022-10-13 15:00:31.157156: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b192d00 of size 32768 next 457\n",
      "2022-10-13 15:00:31.157159: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b19ad00 of size 32768 next 453\n",
      "2022-10-13 15:00:31.157160: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1a2d00 of size 256 next 381\n",
      "2022-10-13 15:00:31.157162: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1a2e00 of size 1280 next 444\n",
      "2022-10-13 15:00:31.157164: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1a3300 of size 7680 next 396\n",
      "2022-10-13 15:00:31.157166: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1a5100 of size 9216 next 397\n",
      "2022-10-13 15:00:31.157168: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1a7500 of size 14336 next 405\n",
      "2022-10-13 15:00:31.157171: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1aad00 of size 65536 next 454\n",
      "2022-10-13 15:00:31.157175: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1bad00 of size 256 next 377\n",
      "2022-10-13 15:00:31.157178: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1bae00 of size 256 next 439\n",
      "2022-10-13 15:00:31.157181: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1baf00 of size 2304 next 375\n",
      "2022-10-13 15:00:31.157183: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1bb800 of size 16384 next 376\n",
      "2022-10-13 15:00:31.157185: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1bf800 of size 8192 next 374\n",
      "2022-10-13 15:00:31.157188: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c1800 of size 256 next 372\n",
      "2022-10-13 15:00:31.157192: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c1900 of size 256 next 373\n",
      "2022-10-13 15:00:31.157195: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c1a00 of size 2304 next 431\n",
      "2022-10-13 15:00:31.157199: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c2300 of size 512 next 427\n",
      "2022-10-13 15:00:31.157203: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c2500 of size 512 next 428\n",
      "2022-10-13 15:00:31.157207: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c2700 of size 4608 next 423\n",
      "2022-10-13 15:00:31.157210: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c3900 of size 512 next 410\n",
      "2022-10-13 15:00:31.157214: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c3b00 of size 512 next 409\n",
      "2022-10-13 15:00:31.157218: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c3d00 of size 4608 next 393\n",
      "2022-10-13 15:00:31.157222: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c4f00 of size 1024 next 391\n",
      "2022-10-13 15:00:31.157226: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c5300 of size 1024 next 380\n",
      "2022-10-13 15:00:31.157229: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c5700 of size 1024 next 473\n",
      "2022-10-13 15:00:31.157233: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c5b00 of size 512 next 475\n",
      "2022-10-13 15:00:31.157237: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c5d00 of size 512 next 476\n",
      "2022-10-13 15:00:31.157239: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c5f00 of size 256 next 478\n",
      "2022-10-13 15:00:31.157241: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6000 of size 256 next 479\n",
      "2022-10-13 15:00:31.157243: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6100 of size 1280 next 480\n",
      "2022-10-13 15:00:31.157245: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6600 of size 256 next 482\n",
      "2022-10-13 15:00:31.157246: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6700 of size 256 next 483\n",
      "2022-10-13 15:00:31.157248: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6800 of size 1280 next 484\n",
      "2022-10-13 15:00:31.157252: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6d00 of size 256 next 487\n",
      "2022-10-13 15:00:31.157255: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6e00 of size 256 next 488\n",
      "2022-10-13 15:00:31.157259: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c6f00 of size 1536 next 388\n",
      "2022-10-13 15:00:31.157262: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1c7500 of size 79872 next 389\n",
      "2022-10-13 15:00:31.157266: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1dad00 of size 131072 next 401\n",
      "2022-10-13 15:00:31.157270: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b1fad00 of size 131072 next 400\n",
      "2022-10-13 15:00:31.157274: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b21ad00 of size 131072 next 449\n",
      "2022-10-13 15:00:31.157277: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b23ad00 of size 65536 next 416\n",
      "2022-10-13 15:00:31.157280: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b24ad00 of size 9216 next 468\n",
      "2022-10-13 15:00:31.157281: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b24d100 of size 56320 next 394\n",
      "2022-10-13 15:00:31.157283: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b25ad00 of size 262144 next 451\n",
      "2022-10-13 15:00:31.157285: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b29ad00 of size 400128 next 386\n",
      "2022-10-13 15:00:31.157287: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b2fc800 of size 131072 next 392\n",
      "2022-10-13 15:00:31.157288: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b31c800 of size 262144 next 471\n",
      "2022-10-13 15:00:31.157292: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b35c800 of size 131072 next 472\n",
      "2022-10-13 15:00:31.157296: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b37c800 of size 1172736 next 448\n",
      "2022-10-13 15:00:31.157299: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b49ad00 of size 1048576 next 446\n",
      "2022-10-13 15:00:31.157303: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd1b59ad00 of size 153600000 next 385\n",
      "2022-10-13 15:00:31.157307: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24816d00 of size 131072 next 474\n",
      "2022-10-13 15:00:31.157311: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24836d00 of size 9728 next 477\n",
      "2022-10-13 15:00:31.157314: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24839300 of size 4096 next 481\n",
      "2022-10-13 15:00:31.157317: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483a300 of size 4096 next 485\n",
      "2022-10-13 15:00:31.157319: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483b300 of size 4096 next 486\n",
      "2022-10-13 15:00:31.157321: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483c300 of size 8192 next 489\n",
      "2022-10-13 15:00:31.157323: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483e300 of size 256 next 490\n",
      "2022-10-13 15:00:31.157324: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483e400 of size 256 next 491\n",
      "2022-10-13 15:00:31.157326: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483e500 of size 2304 next 492\n",
      "2022-10-13 15:00:31.157328: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2483ee00 of size 16384 next 493\n",
      "2022-10-13 15:00:31.157329: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24842e00 of size 8192 next 494\n",
      "2022-10-13 15:00:31.157333: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24844e00 of size 256 next 495\n",
      "2022-10-13 15:00:31.157337: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24844f00 of size 256 next 496\n",
      "2022-10-13 15:00:31.157341: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24845000 of size 2304 next 497\n",
      "2022-10-13 15:00:31.157344: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24845900 of size 32768 next 498\n",
      "2022-10-13 15:00:31.157348: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2484d900 of size 512 next 499\n",
      "2022-10-13 15:00:31.157352: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2484db00 of size 512 next 500\n",
      "2022-10-13 15:00:31.157355: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2484dd00 of size 4608 next 501\n",
      "2022-10-13 15:00:31.157360: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2484ef00 of size 65536 next 502\n",
      "2022-10-13 15:00:31.157363: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2485ef00 of size 32768 next 503\n",
      "2022-10-13 15:00:31.157365: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24866f00 of size 512 next 504\n",
      "2022-10-13 15:00:31.157366: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24867100 of size 512 next 505\n",
      "2022-10-13 15:00:31.157368: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24867300 of size 4608 next 506\n",
      "2022-10-13 15:00:31.157370: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24868500 of size 131072 next 507\n",
      "2022-10-13 15:00:31.157372: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24888500 of size 1024 next 508\n",
      "2022-10-13 15:00:31.157374: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24888900 of size 1024 next 509\n",
      "2022-10-13 15:00:31.157377: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24888d00 of size 9216 next 510\n",
      "2022-10-13 15:00:31.157381: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2488b100 of size 262144 next 511\n",
      "2022-10-13 15:00:31.157385: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd248cb100 of size 131072 next 512\n",
      "2022-10-13 15:00:31.157388: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd248eb100 of size 1048576 next 513\n",
      "2022-10-13 15:00:31.157392: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd249eb100 of size 1024 next 514\n",
      "2022-10-13 15:00:31.157396: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd249eb500 of size 131072 next 515\n",
      "2022-10-13 15:00:31.157400: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a0b500 of size 512 next 516\n",
      "2022-10-13 15:00:31.157403: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a0b700 of size 51200 next 517\n",
      "2022-10-13 15:00:31.157406: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a17f00 of size 512 next 518\n",
      "2022-10-13 15:00:31.157408: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18100 of size 256 next 519\n",
      "2022-10-13 15:00:31.157410: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18200 of size 256 next 520\n",
      "2022-10-13 15:00:31.157412: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18300 of size 256 next 521\n",
      "2022-10-13 15:00:31.157413: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18400 of size 256 next 522\n",
      "2022-10-13 15:00:31.157415: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18500 of size 256 next 523\n",
      "2022-10-13 15:00:31.157418: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18600 of size 256 next 524\n",
      "2022-10-13 15:00:31.157421: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18700 of size 256 next 525\n",
      "2022-10-13 15:00:31.157425: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18800 of size 256 next 526\n",
      "2022-10-13 15:00:31.157429: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18900 of size 256 next 527\n",
      "2022-10-13 15:00:31.157432: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18a00 of size 256 next 528\n",
      "2022-10-13 15:00:31.157436: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd24a18b00 of size 153600000 next 534\n",
      "2022-10-13 15:00:31.157440: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dc94b00 of size 400128 next 535\n",
      "2022-10-13 15:00:31.157443: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6600 of size 256 next 529\n",
      "2022-10-13 15:00:31.157446: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6700 of size 256 next 537\n",
      "2022-10-13 15:00:31.157448: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6800 of size 256 next 536\n",
      "2022-10-13 15:00:31.157450: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6900 of size 256 next 533\n",
      "2022-10-13 15:00:31.157451: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6a00 of size 256 next 538\n",
      "2022-10-13 15:00:31.157453: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6b00 of size 256 next 539\n",
      "2022-10-13 15:00:31.157455: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6c00 of size 256 next 542\n",
      "2022-10-13 15:00:31.157456: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6d00 of size 256 next 543\n",
      "2022-10-13 15:00:31.157459: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf6e00 of size 512 next 572\n",
      "2022-10-13 15:00:31.157463: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7000 of size 512 next 573\n",
      "2022-10-13 15:00:31.157466: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7200 of size 768 next 540\n",
      "2022-10-13 15:00:31.157470: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7500 of size 1280 next 531\n",
      "2022-10-13 15:00:31.157474: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7a00 of size 256 next 549\n",
      "2022-10-13 15:00:31.157477: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7b00 of size 256 next 550\n",
      "2022-10-13 15:00:31.157481: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7c00 of size 256 next 551\n",
      "2022-10-13 15:00:31.157485: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7d00 of size 256 next 552\n",
      "2022-10-13 15:00:31.157487: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7e00 of size 256 next 555\n",
      "2022-10-13 15:00:31.157489: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf7f00 of size 256 next 545\n",
      "2022-10-13 15:00:31.157491: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8000 of size 1280 next 546\n",
      "2022-10-13 15:00:31.157493: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8500 of size 512 next 577\n",
      "2022-10-13 15:00:31.157494: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8700 of size 512 next 579\n",
      "2022-10-13 15:00:31.157496: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8900 of size 512 next 580\n",
      "2022-10-13 15:00:31.157498: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8b00 of size 1024 next 581\n",
      "2022-10-13 15:00:31.157500: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf8f00 of size 1024 next 585\n",
      "2022-10-13 15:00:31.157503: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf9300 of size 1792 next 541\n",
      "2022-10-13 15:00:31.157507: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcf9a00 of size 6400 next 532\n",
      "2022-10-13 15:00:31.157510: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfb300 of size 9728 next 530\n",
      "2022-10-13 15:00:31.157514: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfd900 of size 256 next 558\n",
      "2022-10-13 15:00:31.157518: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfda00 of size 256 next 559\n",
      "2022-10-13 15:00:31.157521: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfdb00 of size 256 next 565\n",
      "2022-10-13 15:00:31.157525: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfdc00 of size 256 next 566\n",
      "2022-10-13 15:00:31.157529: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfdd00 of size 256 next 554\n",
      "2022-10-13 15:00:31.157532: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfde00 of size 1280 next 553\n",
      "2022-10-13 15:00:31.157534: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfe300 of size 256 next 567\n",
      "2022-10-13 15:00:31.157537: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfe400 of size 512 next 568\n",
      "2022-10-13 15:00:31.157539: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfe600 of size 768 next 547\n",
      "2022-10-13 15:00:31.157540: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcfe900 of size 4096 next 544\n",
      "2022-10-13 15:00:31.157542: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dcff900 of size 4096 next 548\n",
      "2022-10-13 15:00:31.157545: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd00900 of size 2304 next 561\n",
      "2022-10-13 15:00:31.157549: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd01200 of size 4608 next 574\n",
      "2022-10-13 15:00:31.157553: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd02400 of size 1024 next 586\n",
      "2022-10-13 15:00:31.157556: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd02800 of size 1024 next 591\n",
      "2022-10-13 15:00:31.157560: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd02c00 of size 512 next 595\n",
      "2022-10-13 15:00:31.157563: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd02e00 of size 512 next 597\n",
      "2022-10-13 15:00:31.157567: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03000 of size 256 next 599\n",
      "2022-10-13 15:00:31.157571: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03100 of size 256 next 600\n",
      "2022-10-13 15:00:31.157573: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03200 of size 256 next 602\n",
      "2022-10-13 15:00:31.157575: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03300 of size 256 next 603\n",
      "2022-10-13 15:00:31.157577: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03400 of size 256 next 604\n",
      "2022-10-13 15:00:31.157578: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03500 of size 256 next 582\n",
      "2022-10-13 15:00:31.157581: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd03600 of size 4864 next 557\n",
      "2022-10-13 15:00:31.157585: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd04900 of size 8192 next 556\n",
      "2022-10-13 15:00:31.157589: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd06900 of size 2304 next 569\n",
      "2022-10-13 15:00:31.157592: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd07200 of size 1280 next 605\n",
      "2022-10-13 15:00:31.157596: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd07700 of size 4608 next 564\n",
      "2022-10-13 15:00:31.157600: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd08900 of size 8192 next 560\n",
      "2022-10-13 15:00:31.157603: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0a900 of size 4096 next 608\n",
      "2022-10-13 15:00:31.157607: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0b900 of size 256 next 609\n",
      "2022-10-13 15:00:31.157609: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0ba00 of size 256 next 610\n",
      "2022-10-13 15:00:31.157611: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0bb00 of size 1280 next 611\n",
      "2022-10-13 15:00:31.157612: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0c000 of size 10496 next 563\n",
      "2022-10-13 15:00:31.157615: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd0e900 of size 16384 next 562\n",
      "2022-10-13 15:00:31.157618: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd12900 of size 32768 next 620\n",
      "2022-10-13 15:00:31.157622: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd1a900 of size 32768 next 571\n",
      "2022-10-13 15:00:31.157626: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd22900 of size 32768 next 570\n",
      "2022-10-13 15:00:31.157630: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd2a900 of size 32768 next 578\n",
      "2022-10-13 15:00:31.157634: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd32900 of size 256 next 606\n",
      "2022-10-13 15:00:31.157638: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd32a00 of size 1280 next 607\n",
      "2022-10-13 15:00:31.157642: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd32f00 of size 7680 next 588\n",
      "2022-10-13 15:00:31.157644: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd34d00 of size 9216 next 587\n",
      "2022-10-13 15:00:31.157645: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd37100 of size 14336 next 576\n",
      "2022-10-13 15:00:31.157647: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd3a900 of size 65536 next 575\n",
      "2022-10-13 15:00:31.157650: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd4a900 of size 256 next 612\n",
      "2022-10-13 15:00:31.157653: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd4aa00 of size 256 next 613\n",
      "2022-10-13 15:00:31.157657: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd4ab00 of size 2304 next 614\n",
      "2022-10-13 15:00:31.157661: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd4b400 of size 16384 next 615\n",
      "2022-10-13 15:00:31.157664: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd4f400 of size 8192 next 616\n",
      "2022-10-13 15:00:31.157668: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd51400 of size 256 next 617\n",
      "2022-10-13 15:00:31.157671: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd51500 of size 256 next 618\n",
      "2022-10-13 15:00:31.157675: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd51600 of size 2304 next 619\n",
      "2022-10-13 15:00:31.157678: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd51f00 of size 512 next 621\n",
      "2022-10-13 15:00:31.157680: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd52100 of size 512 next 622\n",
      "2022-10-13 15:00:31.157681: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd52300 of size 4608 next 623\n",
      "2022-10-13 15:00:31.157683: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd53500 of size 512 next 625\n",
      "2022-10-13 15:00:31.157685: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd53700 of size 512 next 626\n",
      "2022-10-13 15:00:31.157686: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd53900 of size 4608 next 627\n",
      "2022-10-13 15:00:31.157688: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd54b00 of size 1024 next 629\n",
      "2022-10-13 15:00:31.157690: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd54f00 of size 1024 next 630\n",
      "2022-10-13 15:00:31.157692: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd55300 of size 1024 next 634\n",
      "2022-10-13 15:00:31.157694: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7fdd2dd55700 of size 6656 next 598\n",
      "2022-10-13 15:00:31.157695: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd57100 of size 79872 next 596\n",
      "2022-10-13 15:00:31.157697: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd6a900 of size 131072 next 584\n",
      "2022-10-13 15:00:31.157699: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2dd8a900 of size 131072 next 583\n",
      "2022-10-13 15:00:31.157701: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2ddaa900 of size 131072 next 592\n",
      "2022-10-13 15:00:31.157702: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2ddca900 of size 65536 next 624\n",
      "2022-10-13 15:00:31.157704: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2ddda900 of size 9216 next 631\n",
      "2022-10-13 15:00:31.157707: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] Free  at 7fdd2dddcd00 of size 56320 next 590\n",
      "2022-10-13 15:00:31.157712: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2ddea900 of size 262144 next 589\n",
      "2022-10-13 15:00:31.157715: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2de2a900 of size 400128 next 601\n",
      "2022-10-13 15:00:31.157719: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2de8c400 of size 131072 next 628\n",
      "2022-10-13 15:00:31.157723: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2deac400 of size 262144 next 632\n",
      "2022-10-13 15:00:31.157727: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2deec400 of size 131072 next 633\n",
      "2022-10-13 15:00:31.157730: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2df0c400 of size 1172736 next 594\n",
      "2022-10-13 15:00:31.157734: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2e02a900 of size 1048576 next 593\n",
      "2022-10-13 15:00:31.157738: I tensorflow/core/common_runtime/bfc_allocator.cc:1083] InUse at 7fdd2e12a900 of size 215570176 next 18446744073709551615\n",
      "2022-10-13 15:00:31.157740: I tensorflow/core/common_runtime/bfc_allocator.cc:1088]      Summary of in-use Chunks by size: \n",
      "2022-10-13 15:00:31.157744: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 239 Chunks of size 256 totalling 59.8KiB\n",
      "2022-10-13 15:00:31.157747: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 74 Chunks of size 512 totalling 37.0KiB\n",
      "2022-10-13 15:00:31.157751: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 6 Chunks of size 768 totalling 4.5KiB\n",
      "2022-10-13 15:00:31.157756: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 37 Chunks of size 1024 totalling 37.0KiB\n",
      "2022-10-13 15:00:31.157760: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 32 Chunks of size 1280 totalling 40.0KiB\n",
      "2022-10-13 15:00:31.157764: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 1536 totalling 4.5KiB\n",
      "2022-10-13 15:00:31.157768: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 1792 totalling 5.2KiB\n",
      "2022-10-13 15:00:31.157773: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 21 Chunks of size 2304 totalling 47.2KiB\n",
      "2022-10-13 15:00:31.157778: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 23 Chunks of size 4096 totalling 92.0KiB\n",
      "2022-10-13 15:00:31.157782: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 20 Chunks of size 4608 totalling 90.0KiB\n",
      "2022-10-13 15:00:31.157785: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 4864 totalling 14.2KiB\n",
      "2022-10-13 15:00:31.157787: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 5376 totalling 5.2KiB\n",
      "2022-10-13 15:00:31.157790: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 5632 totalling 5.5KiB\n",
      "2022-10-13 15:00:31.157794: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 5888 totalling 5.8KiB\n",
      "2022-10-13 15:00:31.157799: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 6400 totalling 18.8KiB\n",
      "2022-10-13 15:00:31.157803: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 2 Chunks of size 7680 totalling 15.0KiB\n",
      "2022-10-13 15:00:31.157807: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 7936 totalling 7.8KiB\n",
      "2022-10-13 15:00:31.157812: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 19 Chunks of size 8192 totalling 152.0KiB\n",
      "2022-10-13 15:00:31.157816: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 8960 totalling 8.8KiB\n",
      "2022-10-13 15:00:31.157820: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 11 Chunks of size 9216 totalling 99.0KiB\n",
      "2022-10-13 15:00:31.157822: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 7 Chunks of size 9728 totalling 66.5KiB\n",
      "2022-10-13 15:00:31.157825: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 10496 totalling 30.8KiB\n",
      "2022-10-13 15:00:31.157828: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 4 Chunks of size 14336 totalling 56.0KiB\n",
      "2022-10-13 15:00:31.157833: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 11 Chunks of size 16384 totalling 176.0KiB\n",
      "2022-10-13 15:00:31.157837: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 22 Chunks of size 32768 totalling 704.0KiB\n",
      "2022-10-13 15:00:31.157841: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 3 Chunks of size 51200 totalling 150.0KiB\n",
      "2022-10-13 15:00:31.157846: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 52736 totalling 51.5KiB\n",
      "2022-10-13 15:00:31.157851: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 2 Chunks of size 56320 totalling 110.0KiB\n",
      "2022-10-13 15:00:31.157855: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 11 Chunks of size 65536 totalling 704.0KiB\n",
      "2022-10-13 15:00:31.157858: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 4 Chunks of size 79872 totalling 312.0KiB\n",
      "2022-10-13 15:00:31.157862: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 32 Chunks of size 131072 totalling 4.00MiB\n",
      "2022-10-13 15:00:31.157866: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 11 Chunks of size 262144 totalling 2.75MiB\n",
      "2022-10-13 15:00:31.157870: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 5 Chunks of size 400128 totalling 1.91MiB\n",
      "2022-10-13 15:00:31.157875: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 7 Chunks of size 1048576 totalling 7.00MiB\n",
      "2022-10-13 15:00:31.157879: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 4 Chunks of size 1172736 totalling 4.47MiB\n",
      "2022-10-13 15:00:31.157884: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 4 Chunks of size 153600000 totalling 585.94MiB\n",
      "2022-10-13 15:00:31.157888: I tensorflow/core/common_runtime/bfc_allocator.cc:1091] 1 Chunks of size 215570176 totalling 205.58MiB\n",
      "2022-10-13 15:00:31.157891: I tensorflow/core/common_runtime/bfc_allocator.cc:1095] Sum Total of in-use chunks: 814.69MiB\n",
      "2022-10-13 15:00:31.157895: I tensorflow/core/common_runtime/bfc_allocator.cc:1097] total_region_allocated_bytes_: 854327296 memory_limit_: 854327296 available bytes: 0 curr_region_allocation_bytes_: 1708654592\n",
      "2022-10-13 15:00:31.157903: I tensorflow/core/common_runtime/bfc_allocator.cc:1103] Stats: \n",
      "Limit:                       854327296\n",
      "InUse:                       854264320\n",
      "MaxInUse:                    854264320\n",
      "NumAllocs:                        2589\n",
      "MaxAllocSize:                215570176\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2022-10-13 15:00:31.157916: W tensorflow/core/common_runtime/bfc_allocator.cc:491] *********************************************************************************************xxxxxxx\n",
      "2022-10-13 15:00:31.157948: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at constant_op.cc:175 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "in user code:\n\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 646, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 860, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/adam.py\", line 122, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 946, in add_slot\n        weight = tf.Variable(\n    File \"/usr/lib/python3/dist-packages/keras/initializers/initializers_v2.py\", line 152, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4117/1196127185.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1125\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1128\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: in user code:\n\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/lib/python3/dist-packages/keras/engine/training.py\", line 893, in train_step\n        self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 539, in minimize\n        return self.apply_gradients(grads_and_vars, name=name)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 646, in apply_gradients\n        self._create_all_weights(var_list)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 860, in _create_all_weights\n        self._create_slots(var_list)\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/adam.py\", line 122, in _create_slots\n        self.add_slot(var, 'm')\n    File \"/usr/lib/python3/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py\", line 946, in add_slot\n        weight = tf.Variable(\n    File \"/usr/lib/python3/dist-packages/keras/initializers/initializers_v2.py\", line 152, in __call__\n        return tf.zeros(shape, dtype)\n\n    ResourceExhaustedError: OOM when allocating tensor with shape[256,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Fill]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train, y_train,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy = 39.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdr_blend import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = len(x_train)\n",
    "\n",
    "x_dataAug = []\n",
    "y_dataAug = []\n",
    "for image in range(0,end):\n",
    "    x_dataAug.append(hdr(x_train[image]))\n",
    "    y_dataAug.append(y_train[image])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataAug = np.asarray(y_dataAug)\n",
    "x_dataAug = np.asarray(x_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show images. \n",
    "fig = plt.figure(figsize = (10,20))\n",
    "columns = 2\n",
    "rows = 1\n",
    "fig.add_subplot(rows,columns, 1)\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('Original LDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 2)\n",
    "plt.imshow(x_dataAug[0])\n",
    "plt.title('Blended 1X HDR', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_dataAug)\n",
    "len(y_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_12 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_11 (Rescaling)       (None, 32, 32, 3)    0           ['input_12[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_66 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_11[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_94 (BatchN  (None, 28, 28, 32)  128         ['conv2d_66[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_94 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_94[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_94 (Separable  (None, 28, 28, 32)  1312        ['activation_94[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_95 (BatchN  (None, 28, 28, 32)  128         ['separable_conv2d_94[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_95 (Activation)     (None, 28, 28, 32)   0           ['batch_normalization_95[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_95 (Separable  (None, 28, 28, 32)  1312        ['activation_95[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_53 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_95[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_67 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_66[0][0]']              \n",
      "                                                                                                  \n",
      " add_47 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_53[0][0]',       \n",
      "                                                                  'conv2d_67[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_96 (BatchN  (None, 14, 14, 32)  128         ['add_47[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_96 (Activation)     (None, 14, 14, 32)   0           ['batch_normalization_96[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_96 (Separable  (None, 14, 14, 64)  2336        ['activation_96[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_97 (BatchN  (None, 14, 14, 64)  256         ['separable_conv2d_96[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_97 (Activation)     (None, 14, 14, 64)   0           ['batch_normalization_97[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_97 (Separable  (None, 14, 14, 64)  4672        ['activation_97[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_54 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_97[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_68 (Conv2D)             (None, 7, 7, 64)     2048        ['add_47[0][0]']                 \n",
      "                                                                                                  \n",
      " add_48 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_54[0][0]',       \n",
      "                                                                  'conv2d_68[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_98 (BatchN  (None, 7, 7, 64)    256         ['add_48[0][0]']                 \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_98 (Activation)     (None, 7, 7, 64)     0           ['batch_normalization_98[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_98 (Separable  (None, 7, 7, 128)   8768        ['activation_98[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_99 (BatchN  (None, 7, 7, 128)   512         ['separable_conv2d_98[0][0]']    \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " activation_99 (Activation)     (None, 7, 7, 128)    0           ['batch_normalization_99[0][0]'] \n",
      "                                                                                                  \n",
      " separable_conv2d_99 (Separable  (None, 7, 7, 128)   17536       ['activation_99[0][0]']          \n",
      " Conv2D)                                                                                          \n",
      "                                                                                                  \n",
      " max_pooling2d_55 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_99[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_69 (Conv2D)             (None, 4, 4, 128)    8192        ['add_48[0][0]']                 \n",
      "                                                                                                  \n",
      " add_49 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_55[0][0]',       \n",
      "                                                                  'conv2d_69[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_100 (Batch  (None, 4, 4, 128)   512         ['add_49[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_100 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_100[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_100 (Separabl  (None, 4, 4, 256)   33920       ['activation_100[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_101 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_100[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_101 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_101[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_101 (Separabl  (None, 4, 4, 256)   67840       ['activation_101[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_56 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_101[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_70 (Conv2D)             (None, 2, 2, 256)    32768       ['add_49[0][0]']                 \n",
      "                                                                                                  \n",
      " add_50 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_56[0][0]',       \n",
      "                                                                  'conv2d_70[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_8 (Flatten)            (None, 1024)         0           ['add_50[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_41 (Dropout)           (None, 1024)         0           ['flatten_8[0][0]']              \n",
      "                                                                                                  \n",
      " dense_39 (Dense)               (None, 256)          262400      ['dropout_41[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_42 (Dropout)           (None, 256)          0           ['dense_39[0][0]']               \n",
      "                                                                                                  \n",
      " dense_40 (Dense)               (None, 128)          32896       ['dropout_42[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_43 (Dropout)           (None, 128)          0           ['dense_40[0][0]']               \n",
      "                                                                                                  \n",
      " dense_41 (Dense)               (None, 100)          12900       ['dropout_43[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 4.2082 - accuracy: 0.0627 - val_loss: 3.8395 - val_accuracy: 0.1264\n",
      "Epoch 2/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8474 - accuracy: 0.1094 - val_loss: 3.5755 - val_accuracy: 0.1692\n",
      "Epoch 3/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.6768 - accuracy: 0.1318 - val_loss: 3.4109 - val_accuracy: 0.1936\n",
      "Epoch 4/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.5466 - accuracy: 0.1536 - val_loss: 3.3862 - val_accuracy: 0.1891\n",
      "Epoch 5/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4371 - accuracy: 0.1708 - val_loss: 3.2195 - val_accuracy: 0.2220\n",
      "Epoch 6/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.3379 - accuracy: 0.1884 - val_loss: 3.1177 - val_accuracy: 0.2392\n",
      "Epoch 7/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.2639 - accuracy: 0.1992 - val_loss: 3.0523 - val_accuracy: 0.2516\n",
      "Epoch 8/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1854 - accuracy: 0.2134 - val_loss: 3.0137 - val_accuracy: 0.2598\n",
      "Epoch 9/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.1174 - accuracy: 0.2276 - val_loss: 2.9720 - val_accuracy: 0.2618\n",
      "Epoch 10/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.0528 - accuracy: 0.2393 - val_loss: 2.9739 - val_accuracy: 0.2653\n",
      "Epoch 11/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0006 - accuracy: 0.2461 - val_loss: 2.8734 - val_accuracy: 0.2854\n",
      "Epoch 12/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9455 - accuracy: 0.2588 - val_loss: 2.7946 - val_accuracy: 0.3011\n",
      "Epoch 13/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8973 - accuracy: 0.2662 - val_loss: 2.8099 - val_accuracy: 0.2972\n",
      "Epoch 14/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8425 - accuracy: 0.2776 - val_loss: 2.7715 - val_accuracy: 0.3092\n",
      "Epoch 15/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.8022 - accuracy: 0.2855 - val_loss: 2.7406 - val_accuracy: 0.3136\n",
      "Epoch 16/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7571 - accuracy: 0.2923 - val_loss: 2.7117 - val_accuracy: 0.3187\n",
      "Epoch 17/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.7139 - accuracy: 0.3024 - val_loss: 2.7115 - val_accuracy: 0.3129\n",
      "Epoch 18/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6770 - accuracy: 0.3096 - val_loss: 2.7782 - val_accuracy: 0.3057\n",
      "Epoch 19/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6398 - accuracy: 0.3145 - val_loss: 2.6224 - val_accuracy: 0.3331\n",
      "Epoch 20/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5982 - accuracy: 0.3263 - val_loss: 2.6036 - val_accuracy: 0.3355\n",
      "Epoch 21/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5650 - accuracy: 0.3332 - val_loss: 2.6115 - val_accuracy: 0.3330\n",
      "Epoch 22/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5301 - accuracy: 0.3380 - val_loss: 2.6342 - val_accuracy: 0.3348\n",
      "Epoch 23/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5100 - accuracy: 0.3425 - val_loss: 2.5600 - val_accuracy: 0.3490\n",
      "Epoch 24/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4673 - accuracy: 0.3515 - val_loss: 2.5738 - val_accuracy: 0.3411\n",
      "Epoch 25/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4359 - accuracy: 0.3581 - val_loss: 2.6310 - val_accuracy: 0.3395\n",
      "Epoch 26/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.4087 - accuracy: 0.3648 - val_loss: 2.5100 - val_accuracy: 0.3574\n",
      "Epoch 27/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3822 - accuracy: 0.3674 - val_loss: 2.5120 - val_accuracy: 0.3561\n",
      "Epoch 28/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3562 - accuracy: 0.3744 - val_loss: 2.5472 - val_accuracy: 0.3506\n",
      "Epoch 29/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3281 - accuracy: 0.3818 - val_loss: 2.4880 - val_accuracy: 0.3649\n",
      "Epoch 30/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3109 - accuracy: 0.3854 - val_loss: 2.5420 - val_accuracy: 0.3541\n",
      "Epoch 31/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2807 - accuracy: 0.3905 - val_loss: 2.5430 - val_accuracy: 0.3531\n",
      "Epoch 32/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2625 - accuracy: 0.3936 - val_loss: 2.5454 - val_accuracy: 0.3549\n",
      "Epoch 33/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2265 - accuracy: 0.4011 - val_loss: 2.5214 - val_accuracy: 0.3590\n",
      "Epoch 34/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2120 - accuracy: 0.4053 - val_loss: 2.4766 - val_accuracy: 0.3665\n",
      "Epoch 35/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1836 - accuracy: 0.4097 - val_loss: 2.4540 - val_accuracy: 0.3711\n",
      "Epoch 36/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1614 - accuracy: 0.4186 - val_loss: 2.4609 - val_accuracy: 0.3716\n",
      "Epoch 37/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1459 - accuracy: 0.4194 - val_loss: 2.4755 - val_accuracy: 0.3700\n",
      "Epoch 38/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1266 - accuracy: 0.4216 - val_loss: 2.4296 - val_accuracy: 0.3782\n",
      "Epoch 39/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1064 - accuracy: 0.4280 - val_loss: 2.4599 - val_accuracy: 0.3780\n",
      "Epoch 40/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0744 - accuracy: 0.4333 - val_loss: 2.5178 - val_accuracy: 0.3683\n",
      "Epoch 41/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0614 - accuracy: 0.4361 - val_loss: 2.5181 - val_accuracy: 0.3666\n",
      "Epoch 42/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0507 - accuracy: 0.4398 - val_loss: 2.4428 - val_accuracy: 0.3808\n",
      "Epoch 43/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0240 - accuracy: 0.4449 - val_loss: 2.4573 - val_accuracy: 0.3803\n",
      "Epoch 44/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0049 - accuracy: 0.4504 - val_loss: 2.4420 - val_accuracy: 0.3849\n",
      "Epoch 45/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9830 - accuracy: 0.4527 - val_loss: 2.4497 - val_accuracy: 0.3864\n",
      "Epoch 46/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9681 - accuracy: 0.4592 - val_loss: 2.5216 - val_accuracy: 0.3754\n",
      "Epoch 47/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9561 - accuracy: 0.4581 - val_loss: 2.4494 - val_accuracy: 0.3832\n",
      "Epoch 48/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9308 - accuracy: 0.4645 - val_loss: 2.4149 - val_accuracy: 0.3889\n",
      "Epoch 49/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9137 - accuracy: 0.4695 - val_loss: 2.4589 - val_accuracy: 0.3882\n",
      "Epoch 50/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9029 - accuracy: 0.4708 - val_loss: 2.4243 - val_accuracy: 0.3877\n",
      "Epoch 51/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8887 - accuracy: 0.4724 - val_loss: 2.4418 - val_accuracy: 0.3871\n",
      "Epoch 52/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8696 - accuracy: 0.4757 - val_loss: 2.4872 - val_accuracy: 0.3801\n",
      "Epoch 53/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8617 - accuracy: 0.4803 - val_loss: 2.5149 - val_accuracy: 0.3845\n",
      "Epoch 54/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8404 - accuracy: 0.4843 - val_loss: 2.4265 - val_accuracy: 0.3935\n",
      "Epoch 55/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8216 - accuracy: 0.4894 - val_loss: 2.4659 - val_accuracy: 0.3871\n",
      "Epoch 56/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8074 - accuracy: 0.4928 - val_loss: 2.5037 - val_accuracy: 0.3873\n",
      "Epoch 57/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7888 - accuracy: 0.4972 - val_loss: 2.5193 - val_accuracy: 0.3849\n",
      "Epoch 58/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7806 - accuracy: 0.4985 - val_loss: 2.4920 - val_accuracy: 0.3875\n",
      "Epoch 59/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7676 - accuracy: 0.5018 - val_loss: 2.5876 - val_accuracy: 0.3736\n",
      "Epoch 60/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7482 - accuracy: 0.5050 - val_loss: 2.4927 - val_accuracy: 0.3938\n",
      "Epoch 61/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7399 - accuracy: 0.5074 - val_loss: 2.4636 - val_accuracy: 0.3938\n",
      "Epoch 62/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7220 - accuracy: 0.5109 - val_loss: 2.4884 - val_accuracy: 0.3933\n",
      "Epoch 63/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7201 - accuracy: 0.5129 - val_loss: 2.4935 - val_accuracy: 0.3880\n",
      "Epoch 64/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6990 - accuracy: 0.5155 - val_loss: 2.5346 - val_accuracy: 0.3882\n",
      "Epoch 65/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6925 - accuracy: 0.5164 - val_loss: 2.6273 - val_accuracy: 0.3785\n",
      "Epoch 66/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6802 - accuracy: 0.5213 - val_loss: 2.5438 - val_accuracy: 0.3880\n",
      "Epoch 67/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6601 - accuracy: 0.5238 - val_loss: 2.5107 - val_accuracy: 0.3983\n",
      "Epoch 68/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6544 - accuracy: 0.5264 - val_loss: 2.5242 - val_accuracy: 0.3919\n",
      "Epoch 69/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6355 - accuracy: 0.5307 - val_loss: 2.4882 - val_accuracy: 0.3944\n",
      "Epoch 70/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6304 - accuracy: 0.5323 - val_loss: 2.6037 - val_accuracy: 0.3867\n",
      "Epoch 71/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6190 - accuracy: 0.5338 - val_loss: 2.5296 - val_accuracy: 0.3906\n",
      "Epoch 72/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6063 - accuracy: 0.5379 - val_loss: 2.5476 - val_accuracy: 0.3943\n",
      "Epoch 73/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5856 - accuracy: 0.5402 - val_loss: 2.6705 - val_accuracy: 0.3784\n",
      "Epoch 74/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5791 - accuracy: 0.5450 - val_loss: 2.6172 - val_accuracy: 0.3872\n",
      "Epoch 75/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5686 - accuracy: 0.5462 - val_loss: 2.5821 - val_accuracy: 0.3908\n",
      "Epoch 76/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5634 - accuracy: 0.5474 - val_loss: 2.5443 - val_accuracy: 0.3942\n",
      "Epoch 77/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5522 - accuracy: 0.5491 - val_loss: 2.5200 - val_accuracy: 0.3966\n",
      "Epoch 78/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5453 - accuracy: 0.5530 - val_loss: 2.6209 - val_accuracy: 0.3884\n",
      "Epoch 79/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5338 - accuracy: 0.5553 - val_loss: 2.7477 - val_accuracy: 0.3773\n",
      "Epoch 80/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5320 - accuracy: 0.5546 - val_loss: 2.5715 - val_accuracy: 0.3940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_dataAug, y_dataAug,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Validation Accuracy: 39.44%.  One pass through HDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = len(x_dataAug)\n",
    "\n",
    "x_dataAug_2 = []\n",
    "y_dataAug_2 = []\n",
    "for image in range(0,end):\n",
    "    x_dataAug_2.append(hdr(x_dataAug[image]))\n",
    "    y_dataAug_2.append(y_train[image])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dataAug_2 = np.asarray(y_dataAug)\n",
    "x_dataAug_2 = np.asarray(x_dataAug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x2160 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show images. \n",
    "fig = plt.figure(figsize = (10,30))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows,columns, 1)\n",
    "plt.imshow(x_train[0])\n",
    "plt.title('Original LDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 2)\n",
    "plt.imshow(x_dataAug[0])\n",
    "plt.title('Blended 1X HDR', fontdict={'fontsize': 20})\n",
    "fig.add_subplot(rows,columns, 3)\n",
    "plt.imshow(x_dataAug_2[0])\n",
    "plt.title('Blended 2X HDR', fontdict={'fontsize': 20})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_12 (Rescaling)       (None, 32, 32, 3)    0           ['input_13[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_71 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_12[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_102 (Batch  (None, 28, 28, 32)  128         ['conv2d_71[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_102 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_102[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_102 (Separabl  (None, 28, 28, 32)  1312        ['activation_102[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_103 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_102[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_103 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_103[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_103 (Separabl  (None, 28, 28, 32)  1312        ['activation_103[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_57 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_103[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_72 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_71[0][0]']              \n",
      "                                                                                                  \n",
      " add_51 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_57[0][0]',       \n",
      "                                                                  'conv2d_72[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_104 (Batch  (None, 14, 14, 32)  128         ['add_51[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_104 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_104[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_104 (Separabl  (None, 14, 14, 64)  2336        ['activation_104[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_105 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_104[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_105 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_105[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_105 (Separabl  (None, 14, 14, 64)  4672        ['activation_105[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_58 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_105[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_73 (Conv2D)             (None, 7, 7, 64)     2048        ['add_51[0][0]']                 \n",
      "                                                                                                  \n",
      " add_52 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_58[0][0]',       \n",
      "                                                                  'conv2d_73[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_106 (Batch  (None, 7, 7, 64)    256         ['add_52[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_106 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_106[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_106 (Separabl  (None, 7, 7, 128)   8768        ['activation_106[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_107 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_106[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_107 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_107[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_107 (Separabl  (None, 7, 7, 128)   17536       ['activation_107[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_59 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_107[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_74 (Conv2D)             (None, 4, 4, 128)    8192        ['add_52[0][0]']                 \n",
      "                                                                                                  \n",
      " add_53 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_59[0][0]',       \n",
      "                                                                  'conv2d_74[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_108 (Batch  (None, 4, 4, 128)   512         ['add_53[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_108 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_108[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_108 (Separabl  (None, 4, 4, 256)   33920       ['activation_108[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_109 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_108[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_109 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_109[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_109 (Separabl  (None, 4, 4, 256)   67840       ['activation_109[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_60 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_109[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_75 (Conv2D)             (None, 2, 2, 256)    32768       ['add_53[0][0]']                 \n",
      "                                                                                                  \n",
      " add_54 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_60[0][0]',       \n",
      "                                                                  'conv2d_75[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_9 (Flatten)            (None, 1024)         0           ['add_54[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_44 (Dropout)           (None, 1024)         0           ['flatten_9[0][0]']              \n",
      "                                                                                                  \n",
      " dense_42 (Dense)               (None, 256)          262400      ['dropout_44[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_45 (Dropout)           (None, 256)          0           ['dense_42[0][0]']               \n",
      "                                                                                                  \n",
      " dense_43 (Dense)               (None, 128)          32896       ['dropout_45[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_46 (Dropout)           (None, 128)          0           ['dense_43[0][0]']               \n",
      "                                                                                                  \n",
      " dense_44 (Dense)               (None, 100)          12900       ['dropout_46[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation2X.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 4.2247 - accuracy: 0.0595 - val_loss: 3.8486 - val_accuracy: 0.1183\n",
      "Epoch 2/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.8677 - accuracy: 0.1067 - val_loss: 3.5953 - val_accuracy: 0.1564\n",
      "Epoch 3/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.6994 - accuracy: 0.1284 - val_loss: 3.4667 - val_accuracy: 0.1783\n",
      "Epoch 4/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.5652 - accuracy: 0.1497 - val_loss: 3.3374 - val_accuracy: 0.2012\n",
      "Epoch 5/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.4604 - accuracy: 0.1685 - val_loss: 3.2483 - val_accuracy: 0.2162\n",
      "Epoch 6/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.3706 - accuracy: 0.1822 - val_loss: 3.1881 - val_accuracy: 0.2296\n",
      "Epoch 7/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2939 - accuracy: 0.1975 - val_loss: 3.1154 - val_accuracy: 0.2413\n",
      "Epoch 8/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 3.2174 - accuracy: 0.2113 - val_loss: 3.0457 - val_accuracy: 0.2506\n",
      "Epoch 9/80\n",
      "782/782 [==============================] - 5s 6ms/step - loss: 3.1498 - accuracy: 0.2226 - val_loss: 2.9857 - val_accuracy: 0.2629\n",
      "Epoch 10/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0886 - accuracy: 0.2317 - val_loss: 2.9425 - val_accuracy: 0.2745\n",
      "Epoch 11/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 3.0336 - accuracy: 0.2448 - val_loss: 2.9400 - val_accuracy: 0.2742\n",
      "Epoch 12/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.9753 - accuracy: 0.2556 - val_loss: 2.8544 - val_accuracy: 0.2860\n",
      "Epoch 13/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.9212 - accuracy: 0.2676 - val_loss: 2.8242 - val_accuracy: 0.2933\n",
      "Epoch 14/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.8757 - accuracy: 0.2743 - val_loss: 2.7722 - val_accuracy: 0.3031\n",
      "Epoch 15/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.8274 - accuracy: 0.2852 - val_loss: 2.7362 - val_accuracy: 0.3123\n",
      "Epoch 16/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.7767 - accuracy: 0.2944 - val_loss: 2.7310 - val_accuracy: 0.3106\n",
      "Epoch 17/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.7319 - accuracy: 0.3034 - val_loss: 2.6945 - val_accuracy: 0.3222\n",
      "Epoch 18/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6992 - accuracy: 0.3068 - val_loss: 2.7017 - val_accuracy: 0.3172\n",
      "Epoch 19/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.6461 - accuracy: 0.3187 - val_loss: 2.6456 - val_accuracy: 0.3319\n",
      "Epoch 20/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.6199 - accuracy: 0.3246 - val_loss: 2.6188 - val_accuracy: 0.3380\n",
      "Epoch 21/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5730 - accuracy: 0.3342 - val_loss: 2.5853 - val_accuracy: 0.3379\n",
      "Epoch 22/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.5438 - accuracy: 0.3384 - val_loss: 2.5872 - val_accuracy: 0.3446\n",
      "Epoch 23/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.5085 - accuracy: 0.3473 - val_loss: 2.6299 - val_accuracy: 0.3336\n",
      "Epoch 24/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4778 - accuracy: 0.3531 - val_loss: 2.5674 - val_accuracy: 0.3445\n",
      "Epoch 25/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4500 - accuracy: 0.3569 - val_loss: 2.5702 - val_accuracy: 0.3489\n",
      "Epoch 26/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.4137 - accuracy: 0.3660 - val_loss: 2.5562 - val_accuracy: 0.3489\n",
      "Epoch 27/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.3887 - accuracy: 0.3694 - val_loss: 2.5075 - val_accuracy: 0.3607\n",
      "Epoch 28/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3549 - accuracy: 0.3755 - val_loss: 2.5117 - val_accuracy: 0.3568\n",
      "Epoch 29/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.3362 - accuracy: 0.3802 - val_loss: 2.5397 - val_accuracy: 0.3555\n",
      "Epoch 30/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3101 - accuracy: 0.3890 - val_loss: 2.5367 - val_accuracy: 0.3548\n",
      "Epoch 31/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2849 - accuracy: 0.3918 - val_loss: 2.4647 - val_accuracy: 0.3657\n",
      "Epoch 32/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2565 - accuracy: 0.3977 - val_loss: 2.4813 - val_accuracy: 0.3671\n",
      "Epoch 33/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2366 - accuracy: 0.4032 - val_loss: 2.4656 - val_accuracy: 0.3687\n",
      "Epoch 34/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.2070 - accuracy: 0.4063 - val_loss: 2.4790 - val_accuracy: 0.3665\n",
      "Epoch 35/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1850 - accuracy: 0.4098 - val_loss: 2.4752 - val_accuracy: 0.3670\n",
      "Epoch 36/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1612 - accuracy: 0.4166 - val_loss: 2.4698 - val_accuracy: 0.3713\n",
      "Epoch 37/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.1325 - accuracy: 0.4237 - val_loss: 2.5279 - val_accuracy: 0.3616\n",
      "Epoch 38/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.1172 - accuracy: 0.4260 - val_loss: 2.4594 - val_accuracy: 0.3750\n",
      "Epoch 39/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0955 - accuracy: 0.4326 - val_loss: 2.4229 - val_accuracy: 0.3812\n",
      "Epoch 40/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 2.0747 - accuracy: 0.4345 - val_loss: 2.4836 - val_accuracy: 0.3713\n",
      "Epoch 41/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0546 - accuracy: 0.4375 - val_loss: 2.4349 - val_accuracy: 0.3799\n",
      "Epoch 42/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0293 - accuracy: 0.4435 - val_loss: 2.4421 - val_accuracy: 0.3789\n",
      "Epoch 43/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0155 - accuracy: 0.4454 - val_loss: 2.4661 - val_accuracy: 0.3810\n",
      "Epoch 44/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9960 - accuracy: 0.4507 - val_loss: 2.4396 - val_accuracy: 0.3791\n",
      "Epoch 45/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9819 - accuracy: 0.4538 - val_loss: 2.4243 - val_accuracy: 0.3833\n",
      "Epoch 46/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.9566 - accuracy: 0.4601 - val_loss: 2.4996 - val_accuracy: 0.3685\n",
      "Epoch 47/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9475 - accuracy: 0.4631 - val_loss: 2.4509 - val_accuracy: 0.3826\n",
      "Epoch 48/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9249 - accuracy: 0.4661 - val_loss: 2.4396 - val_accuracy: 0.3875\n",
      "Epoch 49/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9025 - accuracy: 0.4717 - val_loss: 2.4525 - val_accuracy: 0.3817\n",
      "Epoch 50/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8923 - accuracy: 0.4733 - val_loss: 2.4356 - val_accuracy: 0.3849\n",
      "Epoch 51/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8704 - accuracy: 0.4772 - val_loss: 2.4103 - val_accuracy: 0.3900\n",
      "Epoch 52/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8513 - accuracy: 0.4820 - val_loss: 2.5502 - val_accuracy: 0.3661\n",
      "Epoch 53/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8407 - accuracy: 0.4856 - val_loss: 2.4513 - val_accuracy: 0.3845\n",
      "Epoch 54/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8270 - accuracy: 0.4898 - val_loss: 2.4654 - val_accuracy: 0.3809\n",
      "Epoch 55/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.8040 - accuracy: 0.4942 - val_loss: 2.5131 - val_accuracy: 0.3797\n",
      "Epoch 56/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7942 - accuracy: 0.4941 - val_loss: 2.4559 - val_accuracy: 0.3875\n",
      "Epoch 57/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7796 - accuracy: 0.4978 - val_loss: 2.4625 - val_accuracy: 0.3869\n",
      "Epoch 58/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7610 - accuracy: 0.5041 - val_loss: 2.4859 - val_accuracy: 0.3892\n",
      "Epoch 59/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7518 - accuracy: 0.5060 - val_loss: 2.4834 - val_accuracy: 0.3852\n",
      "Epoch 60/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7363 - accuracy: 0.5084 - val_loss: 2.5227 - val_accuracy: 0.3805\n",
      "Epoch 61/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.7188 - accuracy: 0.5134 - val_loss: 2.4622 - val_accuracy: 0.3926\n",
      "Epoch 62/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7117 - accuracy: 0.5128 - val_loss: 2.5279 - val_accuracy: 0.3861\n",
      "Epoch 63/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6912 - accuracy: 0.5195 - val_loss: 2.4755 - val_accuracy: 0.3913\n",
      "Epoch 64/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6797 - accuracy: 0.5205 - val_loss: 2.5624 - val_accuracy: 0.3827\n",
      "Epoch 65/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6647 - accuracy: 0.5250 - val_loss: 2.5186 - val_accuracy: 0.3888\n",
      "Epoch 66/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6521 - accuracy: 0.5276 - val_loss: 2.4817 - val_accuracy: 0.3900\n",
      "Epoch 67/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.6394 - accuracy: 0.5296 - val_loss: 2.4883 - val_accuracy: 0.3929\n",
      "Epoch 68/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6355 - accuracy: 0.5306 - val_loss: 2.4824 - val_accuracy: 0.3919\n",
      "Epoch 69/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6124 - accuracy: 0.5364 - val_loss: 2.5112 - val_accuracy: 0.3883\n",
      "Epoch 70/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6038 - accuracy: 0.5364 - val_loss: 2.5568 - val_accuracy: 0.3815\n",
      "Epoch 71/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5936 - accuracy: 0.5399 - val_loss: 2.5420 - val_accuracy: 0.3829\n",
      "Epoch 72/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5799 - accuracy: 0.5440 - val_loss: 2.5629 - val_accuracy: 0.3892\n",
      "Epoch 73/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5688 - accuracy: 0.5451 - val_loss: 2.5225 - val_accuracy: 0.3918\n",
      "Epoch 74/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5554 - accuracy: 0.5511 - val_loss: 2.5116 - val_accuracy: 0.3924\n",
      "Epoch 75/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5486 - accuracy: 0.5520 - val_loss: 2.5232 - val_accuracy: 0.3923\n",
      "Epoch 76/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5369 - accuracy: 0.5546 - val_loss: 2.5569 - val_accuracy: 0.3918\n",
      "Epoch 77/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.5216 - accuracy: 0.5581 - val_loss: 2.5748 - val_accuracy: 0.3835\n",
      "Epoch 78/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5205 - accuracy: 0.5587 - val_loss: 2.5618 - val_accuracy: 0.3940\n",
      "Epoch 79/80\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5035 - accuracy: 0.5604 - val_loss: 2.6117 - val_accuracy: 0.3826\n",
      "Epoch 80/80\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 1.4956 - accuracy: 0.5644 - val_loss: 2.5604 - val_accuracy: 0.3924\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_dataAug_2, y_dataAug_2,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Validation Accuracy: 39.40%.  Two passes through HDR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153600000\n",
      "(50000, 32, 32, 3)\n",
      "uint8\n",
      "153600000\n",
      "(50000, 32, 32, 3)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "#How many elements are in that data. \n",
    "print(x_train.size)\n",
    "\n",
    "#What is the shape of the array.\n",
    "print(x_train.shape)\n",
    "\n",
    "#What type are the elements.\n",
    "print(x_train.dtype)\n",
    "\n",
    "#How many elements are in that data. \n",
    "print(x_dataAug_2.size)\n",
    "\n",
    "#What is the shape of the array.\n",
    "print(x_dataAug_2.shape)\n",
    "\n",
    "#What type are the elements.\n",
    "print(x_dataAug_2.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_full = np.concatenate((x_train, x_dataAug_2))\n",
    "y_full = np.concatenate((y_train, y_dataAug_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_14 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_13 (Rescaling)       (None, 32, 32, 3)    0           ['input_14[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_76 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_13[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_110 (Batch  (None, 28, 28, 32)  128         ['conv2d_76[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_110 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_110[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_110 (Separabl  (None, 28, 28, 32)  1312        ['activation_110[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_111 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_110[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_111 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_111[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_111 (Separabl  (None, 28, 28, 32)  1312        ['activation_111[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_61 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_111[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_77 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_76[0][0]']              \n",
      "                                                                                                  \n",
      " add_55 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_61[0][0]',       \n",
      "                                                                  'conv2d_77[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_112 (Batch  (None, 14, 14, 32)  128         ['add_55[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_112 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_112[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_112 (Separabl  (None, 14, 14, 64)  2336        ['activation_112[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_113 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_112[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_113 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_113[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_113 (Separabl  (None, 14, 14, 64)  4672        ['activation_113[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_62 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_113[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_78 (Conv2D)             (None, 7, 7, 64)     2048        ['add_55[0][0]']                 \n",
      "                                                                                                  \n",
      " add_56 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_62[0][0]',       \n",
      "                                                                  'conv2d_78[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_114 (Batch  (None, 7, 7, 64)    256         ['add_56[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_114 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_114[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_114 (Separabl  (None, 7, 7, 128)   8768        ['activation_114[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_115 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_114[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_115 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_115[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_115 (Separabl  (None, 7, 7, 128)   17536       ['activation_115[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_63 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_115[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_79 (Conv2D)             (None, 4, 4, 128)    8192        ['add_56[0][0]']                 \n",
      "                                                                                                  \n",
      " add_57 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_63[0][0]',       \n",
      "                                                                  'conv2d_79[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_116 (Batch  (None, 4, 4, 128)   512         ['add_57[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_116 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_116[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_116 (Separabl  (None, 4, 4, 256)   33920       ['activation_116[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_117 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_116[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_117 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_117[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_117 (Separabl  (None, 4, 4, 256)   67840       ['activation_117[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_64 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_117[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_80 (Conv2D)             (None, 2, 2, 256)    32768       ['add_57[0][0]']                 \n",
      "                                                                                                  \n",
      " add_58 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_64[0][0]',       \n",
      "                                                                  'conv2d_80[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_10 (Flatten)           (None, 1024)         0           ['add_58[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_47 (Dropout)           (None, 1024)         0           ['flatten_10[0][0]']             \n",
      "                                                                                                  \n",
      " dense_45 (Dense)               (None, 256)          262400      ['dropout_47[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_48 (Dropout)           (None, 256)          0           ['dense_45[0][0]']               \n",
      "                                                                                                  \n",
      " dense_46 (Dense)               (None, 128)          32896       ['dropout_48[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_49 (Dropout)           (None, 128)          0           ['dense_46[0][0]']               \n",
      "                                                                                                  \n",
      " dense_47 (Dense)               (None, 100)          12900       ['dropout_49[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_with_data_augmentation_fullDS.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 4.0324 - accuracy: 0.0831 - val_loss: 3.5463 - val_accuracy: 0.1706\n",
      "Epoch 2/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.5902 - accuracy: 0.1485 - val_loss: 3.2915 - val_accuracy: 0.2133\n",
      "Epoch 3/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.3778 - accuracy: 0.1827 - val_loss: 3.1721 - val_accuracy: 0.2281\n",
      "Epoch 4/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 3.2148 - accuracy: 0.2103 - val_loss: 3.0150 - val_accuracy: 0.2572\n",
      "Epoch 5/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.0813 - accuracy: 0.2361 - val_loss: 2.8813 - val_accuracy: 0.2815\n",
      "Epoch 6/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.9695 - accuracy: 0.2553 - val_loss: 2.9046 - val_accuracy: 0.2829\n",
      "Epoch 7/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.8628 - accuracy: 0.2745 - val_loss: 2.7153 - val_accuracy: 0.3109\n",
      "Epoch 8/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.7703 - accuracy: 0.2939 - val_loss: 2.6935 - val_accuracy: 0.3197\n",
      "Epoch 9/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6842 - accuracy: 0.3082 - val_loss: 2.6688 - val_accuracy: 0.3184\n",
      "Epoch 10/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6037 - accuracy: 0.3252 - val_loss: 2.5660 - val_accuracy: 0.3429\n",
      "Epoch 11/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.5442 - accuracy: 0.3389 - val_loss: 2.5344 - val_accuracy: 0.3462\n",
      "Epoch 12/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4729 - accuracy: 0.3530 - val_loss: 2.4784 - val_accuracy: 0.3569\n",
      "Epoch 13/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4131 - accuracy: 0.3643 - val_loss: 2.5013 - val_accuracy: 0.3546\n",
      "Epoch 14/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3584 - accuracy: 0.3753 - val_loss: 2.4198 - val_accuracy: 0.3719\n",
      "Epoch 15/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3025 - accuracy: 0.3876 - val_loss: 2.4609 - val_accuracy: 0.3651\n",
      "Epoch 16/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2558 - accuracy: 0.3954 - val_loss: 2.3923 - val_accuracy: 0.3792\n",
      "Epoch 17/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2060 - accuracy: 0.4069 - val_loss: 2.4002 - val_accuracy: 0.3843\n",
      "Epoch 18/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.1653 - accuracy: 0.4162 - val_loss: 2.3571 - val_accuracy: 0.3901\n",
      "Epoch 19/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1244 - accuracy: 0.4246 - val_loss: 2.3570 - val_accuracy: 0.3918\n",
      "Epoch 20/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0761 - accuracy: 0.4347 - val_loss: 2.3977 - val_accuracy: 0.3865\n",
      "Epoch 21/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.0392 - accuracy: 0.4422 - val_loss: 2.3558 - val_accuracy: 0.3953\n",
      "Epoch 22/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0079 - accuracy: 0.4489 - val_loss: 2.3444 - val_accuracy: 0.3962\n",
      "Epoch 23/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9649 - accuracy: 0.4586 - val_loss: 2.3414 - val_accuracy: 0.4006\n",
      "Epoch 24/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9331 - accuracy: 0.4647 - val_loss: 2.3326 - val_accuracy: 0.4031\n",
      "Epoch 25/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8961 - accuracy: 0.4743 - val_loss: 2.3473 - val_accuracy: 0.4065\n",
      "Epoch 26/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8653 - accuracy: 0.4801 - val_loss: 2.4232 - val_accuracy: 0.3916\n",
      "Epoch 27/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8357 - accuracy: 0.4859 - val_loss: 2.3478 - val_accuracy: 0.4097\n",
      "Epoch 28/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8089 - accuracy: 0.4928 - val_loss: 2.3501 - val_accuracy: 0.4098\n",
      "Epoch 29/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7705 - accuracy: 0.5005 - val_loss: 2.3518 - val_accuracy: 0.4103\n",
      "Epoch 30/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7418 - accuracy: 0.5073 - val_loss: 2.3688 - val_accuracy: 0.4073\n",
      "Epoch 31/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7255 - accuracy: 0.5114 - val_loss: 2.3455 - val_accuracy: 0.4146\n",
      "Epoch 32/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6988 - accuracy: 0.5167 - val_loss: 2.3800 - val_accuracy: 0.4075\n",
      "Epoch 33/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6726 - accuracy: 0.5208 - val_loss: 2.3753 - val_accuracy: 0.4128\n",
      "Epoch 34/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.6489 - accuracy: 0.5276 - val_loss: 2.3637 - val_accuracy: 0.4163\n",
      "Epoch 35/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6237 - accuracy: 0.5332 - val_loss: 2.3854 - val_accuracy: 0.4050\n",
      "Epoch 36/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6031 - accuracy: 0.5377 - val_loss: 2.4133 - val_accuracy: 0.4129\n",
      "Epoch 37/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5800 - accuracy: 0.5432 - val_loss: 2.4024 - val_accuracy: 0.4139\n",
      "Epoch 38/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5583 - accuracy: 0.5482 - val_loss: 2.4374 - val_accuracy: 0.4084\n",
      "Epoch 39/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5370 - accuracy: 0.5542 - val_loss: 2.4171 - val_accuracy: 0.4115\n",
      "Epoch 40/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5175 - accuracy: 0.5589 - val_loss: 2.4368 - val_accuracy: 0.4098\n",
      "Epoch 41/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4990 - accuracy: 0.5635 - val_loss: 2.4166 - val_accuracy: 0.4116\n",
      "Epoch 42/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4772 - accuracy: 0.5664 - val_loss: 2.4348 - val_accuracy: 0.4176\n",
      "Epoch 43/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4626 - accuracy: 0.5727 - val_loss: 2.4467 - val_accuracy: 0.4130\n",
      "Epoch 44/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4405 - accuracy: 0.5764 - val_loss: 2.4600 - val_accuracy: 0.4132\n",
      "Epoch 45/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4283 - accuracy: 0.5804 - val_loss: 2.4548 - val_accuracy: 0.4159\n",
      "Epoch 46/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4098 - accuracy: 0.5827 - val_loss: 2.4731 - val_accuracy: 0.4115\n",
      "Epoch 47/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3963 - accuracy: 0.5873 - val_loss: 2.4769 - val_accuracy: 0.4156\n",
      "Epoch 48/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3801 - accuracy: 0.5924 - val_loss: 2.4790 - val_accuracy: 0.4147\n",
      "Epoch 49/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3613 - accuracy: 0.5961 - val_loss: 2.5213 - val_accuracy: 0.4157\n",
      "Epoch 50/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3508 - accuracy: 0.5972 - val_loss: 2.4839 - val_accuracy: 0.4150\n",
      "Epoch 51/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3338 - accuracy: 0.6029 - val_loss: 2.5219 - val_accuracy: 0.4115\n",
      "Epoch 52/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3214 - accuracy: 0.6071 - val_loss: 2.4935 - val_accuracy: 0.4182\n",
      "Epoch 53/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3077 - accuracy: 0.6094 - val_loss: 2.5628 - val_accuracy: 0.4150\n",
      "Epoch 54/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2903 - accuracy: 0.6146 - val_loss: 2.5965 - val_accuracy: 0.4092\n",
      "Epoch 55/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2869 - accuracy: 0.6159 - val_loss: 2.5811 - val_accuracy: 0.4060\n",
      "Epoch 56/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2671 - accuracy: 0.6185 - val_loss: 2.5267 - val_accuracy: 0.4133\n",
      "Epoch 57/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2605 - accuracy: 0.6220 - val_loss: 2.5393 - val_accuracy: 0.4135\n",
      "Epoch 58/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2486 - accuracy: 0.6229 - val_loss: 2.5930 - val_accuracy: 0.4145\n",
      "Epoch 59/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2289 - accuracy: 0.6291 - val_loss: 2.5995 - val_accuracy: 0.4141\n",
      "Epoch 60/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2201 - accuracy: 0.6313 - val_loss: 2.6180 - val_accuracy: 0.4114\n",
      "Epoch 61/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2147 - accuracy: 0.6324 - val_loss: 2.5865 - val_accuracy: 0.4142\n",
      "Epoch 62/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2011 - accuracy: 0.6370 - val_loss: 2.5987 - val_accuracy: 0.4154\n",
      "Epoch 63/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1860 - accuracy: 0.6396 - val_loss: 2.6628 - val_accuracy: 0.4074\n",
      "Epoch 64/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1755 - accuracy: 0.6420 - val_loss: 2.6416 - val_accuracy: 0.4135\n",
      "Epoch 65/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1682 - accuracy: 0.6446 - val_loss: 2.6512 - val_accuracy: 0.4141\n",
      "Epoch 66/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1554 - accuracy: 0.6467 - val_loss: 2.6722 - val_accuracy: 0.4063\n",
      "Epoch 67/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1446 - accuracy: 0.6492 - val_loss: 2.6321 - val_accuracy: 0.4131\n",
      "Epoch 68/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1422 - accuracy: 0.6521 - val_loss: 2.6416 - val_accuracy: 0.4109\n",
      "Epoch 69/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1289 - accuracy: 0.6553 - val_loss: 2.7382 - val_accuracy: 0.4032\n",
      "Epoch 70/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1245 - accuracy: 0.6569 - val_loss: 2.6813 - val_accuracy: 0.4147\n",
      "Epoch 71/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1159 - accuracy: 0.6589 - val_loss: 2.7111 - val_accuracy: 0.4089\n",
      "Epoch 72/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1025 - accuracy: 0.6617 - val_loss: 2.6757 - val_accuracy: 0.4056\n",
      "Epoch 73/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0947 - accuracy: 0.6656 - val_loss: 2.6666 - val_accuracy: 0.4171\n",
      "Epoch 74/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0915 - accuracy: 0.6649 - val_loss: 2.8168 - val_accuracy: 0.3993\n",
      "Epoch 75/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0787 - accuracy: 0.6687 - val_loss: 2.7525 - val_accuracy: 0.4048\n",
      "Epoch 76/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0726 - accuracy: 0.6706 - val_loss: 2.7064 - val_accuracy: 0.4116\n",
      "Epoch 77/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0665 - accuracy: 0.6728 - val_loss: 2.8012 - val_accuracy: 0.4053\n",
      "Epoch 78/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0534 - accuracy: 0.6755 - val_loss: 2.7874 - val_accuracy: 0.4080\n",
      "Epoch 79/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0484 - accuracy: 0.6768 - val_loss: 2.7384 - val_accuracy: 0.4118\n",
      "Epoch 80/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0449 - accuracy: 0.6766 - val_loss: 2.7432 - val_accuracy: 0.4102\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_full, y_full,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy: 41.76%.  Almost 2% better accuracy when combining original dataset with blended dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_2X = np.concatenate((x_train, x_train))\n",
    "y_train_2X = np.concatenate((y_train, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_15 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " rescaling_14 (Rescaling)       (None, 32, 32, 3)    0           ['input_15[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_81 (Conv2D)             (None, 28, 28, 32)   2400        ['rescaling_14[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_118 (Batch  (None, 28, 28, 32)  128         ['conv2d_81[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_118 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_118[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_118 (Separabl  (None, 28, 28, 32)  1312        ['activation_118[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_119 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_118[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_119 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_119[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_119 (Separabl  (None, 28, 28, 32)  1312        ['activation_119[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_65 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_119[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_82 (Conv2D)             (None, 14, 14, 32)   1024        ['conv2d_81[0][0]']              \n",
      "                                                                                                  \n",
      " add_59 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_65[0][0]',       \n",
      "                                                                  'conv2d_82[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_120 (Batch  (None, 14, 14, 32)  128         ['add_59[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_120 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_120[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_120 (Separabl  (None, 14, 14, 64)  2336        ['activation_120[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_121 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_120[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_121 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_121[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_121 (Separabl  (None, 14, 14, 64)  4672        ['activation_121[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_66 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_121[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_83 (Conv2D)             (None, 7, 7, 64)     2048        ['add_59[0][0]']                 \n",
      "                                                                                                  \n",
      " add_60 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_66[0][0]',       \n",
      "                                                                  'conv2d_83[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_122 (Batch  (None, 7, 7, 64)    256         ['add_60[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_122 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_122[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_122 (Separabl  (None, 7, 7, 128)   8768        ['activation_122[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_123 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_122[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_123 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_123[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_123 (Separabl  (None, 7, 7, 128)   17536       ['activation_123[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_67 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_123[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_84 (Conv2D)             (None, 4, 4, 128)    8192        ['add_60[0][0]']                 \n",
      "                                                                                                  \n",
      " add_61 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_67[0][0]',       \n",
      "                                                                  'conv2d_84[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_124 (Batch  (None, 4, 4, 128)   512         ['add_61[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_124 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_124[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_124 (Separabl  (None, 4, 4, 256)   33920       ['activation_124[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_125 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_124[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_125 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_125[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_125 (Separabl  (None, 4, 4, 256)   67840       ['activation_125[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_68 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_125[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_85 (Conv2D)             (None, 2, 2, 256)    32768       ['add_61[0][0]']                 \n",
      "                                                                                                  \n",
      " add_62 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_68[0][0]',       \n",
      "                                                                  'conv2d_85[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_11 (Flatten)           (None, 1024)         0           ['add_62[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_50 (Dropout)           (None, 1024)         0           ['flatten_11[0][0]']             \n",
      "                                                                                                  \n",
      " dense_48 (Dense)               (None, 256)          262400      ['dropout_50[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_51 (Dropout)           (None, 256)          0           ['dense_48[0][0]']               \n",
      "                                                                                                  \n",
      " dense_49 (Dense)               (None, 128)          32896       ['dropout_51[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_52 (Dropout)           (None, 128)          0           ['dense_49[0][0]']               \n",
      "                                                                                                  \n",
      " dense_50 (Dense)               (None, 100)          12900       ['dropout_52[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_DS2X.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 4.0519 - accuracy: 0.0807 - val_loss: 3.6024 - val_accuracy: 0.1607\n",
      "Epoch 2/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.6292 - accuracy: 0.1392 - val_loss: 3.3436 - val_accuracy: 0.2066\n",
      "Epoch 3/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.4199 - accuracy: 0.1757 - val_loss: 3.1548 - val_accuracy: 0.2374\n",
      "Epoch 4/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.2517 - accuracy: 0.2035 - val_loss: 3.0417 - val_accuracy: 0.2553\n",
      "Epoch 5/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 3.1044 - accuracy: 0.2307 - val_loss: 2.8926 - val_accuracy: 0.2870\n",
      "Epoch 6/80\n",
      "1563/1563 [==============================] - 7s 5ms/step - loss: 2.9737 - accuracy: 0.2528 - val_loss: 2.7940 - val_accuracy: 0.2963\n",
      "Epoch 7/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.8680 - accuracy: 0.2718 - val_loss: 2.7598 - val_accuracy: 0.3083\n",
      "Epoch 8/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.7706 - accuracy: 0.2902 - val_loss: 2.6581 - val_accuracy: 0.3254\n",
      "Epoch 9/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6872 - accuracy: 0.3075 - val_loss: 2.6256 - val_accuracy: 0.3314\n",
      "Epoch 10/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6062 - accuracy: 0.3244 - val_loss: 2.5702 - val_accuracy: 0.3437\n",
      "Epoch 11/80\n",
      "1563/1563 [==============================] - 9s 6ms/step - loss: 2.5365 - accuracy: 0.3371 - val_loss: 2.4968 - val_accuracy: 0.3589\n",
      "Epoch 12/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.4724 - accuracy: 0.3516 - val_loss: 2.4708 - val_accuracy: 0.3641\n",
      "Epoch 13/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.4111 - accuracy: 0.3631 - val_loss: 2.4777 - val_accuracy: 0.3659\n",
      "Epoch 14/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.3594 - accuracy: 0.3718 - val_loss: 2.4729 - val_accuracy: 0.3664\n",
      "Epoch 15/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.3067 - accuracy: 0.3855 - val_loss: 2.4206 - val_accuracy: 0.3747\n",
      "Epoch 16/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.2541 - accuracy: 0.3971 - val_loss: 2.4055 - val_accuracy: 0.3824\n",
      "Epoch 17/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2096 - accuracy: 0.4062 - val_loss: 2.5261 - val_accuracy: 0.3590\n",
      "Epoch 18/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.1549 - accuracy: 0.4179 - val_loss: 2.4012 - val_accuracy: 0.3837\n",
      "Epoch 19/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 2.1188 - accuracy: 0.4234 - val_loss: 2.3743 - val_accuracy: 0.3887\n",
      "Epoch 20/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0785 - accuracy: 0.4336 - val_loss: 2.4169 - val_accuracy: 0.3846\n",
      "Epoch 21/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0384 - accuracy: 0.4429 - val_loss: 2.4228 - val_accuracy: 0.3843\n",
      "Epoch 22/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9938 - accuracy: 0.4493 - val_loss: 2.4274 - val_accuracy: 0.3823\n",
      "Epoch 23/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9600 - accuracy: 0.4570 - val_loss: 2.3777 - val_accuracy: 0.3940\n",
      "Epoch 24/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.9232 - accuracy: 0.4666 - val_loss: 2.3634 - val_accuracy: 0.3981\n",
      "Epoch 25/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8922 - accuracy: 0.4746 - val_loss: 2.3868 - val_accuracy: 0.3979\n",
      "Epoch 26/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8547 - accuracy: 0.4802 - val_loss: 2.4034 - val_accuracy: 0.3956\n",
      "Epoch 27/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.8200 - accuracy: 0.4889 - val_loss: 2.4260 - val_accuracy: 0.3926\n",
      "Epoch 28/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7903 - accuracy: 0.4938 - val_loss: 2.4354 - val_accuracy: 0.3945\n",
      "Epoch 29/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7551 - accuracy: 0.5041 - val_loss: 2.4136 - val_accuracy: 0.3978\n",
      "Epoch 30/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7326 - accuracy: 0.5083 - val_loss: 2.3973 - val_accuracy: 0.4015\n",
      "Epoch 31/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7074 - accuracy: 0.5137 - val_loss: 2.4227 - val_accuracy: 0.4021\n",
      "Epoch 32/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6814 - accuracy: 0.5190 - val_loss: 2.4918 - val_accuracy: 0.3909\n",
      "Epoch 33/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6562 - accuracy: 0.5272 - val_loss: 2.5164 - val_accuracy: 0.3913\n",
      "Epoch 34/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6314 - accuracy: 0.5309 - val_loss: 2.4778 - val_accuracy: 0.4058\n",
      "Epoch 35/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6051 - accuracy: 0.5361 - val_loss: 2.4959 - val_accuracy: 0.3975\n",
      "Epoch 36/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5770 - accuracy: 0.5437 - val_loss: 2.4679 - val_accuracy: 0.4032\n",
      "Epoch 37/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5580 - accuracy: 0.5480 - val_loss: 2.5143 - val_accuracy: 0.4005\n",
      "Epoch 38/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5356 - accuracy: 0.5531 - val_loss: 2.5120 - val_accuracy: 0.4007\n",
      "Epoch 39/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5117 - accuracy: 0.5583 - val_loss: 2.5260 - val_accuracy: 0.4030\n",
      "Epoch 40/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4878 - accuracy: 0.5654 - val_loss: 2.5304 - val_accuracy: 0.4026\n",
      "Epoch 41/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4758 - accuracy: 0.5673 - val_loss: 2.5782 - val_accuracy: 0.3953\n",
      "Epoch 42/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4580 - accuracy: 0.5703 - val_loss: 2.5667 - val_accuracy: 0.3989\n",
      "Epoch 43/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4366 - accuracy: 0.5772 - val_loss: 2.5275 - val_accuracy: 0.4045\n",
      "Epoch 44/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.4194 - accuracy: 0.5806 - val_loss: 2.5911 - val_accuracy: 0.4033\n",
      "Epoch 45/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3978 - accuracy: 0.5855 - val_loss: 2.6059 - val_accuracy: 0.3973\n",
      "Epoch 46/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3817 - accuracy: 0.5910 - val_loss: 2.6220 - val_accuracy: 0.4011\n",
      "Epoch 47/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3655 - accuracy: 0.5939 - val_loss: 2.6465 - val_accuracy: 0.3966\n",
      "Epoch 48/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3413 - accuracy: 0.5981 - val_loss: 2.6673 - val_accuracy: 0.3985\n",
      "Epoch 49/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3373 - accuracy: 0.6009 - val_loss: 2.6288 - val_accuracy: 0.4006\n",
      "Epoch 50/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3189 - accuracy: 0.6064 - val_loss: 2.6534 - val_accuracy: 0.3986\n",
      "Epoch 51/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3024 - accuracy: 0.6107 - val_loss: 2.7158 - val_accuracy: 0.3957\n",
      "Epoch 52/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2876 - accuracy: 0.6136 - val_loss: 2.6842 - val_accuracy: 0.3996\n",
      "Epoch 53/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2789 - accuracy: 0.6162 - val_loss: 2.6930 - val_accuracy: 0.3992\n",
      "Epoch 54/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2600 - accuracy: 0.6186 - val_loss: 2.7388 - val_accuracy: 0.3937\n",
      "Epoch 55/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2483 - accuracy: 0.6232 - val_loss: 2.7948 - val_accuracy: 0.3891\n",
      "Epoch 56/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2375 - accuracy: 0.6274 - val_loss: 2.7469 - val_accuracy: 0.3968\n",
      "Epoch 57/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2218 - accuracy: 0.6323 - val_loss: 2.7633 - val_accuracy: 0.3980\n",
      "Epoch 58/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.2067 - accuracy: 0.6330 - val_loss: 2.7352 - val_accuracy: 0.4008\n",
      "Epoch 59/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1986 - accuracy: 0.6369 - val_loss: 2.8109 - val_accuracy: 0.3976\n",
      "Epoch 60/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1848 - accuracy: 0.6392 - val_loss: 2.7425 - val_accuracy: 0.3962\n",
      "Epoch 61/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1691 - accuracy: 0.6432 - val_loss: 2.8553 - val_accuracy: 0.3934\n",
      "Epoch 62/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1637 - accuracy: 0.6442 - val_loss: 2.8282 - val_accuracy: 0.3958\n",
      "Epoch 63/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1590 - accuracy: 0.6455 - val_loss: 2.8294 - val_accuracy: 0.3942\n",
      "Epoch 64/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1403 - accuracy: 0.6507 - val_loss: 2.8305 - val_accuracy: 0.3974\n",
      "Epoch 65/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1399 - accuracy: 0.6522 - val_loss: 2.9124 - val_accuracy: 0.3859\n",
      "Epoch 66/80\n",
      "1563/1563 [==============================] - 9s 5ms/step - loss: 1.1253 - accuracy: 0.6549 - val_loss: 2.8450 - val_accuracy: 0.3966\n",
      "Epoch 67/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1141 - accuracy: 0.6581 - val_loss: 2.9016 - val_accuracy: 0.3943\n",
      "Epoch 68/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0937 - accuracy: 0.6634 - val_loss: 2.9253 - val_accuracy: 0.3942\n",
      "Epoch 69/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0959 - accuracy: 0.6637 - val_loss: 2.9144 - val_accuracy: 0.3973\n",
      "Epoch 70/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0877 - accuracy: 0.6664 - val_loss: 2.8787 - val_accuracy: 0.3932\n",
      "Epoch 71/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0778 - accuracy: 0.6687 - val_loss: 2.9490 - val_accuracy: 0.3876\n",
      "Epoch 72/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0726 - accuracy: 0.6694 - val_loss: 2.8882 - val_accuracy: 0.3975\n",
      "Epoch 73/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0520 - accuracy: 0.6757 - val_loss: 2.9504 - val_accuracy: 0.3944\n",
      "Epoch 74/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0507 - accuracy: 0.6730 - val_loss: 2.9647 - val_accuracy: 0.3929\n",
      "Epoch 75/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0398 - accuracy: 0.6787 - val_loss: 2.9121 - val_accuracy: 0.3921\n",
      "Epoch 76/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0438 - accuracy: 0.6785 - val_loss: 2.9967 - val_accuracy: 0.3899\n",
      "Epoch 77/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0302 - accuracy: 0.6818 - val_loss: 3.0093 - val_accuracy: 0.3920\n",
      "Epoch 78/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0223 - accuracy: 0.6823 - val_loss: 2.9714 - val_accuracy: 0.3947\n",
      "Epoch 79/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0106 - accuracy: 0.6870 - val_loss: 2.9364 - val_accuracy: 0.3987\n",
      "Epoch 80/80\n",
      "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0065 - accuracy: 0.6887 - val_loss: 2.9553 - val_accuracy: 0.3978\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train_2X, y_train_2X,\n",
    "epochs=80,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Highest Val Accuracy: 40.40%. The combonation of the blended and origanal dataset was 1.25% better than the original dataset doubled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "[\n",
    "layers.RandomFlip(\"horizontal\"),\n",
    "layers.RandomRotation(0.1),\n",
    "layers.RandomZoom(0.1),\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_19\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_20 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 32, 32, 3)    0           ['input_20[0][0]']               \n",
      "                                                                                                  \n",
      " rescaling_19 (Rescaling)       (None, 32, 32, 3)    0           ['sequential_6[1][0]']           \n",
      "                                                                                                  \n",
      " conv2d_106 (Conv2D)            (None, 28, 28, 32)   2400        ['rescaling_19[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_158 (Batch  (None, 28, 28, 32)  128         ['conv2d_106[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_158 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_158[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_158 (Separabl  (None, 28, 28, 32)  1312        ['activation_158[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_159 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_158[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_159 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_159[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_159 (Separabl  (None, 28, 28, 32)  1312        ['activation_159[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_85 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_159[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_107 (Conv2D)            (None, 14, 14, 32)   1024        ['conv2d_106[0][0]']             \n",
      "                                                                                                  \n",
      " add_79 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_85[0][0]',       \n",
      "                                                                  'conv2d_107[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_160 (Batch  (None, 14, 14, 32)  128         ['add_79[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_160 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_160[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_160 (Separabl  (None, 14, 14, 64)  2336        ['activation_160[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_161 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_160[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_161 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_161[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_161 (Separabl  (None, 14, 14, 64)  4672        ['activation_161[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_86 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_161[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_108 (Conv2D)            (None, 7, 7, 64)     2048        ['add_79[0][0]']                 \n",
      "                                                                                                  \n",
      " add_80 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_86[0][0]',       \n",
      "                                                                  'conv2d_108[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_162 (Batch  (None, 7, 7, 64)    256         ['add_80[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_162 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_162[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_162 (Separabl  (None, 7, 7, 128)   8768        ['activation_162[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_163 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_162[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_163 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_163[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_163 (Separabl  (None, 7, 7, 128)   17536       ['activation_163[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_87 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_163[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_109 (Conv2D)            (None, 4, 4, 128)    8192        ['add_80[0][0]']                 \n",
      "                                                                                                  \n",
      " add_81 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_87[0][0]',       \n",
      "                                                                  'conv2d_109[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_164 (Batch  (None, 4, 4, 128)   512         ['add_81[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_164 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_164[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_164 (Separabl  (None, 4, 4, 256)   33920       ['activation_164[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_165 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_164[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_165 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_165[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_165 (Separabl  (None, 4, 4, 256)   67840       ['activation_165[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_88 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_165[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_110 (Conv2D)            (None, 2, 2, 256)    32768       ['add_81[0][0]']                 \n",
      "                                                                                                  \n",
      " add_82 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_88[0][0]',       \n",
      "                                                                  'conv2d_110[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_16 (Flatten)           (None, 1024)         0           ['add_82[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_65 (Dropout)           (None, 1024)         0           ['flatten_16[0][0]']             \n",
      "                                                                                                  \n",
      " dense_63 (Dense)               (None, 256)          262400      ['dropout_65[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_66 (Dropout)           (None, 256)          0           ['dense_63[0][0]']               \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 128)          32896       ['dropout_66[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_67 (Dropout)           (None, 128)          0           ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 100)          12900       ['dropout_67[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_fullds_classic.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 30s 18ms/step - loss: 4.0968 - accuracy: 0.0734 - val_loss: 3.6703 - val_accuracy: 0.1474\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.7326 - accuracy: 0.1221 - val_loss: 3.5050 - val_accuracy: 0.1681\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.5729 - accuracy: 0.1486 - val_loss: 3.3526 - val_accuracy: 0.1881\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.4479 - accuracy: 0.1704 - val_loss: 3.2298 - val_accuracy: 0.2100\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 3.3486 - accuracy: 0.1870 - val_loss: 3.0836 - val_accuracy: 0.2400\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.2641 - accuracy: 0.2037 - val_loss: 3.0809 - val_accuracy: 0.2425\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 3.1849 - accuracy: 0.2185 - val_loss: 3.0721 - val_accuracy: 0.2451\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 3.1269 - accuracy: 0.2293 - val_loss: 3.0372 - val_accuracy: 0.2485\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 3.0655 - accuracy: 0.2403 - val_loss: 3.0187 - val_accuracy: 0.2568\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 3.0147 - accuracy: 0.2509 - val_loss: 2.8048 - val_accuracy: 0.2982\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.9655 - accuracy: 0.2592 - val_loss: 3.0530 - val_accuracy: 0.2464\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.9284 - accuracy: 0.2680 - val_loss: 2.8262 - val_accuracy: 0.2887\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.8808 - accuracy: 0.2765 - val_loss: 2.9001 - val_accuracy: 0.2817\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.8463 - accuracy: 0.2838 - val_loss: 2.7031 - val_accuracy: 0.3130\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 2.8145 - accuracy: 0.2919 - val_loss: 2.6723 - val_accuracy: 0.3230\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.7764 - accuracy: 0.2973 - val_loss: 2.6767 - val_accuracy: 0.3230\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.7472 - accuracy: 0.3034 - val_loss: 2.6953 - val_accuracy: 0.3167\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.7169 - accuracy: 0.3111 - val_loss: 2.5986 - val_accuracy: 0.3376\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.6923 - accuracy: 0.3151 - val_loss: 2.6282 - val_accuracy: 0.3325\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 2.6597 - accuracy: 0.3213 - val_loss: 2.5297 - val_accuracy: 0.3526\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.6333 - accuracy: 0.3272 - val_loss: 2.5873 - val_accuracy: 0.3423\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.6136 - accuracy: 0.3313 - val_loss: 2.5905 - val_accuracy: 0.3413\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.5901 - accuracy: 0.3342 - val_loss: 2.6107 - val_accuracy: 0.3409\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.5709 - accuracy: 0.3413 - val_loss: 2.4540 - val_accuracy: 0.3685\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 24s 15ms/step - loss: 2.5493 - accuracy: 0.3449 - val_loss: 2.4896 - val_accuracy: 0.3605\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.5299 - accuracy: 0.3491 - val_loss: 2.4104 - val_accuracy: 0.3833\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.5089 - accuracy: 0.3537 - val_loss: 2.4479 - val_accuracy: 0.3723\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.4871 - accuracy: 0.3581 - val_loss: 2.3996 - val_accuracy: 0.3859\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.4676 - accuracy: 0.3611 - val_loss: 2.3253 - val_accuracy: 0.3983\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.4463 - accuracy: 0.3671 - val_loss: 2.4735 - val_accuracy: 0.3674\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4318 - accuracy: 0.3688 - val_loss: 2.3298 - val_accuracy: 0.3989\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.4146 - accuracy: 0.3729 - val_loss: 2.3950 - val_accuracy: 0.3838\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4092 - accuracy: 0.3751 - val_loss: 2.4302 - val_accuracy: 0.3768\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.3826 - accuracy: 0.3799 - val_loss: 2.3365 - val_accuracy: 0.3966\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.3696 - accuracy: 0.3825 - val_loss: 2.2807 - val_accuracy: 0.4067\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.3567 - accuracy: 0.3862 - val_loss: 2.3795 - val_accuracy: 0.3890\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3394 - accuracy: 0.3903 - val_loss: 2.3674 - val_accuracy: 0.3887\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3231 - accuracy: 0.3926 - val_loss: 2.3206 - val_accuracy: 0.4010\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.3190 - accuracy: 0.3941 - val_loss: 2.2767 - val_accuracy: 0.4121\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.3021 - accuracy: 0.3990 - val_loss: 2.3147 - val_accuracy: 0.3988\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.2883 - accuracy: 0.4020 - val_loss: 2.2890 - val_accuracy: 0.4045\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2733 - accuracy: 0.4054 - val_loss: 2.2689 - val_accuracy: 0.4123\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.2575 - accuracy: 0.4052 - val_loss: 2.2396 - val_accuracy: 0.4153\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.2495 - accuracy: 0.4093 - val_loss: 2.2333 - val_accuracy: 0.4181\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2373 - accuracy: 0.4114 - val_loss: 2.2835 - val_accuracy: 0.4097\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2240 - accuracy: 0.4144 - val_loss: 2.2491 - val_accuracy: 0.4120\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.2220 - accuracy: 0.4135 - val_loss: 2.2680 - val_accuracy: 0.4105\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.2031 - accuracy: 0.4187 - val_loss: 2.2243 - val_accuracy: 0.4212\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2008 - accuracy: 0.4187 - val_loss: 2.2003 - val_accuracy: 0.4267\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.1826 - accuracy: 0.4238 - val_loss: 2.2332 - val_accuracy: 0.4175\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.1721 - accuracy: 0.4266 - val_loss: 2.1879 - val_accuracy: 0.4243\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1682 - accuracy: 0.4262 - val_loss: 2.2449 - val_accuracy: 0.4208\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.1511 - accuracy: 0.4313 - val_loss: 2.1887 - val_accuracy: 0.4297\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.1451 - accuracy: 0.4332 - val_loss: 2.2506 - val_accuracy: 0.4178\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.1368 - accuracy: 0.4342 - val_loss: 2.2459 - val_accuracy: 0.4199\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.1242 - accuracy: 0.4378 - val_loss: 2.1675 - val_accuracy: 0.4363\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1144 - accuracy: 0.4388 - val_loss: 2.2364 - val_accuracy: 0.4192\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.1065 - accuracy: 0.4420 - val_loss: 2.1302 - val_accuracy: 0.4465\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.0987 - accuracy: 0.4425 - val_loss: 2.1545 - val_accuracy: 0.4393\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0919 - accuracy: 0.4447 - val_loss: 2.2177 - val_accuracy: 0.4281\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.0800 - accuracy: 0.4482 - val_loss: 2.1819 - val_accuracy: 0.4313\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.0719 - accuracy: 0.4477 - val_loss: 2.1068 - val_accuracy: 0.4526\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0628 - accuracy: 0.4504 - val_loss: 2.1298 - val_accuracy: 0.4449\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.0519 - accuracy: 0.4518 - val_loss: 2.1478 - val_accuracy: 0.4422\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.0442 - accuracy: 0.4541 - val_loss: 2.1437 - val_accuracy: 0.4393\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0389 - accuracy: 0.4561 - val_loss: 2.1444 - val_accuracy: 0.4411\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.0302 - accuracy: 0.4564 - val_loss: 2.1873 - val_accuracy: 0.4336\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0168 - accuracy: 0.4595 - val_loss: 2.1501 - val_accuracy: 0.4414\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0241 - accuracy: 0.4602 - val_loss: 2.1045 - val_accuracy: 0.4517\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0031 - accuracy: 0.4641 - val_loss: 2.0819 - val_accuracy: 0.4574\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9953 - accuracy: 0.4643 - val_loss: 2.1283 - val_accuracy: 0.4498\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9905 - accuracy: 0.4679 - val_loss: 2.0669 - val_accuracy: 0.4558\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9849 - accuracy: 0.4687 - val_loss: 2.0954 - val_accuracy: 0.4534\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9812 - accuracy: 0.4684 - val_loss: 2.1203 - val_accuracy: 0.4475\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9716 - accuracy: 0.4709 - val_loss: 2.1266 - val_accuracy: 0.4492\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.9631 - accuracy: 0.4710 - val_loss: 2.0753 - val_accuracy: 0.4532\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9607 - accuracy: 0.4745 - val_loss: 2.1630 - val_accuracy: 0.4362\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.9513 - accuracy: 0.4755 - val_loss: 2.0717 - val_accuracy: 0.4633\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9430 - accuracy: 0.4778 - val_loss: 2.0841 - val_accuracy: 0.4571\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.9384 - accuracy: 0.4784 - val_loss: 2.0799 - val_accuracy: 0.4573\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9307 - accuracy: 0.4798 - val_loss: 2.0724 - val_accuracy: 0.4598\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9289 - accuracy: 0.4814 - val_loss: 2.1044 - val_accuracy: 0.4545\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9203 - accuracy: 0.4803 - val_loss: 2.0465 - val_accuracy: 0.4636\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9178 - accuracy: 0.4830 - val_loss: 2.0554 - val_accuracy: 0.4670\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9116 - accuracy: 0.4869 - val_loss: 2.1023 - val_accuracy: 0.4570\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.9014 - accuracy: 0.4867 - val_loss: 2.0578 - val_accuracy: 0.4607\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8890 - accuracy: 0.4880 - val_loss: 2.0347 - val_accuracy: 0.4647\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8923 - accuracy: 0.4890 - val_loss: 2.0332 - val_accuracy: 0.4689\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8873 - accuracy: 0.4898 - val_loss: 2.0160 - val_accuracy: 0.4699\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8809 - accuracy: 0.4912 - val_loss: 2.1024 - val_accuracy: 0.4565\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8813 - accuracy: 0.4923 - val_loss: 2.0197 - val_accuracy: 0.4699\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.8723 - accuracy: 0.4946 - val_loss: 2.0021 - val_accuracy: 0.4765\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.8600 - accuracy: 0.4953 - val_loss: 2.0722 - val_accuracy: 0.4591\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8601 - accuracy: 0.4953 - val_loss: 2.0911 - val_accuracy: 0.4633\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8579 - accuracy: 0.4967 - val_loss: 2.0878 - val_accuracy: 0.4575\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8450 - accuracy: 0.4981 - val_loss: 1.9921 - val_accuracy: 0.4775\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.8444 - accuracy: 0.4997 - val_loss: 2.0093 - val_accuracy: 0.4752\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8365 - accuracy: 0.5017 - val_loss: 2.1230 - val_accuracy: 0.4569\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8317 - accuracy: 0.5031 - val_loss: 2.0136 - val_accuracy: 0.4704\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8277 - accuracy: 0.5045 - val_loss: 1.9986 - val_accuracy: 0.4791\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8249 - accuracy: 0.5053 - val_loss: 1.9499 - val_accuracy: 0.4883\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8203 - accuracy: 0.5057 - val_loss: 2.0018 - val_accuracy: 0.4735\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8131 - accuracy: 0.5062 - val_loss: 2.0589 - val_accuracy: 0.4655\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.8148 - accuracy: 0.5063 - val_loss: 1.9687 - val_accuracy: 0.4787\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8056 - accuracy: 0.5082 - val_loss: 1.9817 - val_accuracy: 0.4797\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8010 - accuracy: 0.5097 - val_loss: 2.0568 - val_accuracy: 0.4673\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7945 - accuracy: 0.5121 - val_loss: 1.9711 - val_accuracy: 0.4794\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7859 - accuracy: 0.5128 - val_loss: 2.0466 - val_accuracy: 0.4721\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7899 - accuracy: 0.5139 - val_loss: 2.0325 - val_accuracy: 0.4741\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7796 - accuracy: 0.5145 - val_loss: 2.0246 - val_accuracy: 0.4754\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7702 - accuracy: 0.5137 - val_loss: 2.0079 - val_accuracy: 0.4769\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7734 - accuracy: 0.5151 - val_loss: 1.9803 - val_accuracy: 0.4843\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7678 - accuracy: 0.5181 - val_loss: 1.9622 - val_accuracy: 0.4856\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7625 - accuracy: 0.5201 - val_loss: 1.9852 - val_accuracy: 0.4794\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7581 - accuracy: 0.5197 - val_loss: 1.9928 - val_accuracy: 0.4847\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7540 - accuracy: 0.5195 - val_loss: 2.0388 - val_accuracy: 0.4719\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7524 - accuracy: 0.5214 - val_loss: 1.9942 - val_accuracy: 0.4829\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7448 - accuracy: 0.5235 - val_loss: 1.9716 - val_accuracy: 0.4814\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7495 - accuracy: 0.5210 - val_loss: 2.0196 - val_accuracy: 0.4747\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7460 - accuracy: 0.5220 - val_loss: 2.0208 - val_accuracy: 0.4784\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7320 - accuracy: 0.5256 - val_loss: 1.9596 - val_accuracy: 0.4856\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7350 - accuracy: 0.5259 - val_loss: 2.0516 - val_accuracy: 0.4738\n",
      "Epoch 123/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7296 - accuracy: 0.5282 - val_loss: 1.9574 - val_accuracy: 0.4916\n",
      "Epoch 124/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7281 - accuracy: 0.5287 - val_loss: 2.0536 - val_accuracy: 0.4738\n",
      "Epoch 125/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7197 - accuracy: 0.5292 - val_loss: 2.0181 - val_accuracy: 0.4803\n",
      "Epoch 126/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7165 - accuracy: 0.5305 - val_loss: 1.9965 - val_accuracy: 0.4824\n",
      "Epoch 127/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7139 - accuracy: 0.5297 - val_loss: 1.9863 - val_accuracy: 0.4900\n",
      "Epoch 128/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7096 - accuracy: 0.5307 - val_loss: 2.0043 - val_accuracy: 0.4828\n",
      "Epoch 129/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7052 - accuracy: 0.5322 - val_loss: 1.9530 - val_accuracy: 0.4882\n",
      "Epoch 130/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7056 - accuracy: 0.5325 - val_loss: 1.9287 - val_accuracy: 0.4977\n",
      "Epoch 131/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7029 - accuracy: 0.5329 - val_loss: 1.9762 - val_accuracy: 0.4882\n",
      "Epoch 132/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6979 - accuracy: 0.5319 - val_loss: 1.9953 - val_accuracy: 0.4835\n",
      "Epoch 133/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6918 - accuracy: 0.5333 - val_loss: 1.9785 - val_accuracy: 0.4862\n",
      "Epoch 134/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6938 - accuracy: 0.5361 - val_loss: 1.9460 - val_accuracy: 0.4911\n",
      "Epoch 135/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.6865 - accuracy: 0.5358 - val_loss: 1.9878 - val_accuracy: 0.4864\n",
      "Epoch 136/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6864 - accuracy: 0.5363 - val_loss: 1.9199 - val_accuracy: 0.4999\n",
      "Epoch 137/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6804 - accuracy: 0.5390 - val_loss: 1.9303 - val_accuracy: 0.4938\n",
      "Epoch 138/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.6733 - accuracy: 0.5406 - val_loss: 1.9486 - val_accuracy: 0.4942\n",
      "Epoch 139/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6602 - accuracy: 0.5415 - val_loss: 1.9727 - val_accuracy: 0.4936\n",
      "Epoch 140/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6676 - accuracy: 0.5379 - val_loss: 1.9258 - val_accuracy: 0.4979\n",
      "Epoch 141/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6637 - accuracy: 0.5420 - val_loss: 1.9407 - val_accuracy: 0.4933\n",
      "Epoch 142/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6573 - accuracy: 0.5412 - val_loss: 1.9633 - val_accuracy: 0.4875\n",
      "Epoch 143/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6624 - accuracy: 0.5430 - val_loss: 1.9105 - val_accuracy: 0.5032\n",
      "Epoch 144/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6503 - accuracy: 0.5426 - val_loss: 1.9851 - val_accuracy: 0.4856\n",
      "Epoch 145/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6499 - accuracy: 0.5449 - val_loss: 1.9783 - val_accuracy: 0.4905\n",
      "Epoch 146/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6455 - accuracy: 0.5444 - val_loss: 1.9549 - val_accuracy: 0.4938\n",
      "Epoch 147/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.6444 - accuracy: 0.5463 - val_loss: 1.9680 - val_accuracy: 0.4895\n",
      "Epoch 148/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6409 - accuracy: 0.5472 - val_loss: 1.9517 - val_accuracy: 0.4927\n",
      "Epoch 149/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.6392 - accuracy: 0.5478 - val_loss: 2.0036 - val_accuracy: 0.4818\n",
      "Epoch 150/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6338 - accuracy: 0.5481 - val_loss: 1.9434 - val_accuracy: 0.4925\n",
      "Epoch 151/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6273 - accuracy: 0.5519 - val_loss: 1.9115 - val_accuracy: 0.5042\n",
      "Epoch 152/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6314 - accuracy: 0.5490 - val_loss: 1.9240 - val_accuracy: 0.5020\n",
      "Epoch 153/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6268 - accuracy: 0.5493 - val_loss: 1.9084 - val_accuracy: 0.5005\n",
      "Epoch 154/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6250 - accuracy: 0.5504 - val_loss: 1.9891 - val_accuracy: 0.4915\n",
      "Epoch 155/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.6133 - accuracy: 0.5507 - val_loss: 1.8957 - val_accuracy: 0.5039\n",
      "Epoch 156/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6159 - accuracy: 0.5519 - val_loss: 1.9527 - val_accuracy: 0.4989\n",
      "Epoch 157/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6227 - accuracy: 0.5515 - val_loss: 1.9286 - val_accuracy: 0.4999\n",
      "Epoch 158/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.6099 - accuracy: 0.5529 - val_loss: 1.9684 - val_accuracy: 0.4923\n",
      "Epoch 159/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.6082 - accuracy: 0.5549 - val_loss: 1.8969 - val_accuracy: 0.5032\n",
      "Epoch 160/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.5991 - accuracy: 0.5568 - val_loss: 1.9441 - val_accuracy: 0.4956\n",
      "Epoch 161/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6021 - accuracy: 0.5552 - val_loss: 1.9680 - val_accuracy: 0.4964\n",
      "Epoch 162/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5989 - accuracy: 0.5573 - val_loss: 1.9096 - val_accuracy: 0.5058\n",
      "Epoch 163/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5984 - accuracy: 0.5571 - val_loss: 1.9394 - val_accuracy: 0.4986\n",
      "Epoch 164/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5873 - accuracy: 0.5581 - val_loss: 1.9732 - val_accuracy: 0.4930\n",
      "Epoch 165/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5923 - accuracy: 0.5586 - val_loss: 1.8654 - val_accuracy: 0.5131\n",
      "Epoch 166/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5849 - accuracy: 0.5592 - val_loss: 1.8833 - val_accuracy: 0.5087\n",
      "Epoch 167/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5854 - accuracy: 0.5609 - val_loss: 1.9340 - val_accuracy: 0.5037\n",
      "Epoch 168/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5887 - accuracy: 0.5587 - val_loss: 1.9175 - val_accuracy: 0.5025\n",
      "Epoch 169/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.5832 - accuracy: 0.5605 - val_loss: 1.9865 - val_accuracy: 0.4931\n",
      "Epoch 170/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5754 - accuracy: 0.5618 - val_loss: 1.9501 - val_accuracy: 0.4994\n",
      "Epoch 171/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 1.5748 - accuracy: 0.5628 - val_loss: 1.9532 - val_accuracy: 0.5035\n",
      "Epoch 172/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5718 - accuracy: 0.5636 - val_loss: 1.9495 - val_accuracy: 0.4972\n",
      "Epoch 173/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5674 - accuracy: 0.5644 - val_loss: 1.9213 - val_accuracy: 0.5044\n",
      "Epoch 174/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.5699 - accuracy: 0.5646 - val_loss: 1.8881 - val_accuracy: 0.5072\n",
      "Epoch 175/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5657 - accuracy: 0.5635 - val_loss: 1.8578 - val_accuracy: 0.5146\n",
      "Epoch 176/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5604 - accuracy: 0.5647 - val_loss: 1.9393 - val_accuracy: 0.5003\n",
      "Epoch 177/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5575 - accuracy: 0.5671 - val_loss: 2.0165 - val_accuracy: 0.4848\n",
      "Epoch 178/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5560 - accuracy: 0.5656 - val_loss: 1.9085 - val_accuracy: 0.5047\n",
      "Epoch 179/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.5585 - accuracy: 0.5670 - val_loss: 1.9235 - val_accuracy: 0.5063\n",
      "Epoch 180/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5491 - accuracy: 0.5681 - val_loss: 1.9170 - val_accuracy: 0.5039\n",
      "Epoch 181/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5495 - accuracy: 0.5688 - val_loss: 1.9216 - val_accuracy: 0.5029\n",
      "Epoch 182/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 1.5430 - accuracy: 0.5687 - val_loss: 1.9001 - val_accuracy: 0.5100\n",
      "Epoch 183/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.5427 - accuracy: 0.5711 - val_loss: 1.9444 - val_accuracy: 0.5041\n",
      "Epoch 184/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5465 - accuracy: 0.5689 - val_loss: 1.8485 - val_accuracy: 0.5139\n",
      "Epoch 185/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.5383 - accuracy: 0.5713 - val_loss: 1.9024 - val_accuracy: 0.5108\n",
      "Epoch 186/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.5351 - accuracy: 0.5718 - val_loss: 1.9030 - val_accuracy: 0.5105\n",
      "Epoch 187/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5341 - accuracy: 0.5722 - val_loss: 1.8764 - val_accuracy: 0.5108\n",
      "Epoch 188/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5318 - accuracy: 0.5726 - val_loss: 1.9023 - val_accuracy: 0.5080\n",
      "Epoch 189/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5332 - accuracy: 0.5726 - val_loss: 1.8864 - val_accuracy: 0.5071\n",
      "Epoch 190/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5302 - accuracy: 0.5737 - val_loss: 1.9493 - val_accuracy: 0.5025\n",
      "Epoch 191/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.5251 - accuracy: 0.5733 - val_loss: 1.8631 - val_accuracy: 0.5123\n",
      "Epoch 192/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5269 - accuracy: 0.5736 - val_loss: 1.9137 - val_accuracy: 0.5061\n",
      "Epoch 193/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5167 - accuracy: 0.5767 - val_loss: 1.9488 - val_accuracy: 0.5029\n",
      "Epoch 194/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5171 - accuracy: 0.5762 - val_loss: 1.8758 - val_accuracy: 0.5151\n",
      "Epoch 195/200\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 1.5178 - accuracy: 0.5760 - val_loss: 1.9301 - val_accuracy: 0.5046\n",
      "Epoch 196/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.5099 - accuracy: 0.5775 - val_loss: 1.9036 - val_accuracy: 0.5115\n",
      "Epoch 197/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.5158 - accuracy: 0.5770 - val_loss: 1.9253 - val_accuracy: 0.5078\n",
      "Epoch 198/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5098 - accuracy: 0.5775 - val_loss: 1.9006 - val_accuracy: 0.5152\n",
      "Epoch 199/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5053 - accuracy: 0.5795 - val_loss: 1.9151 - val_accuracy: 0.5106\n",
      "Epoch 200/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.5097 - accuracy: 0.5786 - val_loss: 1.9033 - val_accuracy: 0.5125\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_full, y_full,\n",
    "epochs=200,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_21 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " sequential_6 (Sequential)      (None, 32, 32, 3)    0           ['input_21[0][0]']               \n",
      "                                                                                                  \n",
      " rescaling_20 (Rescaling)       (None, 32, 32, 3)    0           ['sequential_6[2][0]']           \n",
      "                                                                                                  \n",
      " conv2d_111 (Conv2D)            (None, 28, 28, 32)   2400        ['rescaling_20[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_166 (Batch  (None, 28, 28, 32)  128         ['conv2d_111[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_166 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_166[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_166 (Separabl  (None, 28, 28, 32)  1312        ['activation_166[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_167 (Batch  (None, 28, 28, 32)  128         ['separable_conv2d_166[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_167 (Activation)    (None, 28, 28, 32)   0           ['batch_normalization_167[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_167 (Separabl  (None, 28, 28, 32)  1312        ['activation_167[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_89 (MaxPooling2D  (None, 14, 14, 32)  0           ['separable_conv2d_167[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_112 (Conv2D)            (None, 14, 14, 32)   1024        ['conv2d_111[0][0]']             \n",
      "                                                                                                  \n",
      " add_83 (Add)                   (None, 14, 14, 32)   0           ['max_pooling2d_89[0][0]',       \n",
      "                                                                  'conv2d_112[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_168 (Batch  (None, 14, 14, 32)  128         ['add_83[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_168 (Activation)    (None, 14, 14, 32)   0           ['batch_normalization_168[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_168 (Separabl  (None, 14, 14, 64)  2336        ['activation_168[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_169 (Batch  (None, 14, 14, 64)  256         ['separable_conv2d_168[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_169 (Activation)    (None, 14, 14, 64)   0           ['batch_normalization_169[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_169 (Separabl  (None, 14, 14, 64)  4672        ['activation_169[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_90 (MaxPooling2D  (None, 7, 7, 64)    0           ['separable_conv2d_169[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_113 (Conv2D)            (None, 7, 7, 64)     2048        ['add_83[0][0]']                 \n",
      "                                                                                                  \n",
      " add_84 (Add)                   (None, 7, 7, 64)     0           ['max_pooling2d_90[0][0]',       \n",
      "                                                                  'conv2d_113[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_170 (Batch  (None, 7, 7, 64)    256         ['add_84[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_170 (Activation)    (None, 7, 7, 64)     0           ['batch_normalization_170[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_170 (Separabl  (None, 7, 7, 128)   8768        ['activation_170[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_171 (Batch  (None, 7, 7, 128)   512         ['separable_conv2d_170[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_171 (Activation)    (None, 7, 7, 128)    0           ['batch_normalization_171[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_171 (Separabl  (None, 7, 7, 128)   17536       ['activation_171[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_91 (MaxPooling2D  (None, 4, 4, 128)   0           ['separable_conv2d_171[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_114 (Conv2D)            (None, 4, 4, 128)    8192        ['add_84[0][0]']                 \n",
      "                                                                                                  \n",
      " add_85 (Add)                   (None, 4, 4, 128)    0           ['max_pooling2d_91[0][0]',       \n",
      "                                                                  'conv2d_114[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_172 (Batch  (None, 4, 4, 128)   512         ['add_85[0][0]']                 \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_172 (Activation)    (None, 4, 4, 128)    0           ['batch_normalization_172[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_172 (Separabl  (None, 4, 4, 256)   33920       ['activation_172[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " batch_normalization_173 (Batch  (None, 4, 4, 256)   1024        ['separable_conv2d_172[0][0]']   \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_173 (Activation)    (None, 4, 4, 256)    0           ['batch_normalization_173[0][0]']\n",
      "                                                                                                  \n",
      " separable_conv2d_173 (Separabl  (None, 4, 4, 256)   67840       ['activation_173[0][0]']         \n",
      " eConv2D)                                                                                         \n",
      "                                                                                                  \n",
      " max_pooling2d_92 (MaxPooling2D  (None, 2, 2, 256)   0           ['separable_conv2d_173[0][0]']   \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " conv2d_115 (Conv2D)            (None, 2, 2, 256)    32768       ['add_85[0][0]']                 \n",
      "                                                                                                  \n",
      " add_86 (Add)                   (None, 2, 2, 256)    0           ['max_pooling2d_92[0][0]',       \n",
      "                                                                  'conv2d_115[0][0]']             \n",
      "                                                                                                  \n",
      " flatten_17 (Flatten)           (None, 1024)         0           ['add_86[0][0]']                 \n",
      "                                                                                                  \n",
      " dropout_68 (Dropout)           (None, 1024)         0           ['flatten_17[0][0]']             \n",
      "                                                                                                  \n",
      " dense_66 (Dense)               (None, 256)          262400      ['dropout_68[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_69 (Dropout)           (None, 256)          0           ['dense_66[0][0]']               \n",
      "                                                                                                  \n",
      " dense_67 (Dense)               (None, 128)          32896       ['dropout_69[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_70 (Dropout)           (None, 128)          0           ['dense_67[0][0]']               \n",
      "                                                                                                  \n",
      " dense_68 (Dense)               (None, 100)          12900       ['dropout_70[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 495,268\n",
      "Trainable params: 493,796\n",
      "Non-trainable params: 1,472\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(32,32,3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "for size in [32,64,128,256]:\n",
    "    residual = x\n",
    "    \n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    residual = layers.Conv2D(size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(128, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(100, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "keras.callbacks.ModelCheckpoint(\n",
    "filepath=\"cifar100_feature_extraction_without_data_augmentation_DS2X_classic.keras\",\n",
    "save_best_only=True,\n",
    "monitor=\"val_loss\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 4.0719 - accuracy: 0.0774 - val_loss: 3.6722 - val_accuracy: 0.1452\n",
      "Epoch 2/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 3.7140 - accuracy: 0.1235 - val_loss: 3.4796 - val_accuracy: 0.1701\n",
      "Epoch 3/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.5449 - accuracy: 0.1506 - val_loss: 3.3349 - val_accuracy: 0.1970\n",
      "Epoch 4/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 3.4206 - accuracy: 0.1728 - val_loss: 3.2317 - val_accuracy: 0.2146\n",
      "Epoch 5/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 3.3121 - accuracy: 0.1937 - val_loss: 3.1575 - val_accuracy: 0.2262\n",
      "Epoch 6/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 3.2266 - accuracy: 0.2086 - val_loss: 2.9662 - val_accuracy: 0.2660\n",
      "Epoch 7/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 3.1575 - accuracy: 0.2218 - val_loss: 2.9366 - val_accuracy: 0.2653\n",
      "Epoch 8/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 3.0904 - accuracy: 0.2349 - val_loss: 2.8772 - val_accuracy: 0.2826\n",
      "Epoch 9/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 3.0387 - accuracy: 0.2465 - val_loss: 2.9083 - val_accuracy: 0.2735\n",
      "Epoch 10/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.9808 - accuracy: 0.2551 - val_loss: 2.7865 - val_accuracy: 0.3049\n",
      "Epoch 11/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.9340 - accuracy: 0.2658 - val_loss: 2.8161 - val_accuracy: 0.2912\n",
      "Epoch 12/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.8880 - accuracy: 0.2748 - val_loss: 2.8024 - val_accuracy: 0.2984\n",
      "Epoch 13/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.8528 - accuracy: 0.2833 - val_loss: 2.7270 - val_accuracy: 0.3120\n",
      "Epoch 14/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.8165 - accuracy: 0.2921 - val_loss: 2.7329 - val_accuracy: 0.3084\n",
      "Epoch 15/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.7791 - accuracy: 0.2966 - val_loss: 2.6308 - val_accuracy: 0.3351\n",
      "Epoch 16/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.7428 - accuracy: 0.3051 - val_loss: 2.6158 - val_accuracy: 0.3353\n",
      "Epoch 17/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.7103 - accuracy: 0.3130 - val_loss: 2.5976 - val_accuracy: 0.3373\n",
      "Epoch 18/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.6842 - accuracy: 0.3191 - val_loss: 2.6260 - val_accuracy: 0.3324\n",
      "Epoch 19/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.6506 - accuracy: 0.3239 - val_loss: 2.5896 - val_accuracy: 0.3380\n",
      "Epoch 20/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.6295 - accuracy: 0.3297 - val_loss: 2.4632 - val_accuracy: 0.3645\n",
      "Epoch 21/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 2.6010 - accuracy: 0.3361 - val_loss: 2.5084 - val_accuracy: 0.3599\n",
      "Epoch 22/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.5742 - accuracy: 0.3401 - val_loss: 2.4737 - val_accuracy: 0.3695\n",
      "Epoch 23/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.5546 - accuracy: 0.3449 - val_loss: 2.4781 - val_accuracy: 0.3653\n",
      "Epoch 24/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.5304 - accuracy: 0.3489 - val_loss: 2.4176 - val_accuracy: 0.3766\n",
      "Epoch 25/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.5120 - accuracy: 0.3549 - val_loss: 2.4807 - val_accuracy: 0.3676\n",
      "Epoch 26/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.4874 - accuracy: 0.3575 - val_loss: 2.4022 - val_accuracy: 0.3804\n",
      "Epoch 27/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.4676 - accuracy: 0.3637 - val_loss: 2.4640 - val_accuracy: 0.3698\n",
      "Epoch 28/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.4491 - accuracy: 0.3652 - val_loss: 2.4441 - val_accuracy: 0.3736\n",
      "Epoch 29/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 2.4294 - accuracy: 0.3699 - val_loss: 2.3887 - val_accuracy: 0.3849\n",
      "Epoch 30/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.4134 - accuracy: 0.3755 - val_loss: 2.3329 - val_accuracy: 0.3926\n",
      "Epoch 31/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.3970 - accuracy: 0.3772 - val_loss: 2.3659 - val_accuracy: 0.3919\n",
      "Epoch 32/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3760 - accuracy: 0.3814 - val_loss: 2.3220 - val_accuracy: 0.3982\n",
      "Epoch 33/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.3601 - accuracy: 0.3827 - val_loss: 2.3027 - val_accuracy: 0.4004\n",
      "Epoch 34/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 2.3508 - accuracy: 0.3889 - val_loss: 2.2914 - val_accuracy: 0.4053\n",
      "Epoch 35/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.3338 - accuracy: 0.3913 - val_loss: 2.2511 - val_accuracy: 0.4149\n",
      "Epoch 36/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 2.3205 - accuracy: 0.3942 - val_loss: 2.2724 - val_accuracy: 0.4146\n",
      "Epoch 37/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.3074 - accuracy: 0.3938 - val_loss: 2.2269 - val_accuracy: 0.4194\n",
      "Epoch 38/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2849 - accuracy: 0.4028 - val_loss: 2.3126 - val_accuracy: 0.4052\n",
      "Epoch 39/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2756 - accuracy: 0.4019 - val_loss: 2.3253 - val_accuracy: 0.4091\n",
      "Epoch 40/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.2645 - accuracy: 0.4049 - val_loss: 2.2420 - val_accuracy: 0.4166\n",
      "Epoch 41/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.2534 - accuracy: 0.4090 - val_loss: 2.3665 - val_accuracy: 0.3959\n",
      "Epoch 42/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 2.2367 - accuracy: 0.4117 - val_loss: 2.1969 - val_accuracy: 0.4310\n",
      "Epoch 43/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2141 - accuracy: 0.4163 - val_loss: 2.2462 - val_accuracy: 0.4258\n",
      "Epoch 44/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 2.2091 - accuracy: 0.4190 - val_loss: 2.2315 - val_accuracy: 0.4208\n",
      "Epoch 45/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1912 - accuracy: 0.4238 - val_loss: 2.2285 - val_accuracy: 0.4239\n",
      "Epoch 46/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1824 - accuracy: 0.4235 - val_loss: 2.2388 - val_accuracy: 0.4220\n",
      "Epoch 47/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1764 - accuracy: 0.4242 - val_loss: 2.2226 - val_accuracy: 0.4257\n",
      "Epoch 48/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 2.1649 - accuracy: 0.4263 - val_loss: 2.2365 - val_accuracy: 0.4196\n",
      "Epoch 49/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1493 - accuracy: 0.4310 - val_loss: 2.1518 - val_accuracy: 0.4396\n",
      "Epoch 50/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.1463 - accuracy: 0.4318 - val_loss: 2.1976 - val_accuracy: 0.4299\n",
      "Epoch 51/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.1343 - accuracy: 0.4349 - val_loss: 2.1354 - val_accuracy: 0.4444\n",
      "Epoch 52/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 2.1257 - accuracy: 0.4361 - val_loss: 2.1655 - val_accuracy: 0.4425\n",
      "Epoch 53/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1116 - accuracy: 0.4391 - val_loss: 2.1222 - val_accuracy: 0.4514\n",
      "Epoch 54/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.1000 - accuracy: 0.4427 - val_loss: 2.2432 - val_accuracy: 0.4210\n",
      "Epoch 55/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0897 - accuracy: 0.4434 - val_loss: 2.1584 - val_accuracy: 0.4398\n",
      "Epoch 56/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.0750 - accuracy: 0.4477 - val_loss: 2.1225 - val_accuracy: 0.4492\n",
      "Epoch 57/200\n",
      "1563/1563 [==============================] - 27s 18ms/step - loss: 2.0741 - accuracy: 0.4489 - val_loss: 2.1496 - val_accuracy: 0.4380\n",
      "Epoch 58/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0624 - accuracy: 0.4503 - val_loss: 2.1253 - val_accuracy: 0.4487\n",
      "Epoch 59/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0548 - accuracy: 0.4527 - val_loss: 2.1555 - val_accuracy: 0.4403\n",
      "Epoch 60/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 2.0475 - accuracy: 0.4528 - val_loss: 2.2124 - val_accuracy: 0.4306\n",
      "Epoch 61/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0420 - accuracy: 0.4538 - val_loss: 2.1279 - val_accuracy: 0.4452\n",
      "Epoch 62/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 2.0316 - accuracy: 0.4577 - val_loss: 2.1386 - val_accuracy: 0.4468\n",
      "Epoch 63/200\n",
      "1563/1563 [==============================] - 26s 17ms/step - loss: 2.0201 - accuracy: 0.4609 - val_loss: 2.2245 - val_accuracy: 0.4273\n",
      "Epoch 64/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 2.0129 - accuracy: 0.4628 - val_loss: 2.0960 - val_accuracy: 0.4570\n",
      "Epoch 65/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 2.0074 - accuracy: 0.4617 - val_loss: 2.1089 - val_accuracy: 0.4530\n",
      "Epoch 66/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.9966 - accuracy: 0.4647 - val_loss: 2.1238 - val_accuracy: 0.4502\n",
      "Epoch 67/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.9881 - accuracy: 0.4667 - val_loss: 2.1115 - val_accuracy: 0.4531\n",
      "Epoch 68/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9827 - accuracy: 0.4678 - val_loss: 2.0954 - val_accuracy: 0.4566\n",
      "Epoch 69/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.9763 - accuracy: 0.4704 - val_loss: 2.0830 - val_accuracy: 0.4577\n",
      "Epoch 70/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9733 - accuracy: 0.4708 - val_loss: 2.1498 - val_accuracy: 0.4436\n",
      "Epoch 71/200\n",
      "1563/1563 [==============================] - 34s 21ms/step - loss: 1.9629 - accuracy: 0.4721 - val_loss: 2.0961 - val_accuracy: 0.4523\n",
      "Epoch 72/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9478 - accuracy: 0.4775 - val_loss: 2.0550 - val_accuracy: 0.4627\n",
      "Epoch 73/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9478 - accuracy: 0.4749 - val_loss: 2.0409 - val_accuracy: 0.4646\n",
      "Epoch 74/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.9377 - accuracy: 0.4776 - val_loss: 2.0425 - val_accuracy: 0.4672\n",
      "Epoch 75/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9248 - accuracy: 0.4799 - val_loss: 2.0553 - val_accuracy: 0.4656\n",
      "Epoch 76/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.9254 - accuracy: 0.4818 - val_loss: 2.0248 - val_accuracy: 0.4720\n",
      "Epoch 77/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9209 - accuracy: 0.4829 - val_loss: 2.0552 - val_accuracy: 0.4629\n",
      "Epoch 78/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9126 - accuracy: 0.4843 - val_loss: 2.0761 - val_accuracy: 0.4596\n",
      "Epoch 79/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8993 - accuracy: 0.4864 - val_loss: 2.0152 - val_accuracy: 0.4742\n",
      "Epoch 80/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.9048 - accuracy: 0.4845 - val_loss: 2.0488 - val_accuracy: 0.4622\n",
      "Epoch 81/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.8899 - accuracy: 0.4883 - val_loss: 2.0288 - val_accuracy: 0.4720\n",
      "Epoch 82/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8917 - accuracy: 0.4881 - val_loss: 2.0438 - val_accuracy: 0.4680\n",
      "Epoch 83/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8825 - accuracy: 0.4904 - val_loss: 2.0606 - val_accuracy: 0.4607\n",
      "Epoch 84/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8782 - accuracy: 0.4929 - val_loss: 2.0311 - val_accuracy: 0.4708\n",
      "Epoch 85/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.8788 - accuracy: 0.4927 - val_loss: 2.0484 - val_accuracy: 0.4669\n",
      "Epoch 86/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8655 - accuracy: 0.4952 - val_loss: 1.9801 - val_accuracy: 0.4817\n",
      "Epoch 87/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8548 - accuracy: 0.4986 - val_loss: 2.0107 - val_accuracy: 0.4729\n",
      "Epoch 88/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8513 - accuracy: 0.4977 - val_loss: 2.0011 - val_accuracy: 0.4779\n",
      "Epoch 89/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8485 - accuracy: 0.4983 - val_loss: 2.0273 - val_accuracy: 0.4741\n",
      "Epoch 90/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8471 - accuracy: 0.4977 - val_loss: 2.0388 - val_accuracy: 0.4669\n",
      "Epoch 91/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.8295 - accuracy: 0.5032 - val_loss: 2.0229 - val_accuracy: 0.4731\n",
      "Epoch 92/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.8348 - accuracy: 0.5014 - val_loss: 1.9891 - val_accuracy: 0.4790\n",
      "Epoch 93/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8294 - accuracy: 0.5036 - val_loss: 2.0220 - val_accuracy: 0.4730\n",
      "Epoch 94/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.8120 - accuracy: 0.5088 - val_loss: 1.9700 - val_accuracy: 0.4855\n",
      "Epoch 95/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.8089 - accuracy: 0.5080 - val_loss: 2.0135 - val_accuracy: 0.4783\n",
      "Epoch 96/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.8134 - accuracy: 0.5065 - val_loss: 1.9821 - val_accuracy: 0.4796\n",
      "Epoch 97/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.8024 - accuracy: 0.5081 - val_loss: 1.9770 - val_accuracy: 0.4814\n",
      "Epoch 98/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7936 - accuracy: 0.5110 - val_loss: 2.0005 - val_accuracy: 0.4752\n",
      "Epoch 99/200\n",
      "1563/1563 [==============================] - 27s 17ms/step - loss: 1.7902 - accuracy: 0.5106 - val_loss: 2.0120 - val_accuracy: 0.4773\n",
      "Epoch 100/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7919 - accuracy: 0.5118 - val_loss: 1.9652 - val_accuracy: 0.4881\n",
      "Epoch 101/200\n",
      "1563/1563 [==============================] - 26s 16ms/step - loss: 1.7821 - accuracy: 0.5150 - val_loss: 1.9881 - val_accuracy: 0.4840\n",
      "Epoch 102/200\n",
      "1563/1563 [==============================] - 29s 18ms/step - loss: 1.7769 - accuracy: 0.5154 - val_loss: 2.0146 - val_accuracy: 0.4753\n",
      "Epoch 103/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7724 - accuracy: 0.5166 - val_loss: 1.9726 - val_accuracy: 0.4886\n",
      "Epoch 104/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7680 - accuracy: 0.5175 - val_loss: 1.9895 - val_accuracy: 0.4816\n",
      "Epoch 105/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.7688 - accuracy: 0.5165 - val_loss: 1.9769 - val_accuracy: 0.4844\n",
      "Epoch 106/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7594 - accuracy: 0.5177 - val_loss: 1.9292 - val_accuracy: 0.4948\n",
      "Epoch 107/200\n",
      "1563/1563 [==============================] - 29s 19ms/step - loss: 1.7570 - accuracy: 0.5187 - val_loss: 1.9864 - val_accuracy: 0.4797\n",
      "Epoch 108/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7503 - accuracy: 0.5198 - val_loss: 1.9265 - val_accuracy: 0.4931\n",
      "Epoch 109/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7448 - accuracy: 0.5207 - val_loss: 1.9681 - val_accuracy: 0.4840\n",
      "Epoch 110/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7424 - accuracy: 0.5239 - val_loss: 1.9888 - val_accuracy: 0.4835\n",
      "Epoch 111/200\n",
      "1563/1563 [==============================] - 33s 21ms/step - loss: 1.7354 - accuracy: 0.5248 - val_loss: 1.9105 - val_accuracy: 0.5008\n",
      "Epoch 112/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.7372 - accuracy: 0.5243 - val_loss: 1.9974 - val_accuracy: 0.4812\n",
      "Epoch 113/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7278 - accuracy: 0.5284 - val_loss: 1.9786 - val_accuracy: 0.4839\n",
      "Epoch 114/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7225 - accuracy: 0.5281 - val_loss: 1.9517 - val_accuracy: 0.4915\n",
      "Epoch 115/200\n",
      "1563/1563 [==============================] - 25s 16ms/step - loss: 1.7209 - accuracy: 0.5276 - val_loss: 1.9401 - val_accuracy: 0.4910\n",
      "Epoch 116/200\n",
      "1563/1563 [==============================] - 31s 20ms/step - loss: 1.7182 - accuracy: 0.5296 - val_loss: 1.9434 - val_accuracy: 0.4946\n",
      "Epoch 117/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7143 - accuracy: 0.5304 - val_loss: 1.9181 - val_accuracy: 0.4941\n",
      "Epoch 118/200\n",
      "1563/1563 [==============================] - 34s 22ms/step - loss: 1.7078 - accuracy: 0.5313 - val_loss: 1.9424 - val_accuracy: 0.4960\n",
      "Epoch 119/200\n",
      "1563/1563 [==============================] - 32s 21ms/step - loss: 1.7043 - accuracy: 0.5318 - val_loss: 1.9066 - val_accuracy: 0.5024\n",
      "Epoch 120/200\n",
      "1563/1563 [==============================] - 28s 18ms/step - loss: 1.6993 - accuracy: 0.5328 - val_loss: 1.9858 - val_accuracy: 0.4865\n",
      "Epoch 121/200\n",
      "1563/1563 [==============================] - 30s 19ms/step - loss: 1.6966 - accuracy: 0.5332 - val_loss: 1.9202 - val_accuracy: 0.5027\n",
      "Epoch 122/200\n",
      "1563/1563 [==============================] - 32s 20ms/step - loss: 1.6896 - accuracy: 0.5362 - val_loss: 1.9543 - val_accuracy: 0.4933\n",
      "Epoch 123/200\n",
      " 965/1563 [=================>............] - ETA: 11s - loss: 1.6907 - accuracy: 0.5349"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "x_train_2X, y_train_2X,\n",
    "epochs=200,\n",
    "batch_size=64,\n",
    "validation_data=(x_test, y_test),\n",
    "callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
